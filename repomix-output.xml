This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
MoveInsight/
  Assets.xcassets/
    AccentColor.colorset/
      Contents.json
    AppIcon.appiconset/
      Contents.json
    player_shuttle_distance.imageset/
      Contents.json
    velocity_profile_chart.imageset/
      Contents.json
    Contents.json
  en.lproj/
    Localizable.strings
  Preview Content/
    Preview Assets.xcassets/
      Contents.json
  zh-Hans.lproj/
    Localizable.strings
  BodyPoseTypes.swift
  ColorManager.swift
  CombinedVideo3DView.swift
  ContentView.swift
  CustomTabBar.swift
  Extensions.swift
  HomeView.swift
  Info.plist
  ModelVideoLoader.swift
  MoveInsightApp.swift
  SceneView3D.swift
  TechniqueAnalysisService.swift
  TechniqueComparisonView.swift
  TechniqueDetailView.swift
  TechniquesListView.swift
  TechniqueVideoUploadView.swift
  UIComponents.swift
  UploadTabView.swift
  VideoPlayerRepresentable.swift
  VideoPlayerViewModel.swift
  VideoTransferUtils.swift
MoveInsight.xcodeproj/
  project.xcworkspace/
    contents.xcworkspacedata
  xcuserdata/
    charlie.xcuserdatad/
      xcschemes/
        xcschememanagement.plist
  project.pbxproj
MoveInsightServer/
  analysis_server.py
  requirements.txt
  swing_diagnose.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="MoveInsight/Assets.xcassets/AccentColor.colorset/Contents.json">
{
  "colors" : [
    {
      "idiom" : "universal"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}
</file>

<file path="MoveInsight/Assets.xcassets/player_shuttle_distance.imageset/Contents.json">
{
  "images" : [
    {
      "filename" : "Screenshot 2025-04-25 at 20.31.05.png",
      "idiom" : "universal",
      "scale" : "1x"
    },
    {
      "idiom" : "universal",
      "scale" : "2x"
    },
    {
      "idiom" : "universal",
      "scale" : "3x"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}
</file>

<file path="MoveInsight/Assets.xcassets/velocity_profile_chart.imageset/Contents.json">
{
  "images" : [
    {
      "filename" : "velocity_profile_chart.png",
      "idiom" : "universal",
      "scale" : "1x"
    },
    {
      "idiom" : "universal",
      "scale" : "2x"
    },
    {
      "idiom" : "universal",
      "scale" : "3x"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}
</file>

<file path="MoveInsight/Assets.xcassets/Contents.json">
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}
</file>

<file path="MoveInsight/Preview Content/Preview Assets.xcassets/Contents.json">
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}
</file>

<file path="MoveInsight/ColorManager.swift">
import SwiftUI

struct ColorManager {
    /// Brand purple color used in MoveInsight logo (#5C2D91)
    static let accentColor = Color(hex: "5C2D91")
    
    // Dynamic background: black in dark mode, white in light mode.
    static var background: Color {
        Color(UIColor { traitCollection in
            traitCollection.userInterfaceStyle == .dark ? UIColor.black : UIColor.white
        })
    }
    
    // Primary text color: white in dark mode, black in light mode.
    static var textPrimary: Color {
        Color(UIColor { traitCollection in
            traitCollection.userInterfaceStyle == .dark ? UIColor.white : UIColor.black
        })
    }
    
    // Secondary text color: light gray in dark mode, dark gray in light mode.
    static var textSecondary: Color {
        Color(UIColor { traitCollection in
            traitCollection.userInterfaceStyle == .dark ? UIColor.lightGray : UIColor.darkGray
        })
    }
    
    // Card background: a subtle gray that adapts to light/dark mode.
    static var cardBackground: Color {
        Color(UIColor { traitCollection in
            if traitCollection.userInterfaceStyle == .dark {
                return UIColor.systemGray6
            } else {
                return UIColor.systemGray5
            }
        })
    }
    
    /// Plus button color: white in light mode, black in dark mode.
    static var uploadPlusButtonColor: Color {
        Color(UIColor { traitCollection in
            traitCollection.userInterfaceStyle == .dark ? UIColor.black : UIColor.white
        })
    }
}

extension Color {
    init(hex: String) {
        let hex = hex.trimmingCharacters(in: CharacterSet.alphanumerics.inverted)
        var int: UInt64 = 0
        Scanner(string: hex).scanHexInt64(&int)
        let a, r, g, b: UInt64
        switch hex.count {
        case 3: // RGB (12-bit)
            (a, r, g, b) = (255, (int >> 8) * 17,
                                 (int >> 4 & 0xF) * 17,
                                 (int & 0xF) * 17)
        case 6: // RGB (24-bit)
            (a, r, g, b) = (255,
                           int >> 16,
                           int >> 8 & 0xFF,
                           int & 0xFF)
        case 8: // ARGB (32-bit)
            (a, r, g, b) = (int >> 24,
                           int >> 16 & 0xFF,
                           int >> 8 & 0xFF,
                           int & 0xFF)
        default:
            (a, r, g, b) = (255, 0, 0, 0) // fallback: opaque black
        }

        self.init(
            .sRGB,
            red: Double(r) / 255,
            green: Double(g) / 255,
            blue: Double(b) / 255,
            opacity: Double(a) / 255
        )
    }
}
</file>

<file path="MoveInsight/CustomTabBar.swift">
import SwiftUI

// MARK: - Custom Tab Bar
struct CustomTabBar: View {
    @Binding var selectedTab: Int

    var body: some View {
        EvenlySpacedTabBar(selectedTab: $selectedTab)
    }
}

// MARK: - Evenly Spaced Tab Bar
struct EvenlySpacedTabBar: View {
    @Binding var selectedTab: Int
    
    var body: some View {
        HStack {
            Spacer()
            
            // Home button
            TabBarButtonEvenly(iconName: "house.fill", isSelected: selectedTab == 0) {
                selectedTab = 0
            }
            
            Spacer()
            
            // Training button
            TabBarButtonEvenly(iconName: "figure.run", isSelected: selectedTab == 1) {
                selectedTab = 1
            }
            
            Spacer()
            
            // Plus (upload) button – uses dynamic color for plus icon.
            Button(action: {
                // In this updated design, the Upload tab is directly rendered.
                selectedTab = 2
            }) {
                ZStack {
                    Circle()
                        .fill(ColorManager.accentColor)
                        .frame(width: 44, height: 44)
                    
                    Image(systemName: "plus")
                        .font(.system(size: 18, weight: .bold))
                        .foregroundColor(ColorManager.uploadPlusButtonColor)
                }
            }
            
            Spacer()
            
            // Videos button
            TabBarButtonEvenly(iconName: "play.rectangle.fill", isSelected: selectedTab == 3) {
                selectedTab = 3
            }
            
            Spacer()
            
            // Messages button
            TabBarButtonEvenly(iconName: "message.fill", isSelected: selectedTab == 4) {
                selectedTab = 4
            }
            
            Spacer()
        }
        .padding(.vertical, 10)
        .background(ColorManager.background)
        .overlay(
            Rectangle()
                .frame(height: 1)
                .foregroundColor(ColorManager.textSecondary.opacity(0.3)),
            alignment: .top
        )
    }
}

// MARK: - Tab Bar Button for Evenly Spaced Layout
struct TabBarButtonEvenly: View {
    let iconName: String
    let isSelected: Bool
    let action: () -> Void

    var body: some View {
        Button(action: action) {
            Image(systemName: iconName)
                .font(.system(size: 20))
                .foregroundColor(isSelected ? ColorManager.accentColor : ColorManager.textPrimary)
        }
    }
}

// MARK: - Legacy Tab Bar Button Component (kept for backward compatibility)
struct TabBarButton: View {
    let iconName: String
    let title: LocalizedStringKey
    let isSelected: Bool
    let action: () -> Void

    var body: some View {
        Button(action: action) {
            VStack(spacing: 3) {
                Image(systemName: iconName)
                    .font(.system(size: 18))
                    .foregroundColor(isSelected ? ColorManager.accentColor : ColorManager.textPrimary)

                Text(title)
                    .font(.system(size: 11))
                    .foregroundColor(isSelected ? ColorManager.accentColor : ColorManager.textPrimary)
            }
            .frame(maxWidth: .infinity)
        }
    }
}
</file>

<file path="MoveInsight/Info.plist">
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
    <key>NSAppTransportSecurity</key>
    <dict>
        <key>NSExceptionDomains</key>
        <dict>
            <key>115.188.74.78</key>
            <dict>
                <key>NSIncludesSubdomains</key>
                <true/>
                <key>NSExceptionAllowsInsecureHTTPLoads</key>
                <true/>
                <key>NSExceptionRequiresForwardSecrecy</key>
                <true/>
                <key>NSExceptionMinimumTLSVersion</key>
                <string>TLSv1.2</string>
                <key>NSThirdPartyExceptionAllowsInsecureHTTPLoads</key>
                <false/>
                <key>NSThirdPartyExceptionRequiresForwardSecrecy</key>
                <true/>
                <key>NSThirdPartyExceptionMinimumTLSVersion</key>
                <string>TLSv1.2</string>
                <key>NSRequiresCertificateTransparency</key>
                <false/>
            </dict>
        </dict>
    </dict>
</dict>
</plist>
</file>

<file path="MoveInsight/ModelVideoLoader.swift">
import Foundation
import AVKit

// This class helps manage model videos in the app's bundle
class ModelVideoLoader {
    static let shared = ModelVideoLoader()
    
    // Map of technique names to their video filenames (without extension)
    private let techniqueVideoMap: [String: String] = [
        "Underhand Clear": "underhand_clear_model",
        // Other techniques would be added here as they become available
    ]
    
    private init() {
        // Log available model videos on initialization
        print("ModelVideoLoader initialized with techniques: \(techniqueVideoMap.keys.joined(separator: ", "))")
    }
    
    // Get the URL for a specific technique's model video
    func getModelVideoURL(for technique: String) -> URL? {
        // Try to find the exact technique name in our map
        let cleanTechniqueName = technique.trimmingCharacters(in: .whitespacesAndNewlines)
        
        if let filename = techniqueVideoMap[cleanTechniqueName] {
            if let url = Bundle.main.url(forResource: filename, withExtension: "mov") {
                print("Found model video for \(cleanTechniqueName) at \(url.path)")
                return url
            }
        }
        
        // Try with a normalized technique name (lowercase, underscores)
        let normalizedName = cleanTechniqueName.lowercased().replacingOccurrences(of: " ", with: "_")
        if let url = Bundle.main.url(forResource: normalizedName, withExtension: "mov") {
            print("Found model video using normalized name \(normalizedName)")
            return url
        }
        
        // Special case for backhand clear (our only implemented video currently)
        if cleanTechniqueName.lowercased().contains("backhand") {
            if let fallbackURL = Bundle.main.url(forResource: "backhand_clear_model", withExtension: "mov") {
                print("Found underhand clear model video as fallback")
                return fallbackURL
            }
        }
        
        print("No model video found for technique: \(technique)")
        return nil
    }
    
    // Check if a model video exists for a specific technique
    func hasModelVideo(for technique: String) -> Bool {
        return getModelVideoURL(for: technique) != nil
    }
    
    // Create a VideoPlayerViewModel for a model video
    func createModelVideoViewModel(for technique: String) -> VideoPlayerViewModel? {
        guard let videoURL = getModelVideoURL(for: technique) else {
            print("Failed to create model ViewModel: no URL found for \(technique)")
            return nil
        }
        
        print("Creating model VideoPlayerViewModel for \(technique)")
        return VideoPlayerViewModel(
            videoURL: videoURL,
            videoSource: .secondary
        )
    }
}
</file>

<file path="MoveInsight/UIComponents.swift">
import SwiftUI

// MARK: - Custom Upload Button Style
struct UploadButton: View {
    let title: LocalizedStringKey
    let iconName: String
    let action: () -> Void

    var body: some View {
        Button(action: action) {
            HStack {
                Image(systemName: iconName)
                    .font(.system(size: 22))
                Text(title)
                    .font(.headline)
            }
            .foregroundColor(.white) // Always white text
            .frame(maxWidth: .infinity)
            .padding()
            .background(ColorManager.accentColor)
            .cornerRadius(12)
        }
        .padding(.horizontal)
    }
}

// MARK: - Padding Constant
// Define standard padding to use consistently
extension CGFloat {
    static let standard: CGFloat = 16.0
}
</file>

<file path="MoveInsight/VideoPlayerRepresentable.swift">
import SwiftUI
import AVKit

// MARK: - Video Player Representable (UIViewRepresentable)
struct VideoPlayerRepresentable: UIViewRepresentable {
    let player: AVPlayer
    @Binding var videoRect: CGRect // Binding to pass videoRect back out

    // Create the custom UIView subclass
    func makeUIView(context: Context) -> PlayerUIView {
        print("Making PlayerUIView")
        let view = PlayerUIView(player: player)
        // Set the callback to update the binding
        view.onVideoRectChange = { rect in
            DispatchQueue.main.async {
                if self.videoRect != rect {
                    self.videoRect = rect
                }
            }
        }
        return view
    }

    // Update the UIView
    func updateUIView(_ uiView: PlayerUIView, context: Context) {
        if uiView.playerLayer.player !== player {
            print("Updating player instance in PlayerUIView")
            uiView.playerLayer.player = player
        }
        uiView.onVideoRectChange = { rect in
            DispatchQueue.main.async {
                if self.videoRect != rect {
                    self.videoRect = rect
                }
            }
        }
        uiView.playerLayer.videoGravity = .resizeAspect
    }
    
    // Clean up resources if needed
    static func dismantleUIView(_ uiView: PlayerUIView, coordinator: ()) {
        print("Dismantling PlayerUIView")
        uiView.playerLayer.player = nil
        uiView.onVideoRectChange = nil // Clear callback
    }
}

// MARK: - Custom UIView for AVPlayerLayer
class PlayerUIView: UIView {
    // Callback closure to report videoRect changes
    var onVideoRectChange: ((CGRect) -> Void)?
    private var lastKnownVideoRect: CGRect = .zero // Store last rect to avoid redundant callbacks

    // Override the layerClass property to specify AVPlayerLayer
    override static var layerClass: AnyClass {
        AVPlayerLayer.self
    }

    // Convenience accessor for the layer as an AVPlayerLayer
    var playerLayer: AVPlayerLayer {
        return layer as! AVPlayerLayer
    }

    // Initializer to set the player on the layer
    init(player: AVPlayer) {
        super.init(frame: .zero)
        playerLayer.player = player
        playerLayer.videoGravity = .resizeAspect // Ensure video scales correctly
        playerLayer.backgroundColor = UIColor.black.cgColor // Set background color for the layer
        self.backgroundColor = .black // Set background for the view itself
        print("PlayerUIView initialized, player assigned to layer.")
    }

    required init?(coder: NSCoder) {
        fatalError("init(coder:) has not been implemented")
    }
    
    override func layoutSubviews() {
        super.layoutSubviews()
        // Ensure the player layer's frame always matches the view's bounds
        if playerLayer.frame != self.bounds {
            print("LayoutSubviews: Updating playerLayer frame to \(self.bounds)")
            playerLayer.frame = self.bounds
        }
         
        // Get the current videoRect and report if changed
        let currentVideoRect = playerLayer.videoRect
        if currentVideoRect != lastKnownVideoRect && !currentVideoRect.isInfinite && !currentVideoRect.isNull {
            print("LayoutSubviews: videoRect changed to \(currentVideoRect)")
            lastKnownVideoRect = currentVideoRect
            onVideoRectChange?(currentVideoRect) // Call the callback
        }
    }
}
</file>

<file path="MoveInsight/VideoTransferUtils.swift">
import SwiftUI
import PhotosUI

// MARK: - VideoItem Transferable
struct VideoItem: Transferable {
    let url: URL
    
    static var transferRepresentation: some TransferRepresentation {
        FileRepresentation(contentType: .movie) { movie in
            SentTransferredFile(movie.url)
        } importing: { received in
            // Copy to a temporary location to ensure we have access
            let tempDir = FileManager.default.temporaryDirectory
            let fileName = "\(UUID().uuidString).\(received.file.pathExtension)" // Ensure unique filename
            let copyURL = tempDir.appendingPathComponent(fileName)
            
            // Attempt to remove existing file at destination URL before copying
            try? FileManager.default.removeItem(at: copyURL)

            try FileManager.default.copyItem(at: received.file, to: copyURL)
            return Self.init(url: copyURL)
        }
    }
}
</file>

<file path="MoveInsight.xcodeproj/project.xcworkspace/contents.xcworkspacedata">
<?xml version="1.0" encoding="UTF-8"?>
<Workspace
   version = "1.0">
   <FileRef
      location = "self:">
   </FileRef>
</Workspace>
</file>

<file path="MoveInsight.xcodeproj/xcuserdata/charlie.xcuserdatad/xcschemes/xcschememanagement.plist">
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
	<key>SchemeUserState</key>
	<dict>
		<key>MoveInsight.xcscheme_^#shared#^_</key>
		<dict>
			<key>orderHint</key>
			<integer>0</integer>
		</dict>
	</dict>
</dict>
</plist>
</file>

<file path="MoveInsightServer/requirements.txt">
absl-py==2.2.2
annotated-types==0.7.0
anyio==4.9.0
attrs==25.3.0
cffi==1.17.1
click==8.2.0
contourpy==1.3.2
cycler==0.12.1
fastapi==0.115.12
flatbuffers==25.2.10
fonttools==4.58.0
h11==0.16.0
idna==3.10
jax==0.6.0
jaxlib==0.6.0
kiwisolver==1.4.8
matplotlib==3.10.3
mediapipe==0.10.21
ml_dtypes==0.5.1
numpy==1.26.4
opencv-contrib-python==4.11.0.86
opencv-python==4.11.0.86
opt_einsum==3.4.0
packaging==25.0
pillow==11.2.1
protobuf==4.25.7
pycparser==2.22
pydantic==2.11.4
pydantic_core==2.33.2
pyparsing==3.2.3
python-dateutil==2.9.0.post0
python-multipart==0.0.20
scipy==1.15.3
sentencepiece==0.2.0
setuptools==78.1.1
six==1.17.0
sniffio==1.3.1
sounddevice==0.5.1
starlette==0.46.2
typing-inspection==0.4.0
typing_extensions==4.13.2
uvicorn==0.34.2
wheel==0.45.1
</file>

<file path="MoveInsightServer/swing_diagnose.py">
# MoveInsightServer/swing_diagnose.py
# Technical indicators to be judged:
#   1. **shoulder_abduction**: Avoid dropping the elbow.
#   2. **elbow_flexion**: Keep the elbow joint tight.
#   3. **elbow_lower**: The elbow of the hitting hand should be lower than the non-hitting hand's elbow.
#   4. **foot_direction_aligned**: Are the directions of the toes of both feet consistent?
#   5. **proximal_to_distal_sequence**: Is the sequence of upper limb exertion correct during the swing?
#   6. **hip_forward_shift**: Hip moves forward during the swing.
#   7. **trunk_rotation_completed**: Is the trunk rotation sufficient during the swing?

from typing import Dict, List
import numpy as np
# IMPORTANT: This script currently processes 2D keypoints (x, y).
# If 3D keypoints (x, y, z) are passed from the server, 
# the server should slice them to 2D (x,y) before calling evaluate_swing_rules.
# Future improvements could adapt these rules for 3D analysis.

def evaluate_swing_rules(keypoints: Dict[str, np.ndarray], dominant_side: str ='Right') -> Dict[str, bool]:
    """
    Evaluates swing rules based on 2D keypoint data.

    KEYPOINTS Input (EXPECTS 2D DATA):
    A dictionary where keys are joint names (e.g., 'RightShoulder') and
    values are NumPy arrays of shape (T, 2) representing (X, Y) coordinates
    for T frames.

    If 3D data (T,3) is available, the caller MUST pass data[:, :2].
    """
    result: Dict[str, bool] = {}
    default_rules_keys = [ # These keys define the structure of the output
        'shoulder_abduction', 'elbow_flexion', 'elbow_lower', 
        'foot_direction_aligned', 'proximal_to_distal_sequence', 
        'hip_forward_shift', 'trunk_rotation_completed'
    ]

    # Initialize all rules to False
    for rule_key in default_rules_keys:
        result[rule_key] = False

    # --- Initial checks for data validity ---
    if not keypoints:
        print("⚠️ evaluate_swing_rules: Empty keypoints dictionary. Returning all rules as False.")
        return result.copy()

    D = dominant_side
    ND = 'Left' if D == 'Right' else 'Right'
    
    required_joint_names = [
        f'{D}Shoulder', f'{D}Elbow', f'{D}Wrist', f'{D}Hip',
        f'{ND}Shoulder', f'{ND}Elbow', f'{ND}Hip', # Non-dominant side joints also needed
        'RightToe', 'RightHeel', 'LeftToe', 'LeftHeel'
    ]
    
    # Check for presence and shape of required joints
    min_frames = float('inf')
    for joint_name in required_joint_names:
        if joint_name not in keypoints or not isinstance(keypoints[joint_name], np.ndarray) or keypoints[joint_name].ndim != 2 or keypoints[joint_name].shape[1] != 2:
            print(f"⚠️ evaluate_swing_rules: Missing, not a NumPy array, or incorrect shape for required joint: {joint_name}. Expected (T,2). Returning all rules as False.")
            return result.copy()
        if keypoints[joint_name].shape[0] == 0: # No frames for this joint
             print(f"⚠️ evaluate_swing_rules: Zero frames for required joint: {joint_name}. Returning all rules as False.")
             return result.copy()
        min_frames = min(min_frames, keypoints[joint_name].shape[0])

    if min_frames == float('inf') or min_frames == 0:
        print("⚠️ evaluate_swing_rules: No valid frame data found across required joints. Returning all rules as False.")
        return result.copy()

    T = min_frames # Use the minimum number of frames available across all required joints

    # Ensure all keypoints are consistently sliced to T frames and handle potential NaNs from upstream
    processed_keypoints: Dict[str, np.ndarray] = {}
    for joint_name in required_joint_names: # Only process required joints
        kp_data = keypoints[joint_name][:T] # Slice to T frames
        # If align_keypoints_with_interpolation filled with NaNs, they will persist here.
        # Calculations below need to be NaN-aware.
        processed_keypoints[joint_name] = kp_data
    
    # Add optional hand keypoint if present and valid
    optional_hand_key = f'{D}Hand'
    if optional_hand_key in keypoints and \
       isinstance(keypoints[optional_hand_key], np.ndarray) and \
       keypoints[optional_hand_key].ndim == 2 and \
       keypoints[optional_hand_key].shape[1] == 2 and \
       keypoints[optional_hand_key].shape[0] > 0:
        processed_keypoints[optional_hand_key] = keypoints[optional_hand_key][:T]


    # --- Angle Calculation Helper Functions (NaN-aware) ---
    def angle_between(v1: np.ndarray, v2: np.ndarray) -> float:
        if np.isnan(v1).any() or np.isnan(v2).any(): return np.nan
        norm_v1 = np.linalg.norm(v1)
        norm_v2 = np.linalg.norm(v2)
        if norm_v1 < 1e-6 or norm_v2 < 1e-6: return 180.0 # Default for zero vectors
        
        unit_v1 = v1 / norm_v1
        unit_v2 = v2 / norm_v2
        dot_product = np.clip(np.dot(unit_v1, unit_v2), -1.0, 1.0)
        return np.degrees(np.arccos(dot_product))

    def get_joint_angle_at_frame(kp_dict: Dict[str, np.ndarray], joint_A_name: str, joint_B_name: str, joint_C_name: str, frame_idx: int) -> float:
        pt_A = kp_dict[joint_A_name][frame_idx]
        pt_B = kp_dict[joint_B_name][frame_idx]
        pt_C = kp_dict[joint_C_name][frame_idx]
        if np.isnan(pt_A).any() or np.isnan(pt_B).any() or np.isnan(pt_C).any():
            return np.nan
        return angle_between(pt_A - pt_B, pt_C - pt_B)

    # --- Phase Split Logic ---
    elbow_x_vel = np.zeros(T)
    if T > 1:
        # Ensure dominant elbow data is not all NaN before differencing
        dom_elbow_x_coords = processed_keypoints[f'{D}Elbow'][:, 0]
        if not np.all(np.isnan(dom_elbow_x_coords)):
            elbow_x_vel[1:] = np.diff(dom_elbow_x_coords) # NaNs will propagate if present
        else: # All NaN, velocity is effectively NaN or zero
            elbow_x_vel.fill(np.nan)


    # Smooth velocity, handling NaNs by ignoring them in convolution or using NaN-aware mean
    smoothed_vel = elbow_x_vel.copy() # Default to original if not enough points for smoothing
    window_size = min(5, T - 1 if T > 1 else 1)
    if window_size >= 2 and T > window_size: # window_size must be at least 1 for np.ones
        # Basic NaN-robust smoothing: convolve only valid parts or use rolling mean
        # For simplicity, let's use a simple moving average that handles NaNs
        # This is a placeholder for a more robust NaN-aware smoothing if needed.
        # A common approach is to interpolate NaNs before smoothing, or use pandas.Series.rolling(..).mean()
        # For now, if NaNs are present, smoothing quality might degrade.
        # Let's assume align_keypoints minimizes NaNs for critical joints.
        if not np.all(np.isnan(elbow_x_vel)):
            # Simple convolution, NaNs will propagate.
            conv_weights = np.ones(window_size) / window_size
            smoothed_vel_conv = np.convolve(elbow_x_vel, conv_weights, mode='valid')
            
            padding_len = T - len(smoothed_vel_conv)
            padding_start = padding_len // 2
            padding_end = padding_len - padding_start
            
            if len(smoothed_vel_conv) > 0: # Ensure conv result is not empty
                 smoothed_vel = np.pad(smoothed_vel_conv, (padding_start, padding_end), 'edge')
            # else smoothed_vel remains elbow_x_vel (e.g. if T is too small for 'valid' mode to produce output)

    swing_start_idx = 0
    if T > 1 and not np.all(np.isnan(smoothed_vel)):
        # Find where velocity changes from negative to positive (elbow starts moving forward)
        for t in range(1, T):
            if not np.isnan(smoothed_vel[t-1]) and not np.isnan(smoothed_vel[t]):
                if smoothed_vel[t-1] < 0 and smoothed_vel[t] >= 0:
                    swing_start_idx = t
                    break
        if swing_start_idx == 0: # Fallback: first significant positive velocity
            vel_abs_max = np.nanmax(np.abs(smoothed_vel))
            vel_threshold = vel_abs_max * 0.15 if vel_abs_max > 0 else 0.01
            for t in range(1,T):
                if not np.isnan(smoothed_vel[t]) and smoothed_vel[t] > vel_threshold:
                    swing_start_idx = t
                    break
    if swing_start_idx == 0: swing_start_idx = T // 2 # Default if still not found

    swing_end_idx = T - 1

    if swing_start_idx >= swing_end_idx and T > 0: # If swing phase is too short or invalid
        print(f"⚠️ Swing phase calculation resulted in start ({swing_start_idx}) >= end ({swing_end_idx}). Defaulting rules to False.")
        return result.copy() # Return dict with all False

    # --- Rule Evaluation (NaN-aware) ---

    # 1. Shoulder Abduction (引拍期 - Preparation Phase)
    prep_phase_angles_shoulder = [get_joint_angle_at_frame(processed_keypoints, f'{D}Hip', f'{D}Shoulder', f'{D}Elbow', t) for t in range(swing_start_idx)]
    prep_phase_angles_shoulder_np = np.array(prep_phase_angles_shoulder)
    valid_shoulder_angles = prep_phase_angles_shoulder_np[~np.isnan(prep_phase_angles_shoulder_np)]
    if valid_shoulder_angles.size > 0:
        result['shoulder_abduction'] = np.any((valid_shoulder_angles >= 60) & (valid_shoulder_angles <= 90))

    # 2. Elbow Flexion (引拍期)
    prep_phase_angles_elbow = [get_joint_angle_at_frame(processed_keypoints, f'{D}Shoulder', f'{D}Elbow', f'{D}Wrist', t) for t in range(swing_start_idx)]
    prep_phase_angles_elbow_np = np.array(prep_phase_angles_elbow)
    valid_elbow_angles = prep_phase_angles_elbow_np[~np.isnan(prep_phase_angles_elbow_np)]
    if valid_elbow_angles.size > 0:
        result['elbow_flexion'] = np.any(valid_elbow_angles < 90)

    # 3. Elbow Lower (引拍期)
    dom_elbow_y = processed_keypoints[f'{D}Elbow'][:swing_start_idx, 1]
    nondom_elbow_y = processed_keypoints[f'{ND}Elbow'][:swing_start_idx, 1]
    valid_mask = ~np.isnan(dom_elbow_y) & ~np.isnan(nondom_elbow_y)
    if np.any(valid_mask):
        result['elbow_lower'] = np.any(dom_elbow_y[valid_mask] > nondom_elbow_y[valid_mask]) # In image coords, larger Y is lower

    # 4. Foot Direction Aligned (引拍期)
    foot_angles_prep = []
    for t in range(swing_start_idx):
        v_r = processed_keypoints['RightToe'][t] - processed_keypoints['RightHeel'][t]
        v_l = processed_keypoints['LeftToe'][t] - processed_keypoints['LeftHeel'][t]
        foot_angles_prep.append(angle_between(v_r, v_l))
    foot_angles_prep_np = np.array(foot_angles_prep)
    valid_foot_angles = foot_angles_prep_np[~np.isnan(foot_angles_prep_np)]
    if valid_foot_angles.size > 0:
        result['foot_direction_aligned'] = np.any(valid_foot_angles < 30)

    # --- Swinging Phase Rules ---
    def get_full_angle_series(kp_dict, joint_A, joint_B, joint_C):
        return np.array([get_joint_angle_at_frame(kp_dict, joint_A, joint_B, joint_C, t) for t in range(T)])

    swing_shoulder_angle_seq = get_full_angle_series(processed_keypoints, f'{D}Hip', f'{D}Shoulder', f'{D}Elbow')
    swing_elbow_angle_seq = get_full_angle_series(processed_keypoints, f'{D}Shoulder', f'{D}Elbow', f'{D}Wrist')
    
    # Use optional hand key if available and valid, otherwise default to Wrist (making it a zero-length segment for angle calc)
    hand_key_for_wrist_angle = optional_hand_key if optional_hand_key in processed_keypoints else f'{D}Wrist'
    swing_wrist_angle_seq = get_full_angle_series(processed_keypoints, f'{D}Elbow', f'{D}Wrist', hand_key_for_wrist_angle)

    # 5. Proximal to Distal Sequence (挥拍期)
    # Calculate angular velocities (absolute change in angle per frame)
    # Prepend with NaN to keep array length T for easier indexing with swing_start_idx
    vel_shoulder_angle = np.abs(np.diff(swing_shoulder_angle_seq, prepend=np.nan))
    vel_elbow_angle = np.abs(np.diff(swing_elbow_angle_seq, prepend=np.nan))
    # vel_wrist_angle = np.abs(np.diff(swing_wrist_angle_seq, prepend=np.nan)) # Wrist part currently excluded

    swing_phase_slice = slice(swing_start_idx, swing_end_idx + 1)
    
    # Find time of peak angular velocity for shoulder and elbow in the swing phase
    # Ensure there are non-NaN values in the slice before calling nanargmax
    t_s_peak_vel = swing_start_idx # Default
    if vel_shoulder_angle[swing_phase_slice].size > 0 and not np.all(np.isnan(vel_shoulder_angle[swing_phase_slice])):
        t_s_peak_vel = np.nanargmax(vel_shoulder_angle[swing_phase_slice]) + swing_start_idx
        
    t_e_peak_vel = swing_start_idx # Default
    if vel_elbow_angle[swing_phase_slice].size > 0 and not np.all(np.isnan(vel_elbow_angle[swing_phase_slice])):
        t_e_peak_vel = np.nanargmax(vel_elbow_angle[swing_phase_slice]) + swing_start_idx
    
    result['proximal_to_distal_sequence'] = t_s_peak_vel <= t_e_peak_vel

    # 6. Hip Forward Shift (挥拍期)
    dom_hip_x = processed_keypoints[f'{D}Hip'][:, 0]
    nondom_hip_x = processed_keypoints[f'{ND}Hip'][:, 0]
    valid_hip_x_swing_mask = ~np.isnan(dom_hip_x[swing_phase_slice]) & ~np.isnan(nondom_hip_x[swing_phase_slice])
    
    if np.any(valid_hip_x_swing_mask):
        hip_center_x_swing = 0.5 * (dom_hip_x[swing_phase_slice][valid_hip_x_swing_mask] + nondom_hip_x[swing_phase_slice][valid_hip_x_swing_mask])
        if hip_center_x_swing.size > 0:
            hip_range = np.max(hip_center_x_swing) - np.min(hip_center_x_swing)
            # Assuming normalized coordinates (0-1). Threshold might need adjustment if coords are pixels.
            result['hip_forward_shift'] = hip_range >= 0.08 
            
    # 7. Trunk Rotation Completed (挥拍期)
    dom_shoulder_pts_swing = processed_keypoints[f'{D}Shoulder'][swing_phase_slice]
    nondom_shoulder_pts_swing = processed_keypoints[f'{ND}Shoulder'][swing_phase_slice]

    dist_shoulders_init_frame = processed_keypoints[f'{D}Shoulder'][swing_start_idx] - processed_keypoints[f'{ND}Shoulder'][swing_start_idx]
    if not np.isnan(dist_shoulders_init_frame).any():
        dist_init = np.linalg.norm(dist_shoulders_init_frame)
        
        min_dist_swing = float('inf')
        found_valid_dist_in_swing = False
        for t_swing_local in range(len(dom_shoulder_pts_swing)): # Iterate over local indices of swing phase slice
            d_s_pt = dom_shoulder_pts_swing[t_swing_local]
            nd_s_pt = nondom_shoulder_pts_swing[t_swing_local]
            if not np.isnan(d_s_pt).any() and not np.isnan(nd_s_pt).any():
                dist = np.linalg.norm(d_s_pt - nd_s_pt)
                min_dist_swing = min(min_dist_swing, dist)
                found_valid_dist_in_swing = True
        
        if found_valid_dist_in_swing and dist_init > 1e-6 : # Avoid division by zero
            result['trunk_rotation_completed'] = (dist_init - min_dist_swing) / dist_init >= 0.4
            
    return result


def align_keypoints_with_interpolation(
    joint_data_raw_lists: Dict[str, List[List[float]]], 
    frame_count: int
) -> Dict[str, np.ndarray]:
    """
    Aligns keypoints from lists of detected coordinates, using linear interpolation for missing frames.
    Handles 3D data (expects lists of [x,y,z]).
    
    Input: 
        joint_data_raw_lists: {'joint_name': [[x,y,z], [x,y,z], ...], ...}
                              where each inner list contains coordinates for frames where the joint was DETECTED.
                              The length of these inner lists can vary per joint.
    Output: 
        {'joint_name': np.ndarray of shape (frame_count, 3), ...}
        Array will contain NaNs for joints with no detections or frames that couldn't be interpolated.
    """
    keypoints_aligned: Dict[str, np.ndarray] = {}
    num_dimensions = 3  # X, Y, Z

    all_joint_names = list(joint_data_raw_lists.keys())

    for name in all_joint_names:
        points_for_joint = joint_data_raw_lists.get(name, [])
        
        # Convert list of detected points to a NumPy array.
        # If points_for_joint is empty, pts_detected_np will be empty.
        pts_detected_np = np.array(points_for_joint, dtype=float) # Shape (num_detections, 3)
        num_detections = pts_detected_np.shape[0]

        # Initialize the full_frames_data array for this joint with NaNs.
        full_frames_data = np.full((frame_count, num_dimensions), np.nan)

        if num_detections == 0:
            # No detections for this joint across all frames.
            print(f"INFO: No detections for joint '{name}'. It will be all NaNs for {frame_count} frames.")
            keypoints_aligned[name] = full_frames_data # Already all NaNs
            continue
        
        if num_detections == frame_count and not np.isnan(pts_detected_np).all():
            # All frames have detections for this joint (ideal case, or already interpolated).
            keypoints_aligned[name] = pts_detected_np
            continue

        # Interpolation logic:
        # We need to know at which of the `frame_count` frames these `num_detections` occurred.
        # The current input `joint_data_raw_lists` doesn't explicitly provide the original frame indices
        # for each detection. It's just a list of detected coordinates.
        # This implies an assumption: the `k`-th item in `points_for_joint` corresponds to the `k`-th
        # frame *in which this joint was visible*, not necessarily the `k`-th frame of the video.
        
        # To perform meaningful interpolation, we need a mapping from these sparse detections
        # to the global `frame_count`.
        #
        # Assumption made by the original `np.linspace` approach:
        # The `num_detections` are spread somewhat evenly across `frame_count`.
        # `detected_at_video_frames`: indices in the original video (0 to frame_count-1) where this joint was detected.
        
        # If `joint_data_raw_lists` actually means "for each frame, if joint detected, here's its coord, else it's missing from this frame's data"
        # then the server-side processing loop needs to change to build `joint_data_raw_lists` differently.
        # E.g. `joint_data_raw_lists[joint_name]` would be a list of length `frame_count`, with `None` or `[nan,nan,nan]` for missing.
        
        # Given the current structure of `joint_data_raw_lists` (list of detections, length != frame_count):
        # We'll use the `np.linspace` strategy to map these `num_detections` to `frame_count` indices.
        # This is a heuristic if original frame indices are lost.
        
        # Indices in the `full_frames_data` array where we have known (detected) data.
        # These are the "xp" for np.interp.
        known_data_indices = np.linspace(0, frame_count - 1, num=num_detections, dtype=int)
        
        # Remove duplicate indices from known_data_indices that might arise from linspace if num_detections is large
        # and map to the same integer frame index. We take the point corresponding to the first occurrence.
        unique_known_indices, first_occurrence_idx = np.unique(known_data_indices, return_index=True)
        
        # The actual detected points corresponding to these unique known_data_indices. These are "fp" for np.interp.
        points_at_unique_known_indices = pts_detected_np[first_occurrence_idx]

        for dim_idx in range(num_dimensions):
            # Values for the current dimension at the unique known indices
            dim_values_at_known = points_at_unique_known_indices[:, dim_idx]
            
            # Filter out any NaNs that might be in the detected data itself for this dimension
            valid_dim_mask = ~np.isnan(dim_values_at_known)
            
            if np.sum(valid_dim_mask) < 1: # No valid data points for this dimension for this joint
                # print(f"DEBUG: Joint '{name}', Dim {dim_idx}: No valid detected points. Stays NaN.")
                continue # full_frames_data for this dim remains NaN
            
            if np.sum(valid_dim_mask) == 1: # Only one valid data point
                # print(f"DEBUG: Joint '{name}', Dim {dim_idx}: Only one valid point. Filling all frames with it.")
                full_frames_data[:, dim_idx] = dim_values_at_known[valid_dim_mask][0]
                continue

            # At least two valid points exist for this dimension, proceed with interpolation
            interp_xp = unique_known_indices[valid_dim_mask]
            interp_fp = dim_values_at_known[valid_dim_mask]
            
            # Ensure interp_xp is sorted (should be by np.unique) and fp corresponds
            # np.interp requires xp to be increasing.
            sort_indices = np.argsort(interp_xp)
            interp_xp_sorted = interp_xp[sort_indices]
            interp_fp_sorted = interp_fp[sort_indices]
            
            # Final check for unique xp after sorting (in case of issues)
            final_unique_xp, final_unique_idx = np.unique(interp_xp_sorted, return_index=True)
            final_fp = interp_fp_sorted[final_unique_idx]

            if len(final_unique_xp) < 2: # Still not enough unique points after all filtering
                 if len(final_unique_xp) == 1:
                     full_frames_data[:, dim_idx] = final_fp[0]
                 # else: stays NaN
                 continue
            
            full_frames_data[:, dim_idx] = np.interp(
                np.arange(frame_count), # x-coordinates to evaluate interpolation at (all frames)
                final_unique_xp,        # xp: x-coordinates of known data points (must be increasing)
                final_fp                # fp: y-coordinates of known data points
            )
        keypoints_aligned[name] = full_frames_data
        
    return keypoints_aligned
</file>

<file path="MoveInsight/Assets.xcassets/AppIcon.appiconset/Contents.json">
{
  "images" : [
    {
      "filename" : "d938011a54572aa285aa038b0f9eea3043b99b8ea3d567c2b9f568038a243413.png",
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    },
    {
      "appearances" : [
        {
          "appearance" : "luminosity",
          "value" : "dark"
        }
      ],
      "filename" : "d938011a54572aa285aa038b0f9eea3043b99b8ea3d567c2b9f568038a243413 1.png",
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    },
    {
      "appearances" : [
        {
          "appearance" : "luminosity",
          "value" : "tinted"
        }
      ],
      "filename" : "d938011a54572aa285aa038b0f9eea3043b99b8ea3d567c2b9f568038a243413 2.png",
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}
</file>

<file path="MoveInsight/en.lproj/Localizable.strings">
// Main navigation
"Home" = "Home";
"Training" = "Training";
"Videos" = "Videos";
"Messages" = "Messages";

// Upload functionality
"Upload Video" = "Upload Video";
"Uploading..." = "Uploading...";
"Close" = "Close";
"Upload Match Video" = "Upload Match Video";
"Upload Training Video" = "Upload Training Video";
"Match Video" = "Match Video";
"Training Video" = "Training Video";
"Loading Video..." = "Loading Video...";
"Preparing video..." = "Preparing video...";

// Permissions
"Photo Library Access Required" = "Photo Library Access Required";
"Permission to access your photo library is required to upload videos. Please grant access in Settings." = "Permission to access your photo library is required to upload videos. Please grant access in Settings.";
"Go to Settings" = "Go to Settings";
"Cancel" = "Cancel";

// Home View
"Good morning," = "Good morning,";
"Match Performance" = "Match Performance";
"/ last week" = "/ last week";
"Well done on swing path!" = "Well done on swing path!";
"Match History" = "Match History";
"Technicals" = "Technicals";
"Training Goals" = "Training Goals";
"Tutorials Specifically For You" = "Tutorials Specifically For You";

// Chart months
"Jan" = "Jan";
"Feb" = "Feb";
"Mar" = "Mar";
"Apr" = "Apr";
"May" = "May";

// Placeholder screens
"Training Screen" = "Training Screen";
"Upload Screen" = "Upload Screen";
"Videos Screen" = "Videos Screen";
"Messages Screen" = "Messages Screen";

// Numeric values (formats can be localized)
"2.3%" = "2.3%";
"4.3" = "4.3";

// Details screens
"Technicals Detail" = "Technicals Detail";
"Training Goals Detail" = "Training Goals Detail";

// Techniques List View
"Badminton Techniques" = "Badminton Techniques";
"Select a technique to upload and analyze your form" = "Select a technique to upload and analyze your form";
"Techniques" = "Techniques";

// Technique Names
"Backhand Clear" = "Backhand Clear";
"Underhand Clear" = "Underhand Clear";
"Overhead Clear" = "Overhead Clear";
"Drop Shot" = "Drop Shot";
"Smash" = "Smash";
"Net Shot" = "Net Shot";

// Technique Descriptions
"A clear shot played with the back of the hand facing forward." = "A clear shot played with the back of the hand facing forward.";
"A defensive shot played from below waist height, sending the shuttle high to the back of the opponent's court." = "A defensive shot played from below waist height, sending the shuttle high to the back of the opponent's court.";
"A powerful shot played from above the head, sending the shuttle to the back of the opponent's court." = "A powerful shot played from above the head, sending the shuttle to the back of the opponent's court.";
"A gentle shot that just clears the net and drops sharply on the other side." = "A gentle shot that just clears the net and drops sharply on the other side.";
"A powerful overhead shot hit steeply downward into the opponent's court." = "A powerful overhead shot hit steeply downward into the opponent's court.";
"A soft shot played near the net that just clears it and falls close to the net on the other side." = "A soft shot played near the net that just clears it and falls close to the net on the other side.";

// Technique Detail View
"Upload Your %@ Video" = "Upload Your %@ Video";
"Upload Second Video for Comparison" = "Upload Second Video for Comparison";
"Your Analyzed Video" = "Your Analyzed Video";
"Next Steps" = "Next Steps";
"Compare with Model Video" = "Compare with Model Video";
"Re-upload Primary Video" = "Re-upload Primary Video";
"Key Points for %@" = "Key Points for %@";
"Start with proper stance, feet shoulder-width apart." = "Start with proper stance, feet shoulder-width apart.";
"Grip the racket with a relaxed, comfortable hold." = "Grip the racket with a relaxed, comfortable hold.";
"Maintain balance throughout the motion." = "Maintain balance throughout the motion.";
"Keep your eye on the shuttle at all times." = "Keep your eye on the shuttle at all times.";
"Follow through with your swing for better control." = "Follow through with your swing for better control.";
"Processing video..." = "Processing video...";
"Uploading & Analyzing Primary Video..." = "Uploading & Analyzing Primary Video...";
"Uploading & Analyzing Comparison Video..." = "Uploading & Analyzing Comparison Video...";
"Processing Model Video..." = "Processing Model Video...";
"Error" = "Error";
"Try Upload Again" = "Try Upload Again";

// Technique Video Upload View
"Upload Comparison Video" = "Upload Comparison Video";
"Upload %@ Video" = "Upload %@ Video";
"Select a video of yourself performing the %@ technique." = "Select a video of yourself performing the %@ technique.";
"Select Video from Library" = "Select Video from Library";
"For best results, ensure your entire body is visible, and you are performing the technique from start to finish." = "For best results, ensure your entire body is visible, and you are performing the technique from start to finish.";

// Technique Comparison View
"%@ Analysis" = "%@ Analysis";
"Side by Side" = "Side by Side";
"3D Overlay" = "3D Overlay";
"Overview" = "Overview";
"Technical" = "Technical";
"Your Technique" = "Your Technique";
"Model Technique" = "Model Technique";
"Technique Analysis" = "Technique Analysis";
"3D Overlay (Conceptual)" = "3D Overlay (Conceptual)";
"Note: 3D rendering is currently simplified." = "Note: 3D rendering is currently simplified.";
"Analysis & Feedback" = "Analysis & Feedback";
"Analyzing your technique..." = "Analyzing your technique...";
"Overall Technique Score" = "Overall Technique Score";
"Compared to model performance" = "Compared to model performance";
"Technical Report" = "Technical Report";
"Loading technical report..." = "Loading technical report...";
"Scores: You vs. Model" = "Scores: You vs. Model";
"Your Score" = "Your Score";
"Model Score" = "Model Score";
"Technical Elements Breakdown" = "Technical Elements Breakdown";
"Element" = "Element";
"You" = "You";
"Model" = "Model";
"Improvement Suggestions" = "Improvement Suggestions";
"Excellent! All key technical elements are performed correctly." = "Excellent! All key technical elements are performed correctly.";
"Key Technique Elements" = "Key Technique Elements";
"Analysis data not available. Please ensure the video was processed." = "Analysis data not available. Please ensure the video was processed.";
"Technical analysis data not available." = "Technical analysis data not available.";
"Refresh Analysis" = "Refresh Analysis";
"Analysis Error" = "Analysis Error";
"Retry" = "Retry";

// String formatting for technique analysis
"%@: Well done!" = "%@: Well done!";
"%@: Focus on improving this aspect. Check tutorials for guidance." = "%@: Focus on improving this aspect. Check tutorials for guidance.";

// Upload Tab View
"Choose the type of video you want to upload" = "Choose the type of video you want to upload";
"Upload Technique Video" = "Upload Technique Video";
"Analyze and compare your badminton techniques with model performers" = "Analyze and compare your badminton techniques with model performers";
"Upload your match videos for performance analysis" = "Upload your match videos for performance analysis";

// Technical strings - converting for proper display
"shoulder_abduction" = "Shoulder Abduction";
"elbow_flexion" = "Elbow Flexion";
"elbow_lower" = "Elbow Position";
"foot_direction_aligned" = "Foot Direction";
"proximal_to_distal_sequence" = "Movement Sequence";
"hip_forward_shift" = "Hip Movement";
"trunk_rotation_completed" = "Trunk Rotation";
</file>

<file path="MoveInsight/zh-Hans.lproj/Localizable.strings">
// Main navigation
"Home" = "首页";
"Training" = "训练";
"Videos" = "视频";
"Messages" = "消息";

// Upload functionality
"Upload Video" = "上传视频";
"Uploading..." = "上传中...";
"Close" = "关闭";
"Upload Match Video" = "上传比赛视频";
"Upload Training Video" = "上传训练视频";
"Match Video" = "比赛视频";
"Training Video" = "训练视频";
"Loading Video..." = "加载视频中...";
"Preparing video..." = "准备视频中...";

// Permissions
"Photo Library Access Required" = "需要访问照片库";
"Permission to access your photo library is required to upload videos. Please grant access in Settings." = "上传视频需要访问您的照片库。请在设置中授予权限。";
"Go to Settings" = "前往设置";
"Cancel" = "取消";

// Home View
"Good morning," = "早上好，";
"Match Performance" = "比赛表现";
"/ last week" = "/ 上周";
"Well done on swing path!" = "挥杆轨迹很棒！";
"Match History" = "比赛历史";
"Technicals" = "技术分析";
"Training Goals" = "训练目标";
"Tutorials Specifically For You" = "专为您定制的教程";

// Chart months
"Jan" = "一月";
"Feb" = "二月";
"Mar" = "三月";
"Apr" = "四月";
"May" = "五月";

// Placeholder screens
"Training Screen" = "训练界面";
"Upload Screen" = "上传界面";
"Videos Screen" = "视频界面";
"Messages Screen" = "消息界面";

// Numeric values (formats can be localized)
"2.3%" = "2.3%";
"4.3" = "4.3";

// Details screens
"Technicals Detail" = "技术分析详情";
"Training Goals Detail" = "训练目标详情";

// Techniques List View
"Badminton Techniques" = "羽毛球技术";
"Select a technique to upload and analyze your form" = "选择一项技术上传并分析您的表现";
"Techniques" = "技术";

// Technique Names
"Backhand Clear" = "反手高远球";
"Underhand Clear" = "正手高远球";
"Overhead Clear" = "头顶高远球";
"Drop Shot" = "吊球";
"Smash" = "杀球";
"Net Shot" = "网前球";

// Technique Descriptions
"A clear shot played with the back of the hand facing forward." = "一种以手背朝前的方式击打的高远球。";
"A defensive shot played from below waist height, sending the shuttle high to the back of the opponent's court." = "一种从腰部以下高度击打的防守球，将球高高地送到对方场地后方。";
"A powerful shot played from above the head, sending the shuttle to the back of the opponent's court." = "一种从头顶上方击打的有力球，将球送到对方场地后方。";
"A gentle shot that just clears the net and drops sharply on the other side." = "一种轻柔地刚好越过网，在对方场地陡然下落的球。";
"A powerful overhead shot hit steeply downward into the opponent's court." = "一种从高处陡然向下击打进入对方场地的有力球。";
"A soft shot played near the net that just clears it and falls close to the net on the other side." = "一种在网前轻柔地击打，刚好越过网并在对方网前落下的球。";

// Technique Detail View
"Upload Your %@ Video" = "上传您的%@视频";
"Upload Second Video for Comparison" = "上传第二个视频进行比较";
"Your Analyzed Video" = "您的分析视频";
"Next Steps" = "下一步";
"Compare with Model Video" = "与模范视频比较";
"Re-upload Primary Video" = "重新上传主视频";
"Key Points for %@" = "%@的关键点";
"Start with proper stance, feet shoulder-width apart." = "以正确的姿势开始，双脚与肩同宽。";
"Grip the racket with a relaxed, comfortable hold." = "以放松、舒适的方式握拍。";
"Maintain balance throughout the motion." = "在整个动作过程中保持平衡。";
"Keep your eye on the shuttle at all times." = "始终保持眼睛注视球。";
"Follow through with your swing for better control." = "跟随挥拍动作以获得更好的控制。";
"Processing video..." = "处理视频中...";
"Uploading & Analyzing Primary Video..." = "上传并分析主视频中...";
"Uploading & Analyzing Comparison Video..." = "上传并分析比较视频中...";
"Processing Model Video..." = "处理模范视频中...";
"Error" = "错误";
"Try Upload Again" = "重新尝试上传";

// Technique Video Upload View
"Upload Comparison Video" = "上传比较视频";
"Upload %@ Video" = "上传%@视频";
"Select a video of yourself performing the %@ technique." = "选择一个您自己执行%@技术的视频。";
"Select Video from Library" = "从相册中选择视频";
"For best results, ensure your entire body is visible, and you are performing the technique from start to finish." = "为了获得最佳效果，请确保您的整个身体可见，并且从头到尾完成技术动作。";

// Technique Comparison View
"%@ Analysis" = "%@分析";
"Side by Side" = "并排对比";
"3D Overlay" = "3D叠加";
"Overview" = "分析总览";
"Technical" = "技术分析";
"Your Technique" = "您的技术";
"Model Technique" = "模范技术";
"Technique Analysis" = "技术分析";
"3D Overlay (Conceptual)" = "3D叠加（概念性）";
"Note: 3D rendering is currently simplified." = "注意：3D渲染目前已简化。";
"Analysis & Feedback" = "分析与反馈";
"Analyzing your technique..." = "分析您的技术中...";
"Overall Technique Score" = "整体技术得分";
"Compared to model performance" = "与模范表现相比";
"Technical Report" = "技术报告";
"Loading technical report..." = "加载技术报告中...";
"Scores: You vs. Model" = "得分：您 vs 模范";
"Your Score" = "您的得分";
"Model Score" = "模范得分";
"Technical Elements Breakdown" = "技术元素分解";
"Element" = "元素";
"You" = "您";
"Model" = "模范";
"Improvement Suggestions" = "改进建议";
"Excellent! All key technical elements are performed correctly." = "太棒了！所有关键技术元素的执行都是正确的。";
"Key Technique Elements" = "关键技术元素";
"Analysis data not available. Please ensure the video was processed." = "分析数据不可用。请确保视频已处理。";
"Technical analysis data not available." = "技术分析数据不可用。";
"Refresh Analysis" = "刷新分析";
"Analysis Error" = "分析错误";
"Retry" = "重试";

// String formatting for technique analysis
"%@: Well done!" = "%@：做得好！";
"%@: Focus on improving this aspect. Check tutorials for guidance." = "%@：专注于改进这一方面。查看教程获取指导。";

// Upload Tab View
"Choose the type of video you want to upload" = "选择您要上传的视频类型";
"Upload Technique Video" = "上传技术视频";
"Analyze and compare your badminton techniques with model performers" = "分析并比较您的羽毛球技术与模范表演者";
"Upload your match videos for performance analysis" = "上传您的比赛视频进行表现分析";

// Technical strings - converting for proper display
"shoulder_abduction" = "肩部外展";
"elbow_flexion" = "肘部弯曲";
"elbow_lower" = "肘部位置";
"foot_direction_aligned" = "脚部方向";
"proximal_to_distal_sequence" = "运动顺序";
"hip_forward_shift" = "髋部移动";
"trunk_rotation_completed" = "躯干旋转";
</file>

<file path="MoveInsight/CombinedVideo3DView.swift">
// MoveInsight/CombinedVideo3DView.swift
import SwiftUI
import AVKit
import Combine
import simd // Required for SIMD3 if PoseOverlayViewForComparison uses it via ViewModel

// MARK: - Pose Overlay View for Comparison (Adapted for 3D Poses, Renders 2D Projection)
// This view is used by CombinedVideo2DComparisonView to display 2D projections of 3D poses.
struct PoseOverlayViewForComparison: View {
    @ObservedObject var viewModel: VideoPlayerViewModel // viewModel.poses is now [String: SIMD3<Float>]
    let videoActualRect: CGRect // The actual rectangle of the video content within its player view

    var body: some View {
        Canvas { context, size in
            // Ensure poses are available and the video rectangle is valid.
            guard !viewModel.poses.isEmpty,
                  videoActualRect.size.width > 0,
                  videoActualRect.size.height > 0 else {
                // print("PoseOverlayViewForComparison: No poses or invalid videoActualRect \(videoActualRect)")
                return
            }
            let currentPoses3D = viewModel.poses // These are [String: SIMD3<Float>]

            // Draw connections using the x and y components of the 3D poses.
            for connection in viewModel.bodyConnections { // bodyConnections are StringBodyConnection for 2D
                guard let fromPose3D = currentPoses3D[connection.from],
                      let toPose3D = currentPoses3D[connection.to] else {
                    // print("PoseOverlayViewForComparison: Missing joint for connection \(connection.from) -> \(connection.to)")
                    continue
                }
                
                // Extract 2D points (normalized x, y) from the 3D pose data.
                let fromPointNorm = CGPoint(x: CGFloat(fromPose3D.x), y: CGFloat(fromPose3D.y))
                let toPointNorm = CGPoint(x: CGFloat(toPose3D.x), y: CGFloat(toPose3D.y))
                
                // Scale these normalized points to the actual video rectangle's coordinate system.
                let fromCanvasPoint = CGPoint(
                    x: videoActualRect.origin.x + (fromPointNorm.x * videoActualRect.size.width),
                    y: videoActualRect.origin.y + (fromPointNorm.y * videoActualRect.size.height)
                )
                let toCanvasPoint = CGPoint(
                    x: videoActualRect.origin.x + (toPointNorm.x * videoActualRect.size.width),
                    y: videoActualRect.origin.y + (toPointNorm.y * videoActualRect.size.height)
                )
                
                var path = Path()
                path.move(to: fromCanvasPoint)
                path.addLine(to: toCanvasPoint)
                context.stroke(path, with: .color(ColorManager.accentColor.opacity(0.8)), lineWidth: 3)
            }

            // Draw joints using the x and y components.
            for (_, jointPose3D) in currentPoses3D {
                let jointPointNorm = CGPoint(x: CGFloat(jointPose3D.x), y: CGFloat(jointPose3D.y))
                let jointCanvasPoint = CGPoint(
                    x: videoActualRect.origin.x + (jointPointNorm.x * videoActualRect.size.width),
                    y: videoActualRect.origin.y + (jointPointNorm.y * videoActualRect.size.height)
                )
                let rect = CGRect(x: jointCanvasPoint.x - 4, y: jointCanvasPoint.y - 4, width: 8, height: 8)
                context.fill(Path(ellipseIn: rect), with: .color(Color.red.opacity(0.8)))
            }
        }
        .opacity(0.7)
    }
}


// MARK: - Combined 2D Video Comparison View
// This view displays two video players side-by-side, each with a 2D pose overlay.
// It's used when a simple 2D side-by-side comparison is desired, even with 3D data.
// The "3D Overlay" mode in TechniqueComparisonView will use SceneView3D directly.
struct CombinedVideo2DComparisonView: View {
    @ObservedObject var primaryViewModel: VideoPlayerViewModel   // ViewModel now handles 3D poses
    @ObservedObject var secondaryViewModel: VideoPlayerViewModel // ViewModel now handles 3D poses
    
    // State for synchronized playback control
    @State private var currentTime: Double = 0
    @State private var isPlayingUserInitiated: Bool = false // Tracks if user explicitly hit play
    @State private var duration: Double = 0.1 // Default duration to prevent division by zero
    @State private var isLoadingDuration: Bool = true // To show loading state for duration

    // State variables to hold the actual video rectangles for each player's 2D overlay.
    @State private var primaryVideoActualRect: CGRect = .zero
    @State private var secondaryVideoActualRect: CGRect = .zero

    @State private var timeObserverToken: Any? = nil
    private let frameStep: Double = 1.0 / 30.0 // Assuming 30fps for step controls

    var body: some View {
        ZStack {
            ColorManager.background.ignoresSafeArea() // Ensure background color is applied

            VStack(spacing: 0) { // Use 0 spacing for tighter layout if desired
                if primaryViewModel.isVideoReady && secondaryViewModel.isVideoReady && !isLoadingDuration {
                    HStack(spacing: 8) { // Spacing between the two video players
                        // Primary video player with its overlay
                        videoPlayerWithOverlay(
                            for: primaryViewModel,
                            title: "Your Video", // Or LocalizedStringKey("Your Technique")
                            videoActualRectBinding: $primaryVideoActualRect
                        )
                        // Secondary video player with its overlay
                        videoPlayerWithOverlay(
                            for: secondaryViewModel,
                            title: "Comparison Video", // Or LocalizedStringKey("Model Technique")
                            videoActualRectBinding: $secondaryVideoActualRect
                        )
                    }
                    .padding(.horizontal, 8) // Padding around the HStack of players
                    .padding(.top, 8)
                    
                    controlBar // Playback controls
                        .padding(.vertical, 10)

                } else {
                    // Loading indicator while videos or durations are loading
                    ProgressView(isLoadingDuration ? "Loading video durations..." : "Loading Videos for Comparison...")
                        .tint(ColorManager.accentColor)
                        .frame(maxWidth: .infinity, maxHeight: .infinity) // Center the progress view
                }
            }
        }
        .onAppear {
            isLoadingDuration = true
            // Asynchronously load video durations
            Task {
                do {
                    let primaryDurationSeconds = try await primaryViewModel.asset.load(.duration).seconds
                    let secondaryDurationSeconds = try await secondaryViewModel.asset.load(.duration).seconds
                    await MainActor.run {
                        self.duration = max(primaryDurationSeconds, secondaryDurationSeconds, 0.1) // Ensure duration is at least 0.1
                        self.isLoadingDuration = false
                    }
                } catch {
                    await MainActor.run {
                        print("Error loading video durations for CombinedVideo2DComparisonView: \(error)")
                        self.duration = 0.1 // Fallback duration
                        self.isLoadingDuration = false
                    }
                }
            }
            
            setupTimeObserver() // Setup observer for synchronized playback
            primaryViewModel.player.isMuted = false // User's video is unmuted
            secondaryViewModel.player.isMuted = true // Comparison video is muted
            
            // If either video was already playing (e.g., from a previous view state)
            if primaryViewModel.isPlaying || secondaryViewModel.isPlaying {
                isPlayingUserInitiated = true // Reflect that playback was intended
                primaryViewModel.play()
                secondaryViewModel.play()
            }
        }
        .onDisappear {
            removeTimeObserver() // Clean up time observer
            primaryViewModel.pause() // Pause videos when view disappears
            secondaryViewModel.pause()
        }
    }
    
    // Helper view for a single video player card with its 2D overlay.
    @ViewBuilder
    private func videoPlayerWithOverlay(
        for viewModel: VideoPlayerViewModel,
        title: String, // Or LocalizedStringKey
        videoActualRectBinding: Binding<CGRect> // Binding for this player's video rect
    ) -> some View {
        VStack {
            Text(title)
                .font(.caption)
                .foregroundColor(ColorManager.textSecondary)
            
            // VideoPlayerRepresentable displays the video.
            // PoseOverlayViewForComparison displays the 2D projection of 3D poses.
            VideoPlayerRepresentable(player: viewModel.player, videoRect: videoActualRectBinding)
                .overlay(PoseOverlayViewForComparison(viewModel: viewModel, videoActualRect: videoActualRectBinding.wrappedValue))
                .aspectRatio(CGSize(width: 9, height: 16), contentMode: .fit) // Maintain aspect ratio
                .background(Color.black) // Black background for letter/pillarboxing
                .cornerRadius(8)
                .shadow(radius: 3)
        }
    }
    
    // Playback control bar (Slider, Play/Pause, Step buttons).
    // Implementation remains the same as it controls AVPlayer and UI state, not pose data directly.
    private var controlBar: some View { /* ... as before ... */
        HStack(spacing: 15) {
            Button(action: togglePlayPause) {
                Image(systemName: isPlayingUserInitiated && primaryViewModel.player.rate != 0 ? "pause.fill" : "play.fill")
                    .font(.title2)
            }

            Button(action: { step(by: -1) }) { Image(systemName: "backward.frame.fill").font(.title3) }
            
            Slider( value: $currentTime, in: 0...duration, onEditingChanged: sliderEditingChanged )
                .accentColor(ColorManager.accentColor)
                .disabled(duration <= 0.1 || isLoadingDuration)

            Button(action: { step(by: 1) }) { Image(systemName: "forward.frame.fill").font(.title3) }
            
            Text(String(format: "%.2f / %.2f", currentTime, duration))
                .font(.caption).foregroundColor(ColorManager.textSecondary).frame(minWidth: 80, alignment: .leading)
        }
        .padding(.horizontal).foregroundColor(ColorManager.accentColor)
    }

    private func sliderEditingChanged(editing: Bool) {
        if editing { // User started scrubbing
            if primaryViewModel.player.rate != 0 { primaryViewModel.pause() }
            if secondaryViewModel.player.rate != 0 { secondaryViewModel.pause() }
        } else { // User finished scrubbing
            seek(to: currentTime)
            if isPlayingUserInitiated { // Resume playback if it was playing before scrubbing
                primaryViewModel.play()
                secondaryViewModel.play()
            }
        }
    }

    private func togglePlayPause() { /* ... as before ... */
        isPlayingUserInitiated.toggle()
        if isPlayingUserInitiated { primaryViewModel.play(); secondaryViewModel.play() }
        else { primaryViewModel.pause(); secondaryViewModel.pause() }
    }

    private func step(by frames: Int) { /* ... as before ... */
        guard duration > 0.1 && !isLoadingDuration else { return }
        let newTime = max(0, min(duration, currentTime + Double(frames) * frameStep))
        seek(to: newTime) // Seek to the new time
        
        // If paused, briefly play/pause to update the visual frame for both players
        if !isPlayingUserInitiated {
            primaryViewModel.play(); secondaryViewModel.play()
            DispatchQueue.main.asyncAfter(deadline: .now() + 0.05) { // Short delay
                primaryViewModel.pause(); secondaryViewModel.pause()
                // Explicitly update currentTime here if slider doesn't update automatically when paused
                // self.currentTime = newTime
            }
        }
        // If already playing, seek will update the frame, and currentTime updates via timeObserver.
    }

    private func seek(to timeSec: Double) { /* ... as before ... */
        guard duration > 0.1 && !isLoadingDuration else { return }
        let cmTime = CMTime(seconds: timeSec, preferredTimescale: 600)
        let dispatchGroup = DispatchGroup()
        let wasPlaying = isPlayingUserInitiated && primaryViewModel.player.rate != 0

        // Pause before seeking for smoother experience
        if primaryViewModel.player.rate != 0 { primaryViewModel.pause() }
        if secondaryViewModel.player.rate != 0 { secondaryViewModel.pause() }

        dispatchGroup.enter()
        primaryViewModel.player.seek(to: cmTime, toleranceBefore: .zero, toleranceAfter: .zero) { _ in dispatchGroup.leave() }
        
        dispatchGroup.enter()
        secondaryViewModel.player.seek(to: cmTime, toleranceBefore: .zero, toleranceAfter: .zero) { _ in dispatchGroup.leave() }

        dispatchGroup.notify(queue: .main) {
            self.currentTime = timeSec // Ensure slider reflects the seeked time
            if wasPlaying { // Resume playback if it was playing before seek
                primaryViewModel.play(); secondaryViewModel.play()
            }
            // If paused, the VideoPlayerViewModel's displayLinkDidFire (if manually triggered on pause)
            // or the next play action would update the `poses` for the current frame.
        }
    }

    private func setupTimeObserver() { /* ... as before ... */
        guard timeObserverToken == nil else { return }
        // More frequent updates for smoother slider sync, e.g., half of frameStep
        let interval = CMTime(seconds: frameStep / 2.0, preferredTimescale: CMTimeScale(NSEC_PER_SEC))
        
        timeObserverToken = primaryViewModel.player.addPeriodicTimeObserver(forInterval: interval, queue: .main) { [self] time in
            if !duration.isZero && !isLoadingDuration {
                 self.currentTime = time.seconds
            }
        }
    }
    private func removeTimeObserver() { /* ... as before ... */
        if let token = timeObserverToken {
            primaryViewModel.player.removeTimeObserver(token)
            timeObserverToken = nil
        }
    }
}
</file>

<file path="MoveInsight/ContentView.swift">
import SwiftUI

struct ContentView: View {
    @State private var selectedTab = 0

    var body: some View {
        NavigationView {
            ZStack {
                // Main background
                ColorManager.background.ignoresSafeArea()

                // Content area based on selected tab
                VStack {
                    TabView(selection: $selectedTab) {
                        HomeView()
                            .tag(0)
                        
                        Text(LocalizedStringKey("Training Screen"))
                            .foregroundColor(ColorManager.textPrimary)
                            .tag(1)
                        
                        // Use the new UploadTabView directly in the Upload tab
                        UploadTabView()
                            .tag(2)
                        
                        Text(LocalizedStringKey("Videos Screen"))
                            .foregroundColor(ColorManager.textPrimary)
                            .tag(3)
                        
                        Text(LocalizedStringKey("Messages Screen"))
                            .foregroundColor(ColorManager.textPrimary)
                            .tag(4)
                    }
                    .tabViewStyle(PageTabViewStyle(indexDisplayMode: .never))
                    
                    // Custom tab bar now using the evenly-spaced implementation
                    CustomTabBar(selectedTab: $selectedTab)
                }
            }
            .navigationBarHidden(true)
        }
    }
}

// MARK: - Previews
struct ContentView_Previews: PreviewProvider {
    static var previews: some View {
        ContentView()
            .preferredColorScheme(.dark)
    }
}
</file>

<file path="MoveInsight/Extensions.swift">
import SwiftUI
import Combine

// MARK: - Publisher Extension
extension AnyCancellable {
    func cancel(after interval: TimeInterval) {
        DispatchQueue.main.asyncAfter(deadline: .now() + interval) {
            self.cancel()
        }
    }
}

// MARK: - View Extensions
extension View {
    // Apply a conditional modifier
    @ViewBuilder func applyIf<Content: View>(_ condition: Bool, content: (Self) -> Content) -> some View {
        if condition {
            content(self)
        } else {
            self
        }
    }
    
    // Add a shake effect to a view
    func shake(amount: CGFloat = 5, shakesPerUnit: CGFloat = 3, animationDuration: CGFloat = 0.7, isShaking: Bool = true) -> some View {
        self.modifier(ShakeEffect(amount: amount, shakesPerUnit: shakesPerUnit, animationDuration: animationDuration, isShaking: isShaking))
    }
}

// MARK: - Navigation Extensions
extension View {
    // Create a navigation link that's programmatically triggered
    func navigationLinkWithDestination<Destination: View>(isActive: Binding<Bool>, @ViewBuilder destination: @escaping () -> Destination) -> some View {
        ZStack {
            self
            
            NavigationLink(
                destination: destination(),
                isActive: isActive
            ) {
                EmptyView()
            }
            .hidden()
        }
    }
}

// MARK: - Shake Effect Modifier
struct ShakeEffect: ViewModifier {
    var amount: CGFloat = 5
    var shakesPerUnit: CGFloat = 3
    var animationDuration: CGFloat = 0.7
    var isShaking: Bool = true
    
    func body(content: Content) -> some View {
        content
            .offset(x: isShaking ? amount * sin(shakesPerUnit * .pi * animationDuration) : 0)
            .animation(
                isShaking ?
                    Animation.easeInOut(duration: animationDuration)
                    .repeatForever(autoreverses: true) :
                    .default,
                value: isShaking
            )
    }
}

// MARK: - Dynamic Height Modifier
struct DynamicHeightModifier: ViewModifier {
    @Binding var height: CGFloat
    
    func body(content: Content) -> some View {
        content
            .background(
                GeometryReader { geometry -> Color in
                    DispatchQueue.main.async {
                        self.height = geometry.size.height
                    }
                    return Color.clear
                }
            )
    }
}
</file>

<file path="MoveInsight/HomeView.swift">
import SwiftUI

struct HomeView: View {
    // Sample user data – name typically wouldn't be localized
    let username = "Zhang Wei"
    
    var body: some View {
        ZStack {
            // Main background
            ColorManager.background.ignoresSafeArea()
            
            // Content
            ScrollView {
                VStack(alignment: .leading, spacing: 24) {
                    // User greeting
                    HStack {
                        VStack(alignment: .leading) {
                            Text(LocalizedStringKey("Good morning,"))
                                .font(.system(size: 16))
                                .foregroundColor(ColorManager.textSecondary)
                            Text(username)
                                .font(.system(size: 24, weight: .bold))
                                .foregroundColor(ColorManager.textPrimary)
                        }
                        
                        Spacer()
                        
                        // Profile image
                        Image(systemName: "person.crop.circle.fill")
                            .resizable()
                            .frame(width: 40, height: 40)
                            .foregroundColor(ColorManager.accentColor)
                            .background(ColorManager.cardBackground)
                            .clipShape(Circle())
                    }
                    .padding(.top, 12)
                    
                    // Match Performance Card
                    PerformanceCard()
                    
                    // Technicals Section - Updated to navigate to TechniquesListView
                    NavigationLink(destination: TechniquesListView()) {
                        SectionCard(title: LocalizedStringKey("Technicals"))
                    }
                    
                    // Training Goals
                    NavigationLink(destination: Text(LocalizedStringKey("Training Goals Detail"))) {
                        GoalsCard()
                    }
                    
                    // Tutorials Section
                    Text(LocalizedStringKey("Tutorials Specifically For You"))
                        .font(.headline)
                        .foregroundColor(ColorManager.textPrimary)
                        .padding(.top, 5)
                }
                .padding(.horizontal, 16)
                .padding(.bottom, 20)
            }
        }
    }
}

// MARK: - Performance Card
struct PerformanceCard: View {
    var body: some View {
        VStack(alignment: .leading, spacing: 15) {
            HStack {
                Text(LocalizedStringKey("Match Performance"))
                    .font(.headline)
                    .foregroundColor(ColorManager.textPrimary)
                
                Spacer()
                
                Image(systemName: "arrow.up.right.square")
                    .foregroundColor(ColorManager.textSecondary)
            }
            
            HStack(alignment: .top, spacing: 15) {
                // Improvement stat
                VStack(alignment: .leading, spacing: 2) {
                    HStack {
                        Image(systemName: "chart.line.uptrend.xyaxis")
                            .foregroundColor(ColorManager.accentColor)
                        
                        Text(LocalizedStringKey("2.3%"))
                            .fontWeight(.bold)
                            .foregroundColor(ColorManager.textPrimary)
                    }
                    
                    Text(LocalizedStringKey("/ last week"))
                        .font(.caption)
                        .foregroundColor(ColorManager.textSecondary)
                    
                    Text(LocalizedStringKey("Well done on swing path!"))
                        .font(.caption)
                        .foregroundColor(ColorManager.textSecondary)
                        .padding(.top, 5)
                }
                
                Spacer()
                
                // Rating gauge using only purple with a gradient
                ZStack {
                    Circle()
                        .trim(from: 0, to: 0.75)
                        .stroke(
                            AngularGradient(
                                gradient: Gradient(colors: [ColorManager.accentColor.opacity(0.6), ColorManager.accentColor]),
                                center: .center,
                                startAngle: .degrees(0),
                                endAngle: .degrees(270)
                            ),
                            style: StrokeStyle(lineWidth: 8, lineCap: .round)
                        )
                        .frame(width: 80, height: 80)
                        .rotationEffect(.degrees(135))
                    
                    Text(LocalizedStringKey("4.3"))
                        .font(.system(size: 24, weight: .bold))
                        .foregroundColor(ColorManager.textPrimary)
                }
            }
            
            // Progress chart
            Chart()
                .frame(height: 120)
                .padding(.vertical, 8)
            
            // Match history link
            HStack {
                Text(LocalizedStringKey("Match History"))
                    .foregroundColor(ColorManager.textSecondary)
                    .font(.subheadline)
                
                Spacer()
                
                Image(systemName: "chevron.right")
                    .foregroundColor(ColorManager.textSecondary)
                    .font(.caption)
            }
        }
        .padding(20)
        .background(
            RoundedRectangle(cornerRadius: 16)
                .fill(ColorManager.cardBackground.opacity(0.8))
        )
    }
}

// MARK: - Simple Chart Component
struct Chart: View {
    let months = ["Jan", "Feb", "Mar", "Apr", "May"]
    
    var body: some View {
        GeometryReader { geometry in
            let width = geometry.size.width
            let height = geometry.size.height
            
            HStack(spacing: 0) {
                ForEach(months, id: \.self) { month in
                    Text(LocalizedStringKey(month))
                        .font(.system(size: 8))
                        .foregroundColor(ColorManager.textSecondary)
                        .frame(width: width / CGFloat(months.count))
                }
            }
            .position(x: width / 2, y: height - 5)
            
            VStack(spacing: 8) {
                ForEach(["4", "3", "2", "1", "0"], id: \.self) { value in
                    Text(value)
                        .font(.system(size: 8))
                        .foregroundColor(ColorManager.textSecondary)
                }
            }
            .position(x: 8, y: height / 2)
            
            Path { path in
                let points = [
                    CGPoint(x: width * 0.1, y: height * 0.7),
                    CGPoint(x: width * 0.3, y: height * 0.5),
                    CGPoint(x: width * 0.5, y: height * 0.4),
                    CGPoint(x: width * 0.7, y: height * 0.35),
                    CGPoint(x: width * 0.9, y: height * 0.3)
                ]
                
                path.move(to: points[0])
                for point in points.dropFirst() {
                    path.addLine(to: point)
                }
            }
            .stroke(ColorManager.textPrimary, lineWidth: 1.5)
        }
    }
}

// MARK: - Section Card
struct SectionCard: View {
    let title: LocalizedStringKey
    
    var body: some View {
        HStack {
            Text(title)
                .font(.headline)
                .foregroundColor(ColorManager.textPrimary)
            
            Spacer()
            
            Image(systemName: "chevron.right")
                .foregroundColor(ColorManager.textSecondary)
        }
        .padding(20)
        .background(
            RoundedRectangle(cornerRadius: 16)
                .fill(ColorManager.cardBackground.opacity(0.8))
        )
    }
}

// MARK: - Goals Card
struct GoalsCard: View {
    var body: some View {
        HStack {
            VStack(alignment: .leading) {
                Text(LocalizedStringKey("Training Goals"))
                    .font(.headline)
                    .foregroundColor(ColorManager.textPrimary)
                
                HStack(spacing: 8) {
                    ForEach(0..<4) { _ in
                        Image(systemName: "checkmark.circle.fill")
                            .foregroundColor(.green)
                    }
                    
                    Image(systemName: "checkmark.circle.fill")
                        .foregroundColor(ColorManager.textSecondary.opacity(0.5))
                }
            }
            
            Spacer()
            
            Image(systemName: "chevron.right")
                .foregroundColor(ColorManager.textSecondary)
        }
        .padding(20)
        .background(
            RoundedRectangle(cornerRadius: 16)
                .fill(ColorManager.cardBackground.opacity(0.8))
        )
    }
}
</file>

<file path="MoveInsight/MoveInsightApp.swift">
import SwiftUI

@main
struct MoveInsightApp: App {
    init() {
        // Set the accent color for the entire app
        UINavigationBar.appearance().tintColor = UIColor(Color.accentColor)
    }
    
    var body: some Scene {
        WindowGroup {
            ContentView()
                .accentColor(ColorManager.accentColor)
        }
    }
}

// For iOS 17+, we can use the newer API too
#if swift(>=5.9)
extension MoveInsightApp {
    @ViewBuilder
    private func contentWithTint() -> some View {
        if #available(iOS 17.0, *) {
            ContentView()
                .preferredColorScheme(.dark)
                .tint(ColorManager.accentColor)
        } else {
            ContentView()
                .preferredColorScheme(.dark)
                .accentColor(ColorManager.accentColor)
        }
    }
}
#endif
</file>

<file path="MoveInsight/SceneView3D.swift">
// MoveInsight/SceneView3D.swift
import SwiftUI
import SceneKit
import simd // For SIMD3<Float>

// SceneKitViewDelegate: Manages SceneKit renderer updates and camera state.
class SceneKitViewDelegate: NSObject, SCNSceneRendererDelegate, ObservableObject {
    @Published var lastCameraTransform: SCNMatrix4?

    func renderer(_ renderer: SCNSceneRenderer, updateAtTime time: TimeInterval) {
        if let pointOfView = renderer.pointOfView {
            DispatchQueue.main.async {
                if let lastTransform = self.lastCameraTransform {
                    if !SCNMatrix4EqualToMatrix4(lastTransform, pointOfView.transform) {
                        self.lastCameraTransform = pointOfView.transform
                    }
                } else {
                    self.lastCameraTransform = pointOfView.transform
                }
            }
        }
    }
}

// SceneView3D: A SwiftUI UIViewRepresentable for displaying 3D skeletons.
struct SceneView3D: UIViewRepresentable {
    let userPose3D: [String: SIMD3<Float>]?
    let modelPose3D: [String: SIMD3<Float>]?
    let bodyConnections: [BodyConnection3D]

    // zScale: Adjusts the depth perception. Tune as needed.
    private let zScale: Float = 0.7
    private let skeletonNodeName = "skeletonRootNode"
    // floorLevelY: The Y-coordinate where the floor is placed. Skeletons will stand on this.
    private let floorLevelY: Float = -0.75
    // skeletonHeightScale: Multiplies the normalized Y-coordinates to give skeletons a reasonable height in the scene.
    private let skeletonHeightScale: Float = 1.5 // Adjust this to make skeletons taller or shorter

    @ObservedObject var sceneDelegate: SceneKitViewDelegate

    func makeUIView(context: Context) -> SCNView {
        let sceneView = SCNView()
        sceneView.scene = SCNScene()

        setupCamera(in: sceneView.scene!, view: sceneView)
        setupLighting(in: sceneView.scene!)
        setupFloorWithGrid(in: sceneView.scene!) // Updated to add a grid pattern
        
        sceneView.backgroundColor = UIColor.systemGray5
        sceneView.allowsCameraControl = true
        sceneView.showsStatistics = false
        sceneView.delegate = sceneDelegate
        sceneView.antialiasingMode = .multisampling4X
        
        return sceneView
    }
    
    func updateUIView(_ uiView: SCNView, context: Context) {
        guard let scene = uiView.scene else { return }

        // Remove previously drawn skeletons
        scene.rootNode.childNodes { (node, _) -> Bool in
            node.name == skeletonNodeName
        }.forEach { $0.removeFromParentNode() }

        // Add user skeleton
        if let userPose = userPose3D, !userPose.isEmpty {
            addSkeletonToScene(
                pose: userPose,
                scene: scene,
                color: .systemBlue,
                baseOffset: SIMD3<Float>(-0.4, 0, 0) // X-offset for side-by-side placement
            )
        }
        
        // Add model skeleton
        if let modelPose = modelPose3D, !modelPose.isEmpty {
            addSkeletonToScene(
                pose: modelPose,
                scene: scene,
                color: .systemRed,
                baseOffset: SIMD3<Float>(0.4, 0, 0) // X-offset for side-by-side placement
            )
        }
    }
    
    private func setupCamera(in scene: SCNScene, view: SCNView) {
        let cameraNode = SCNNode()
        cameraNode.camera = SCNCamera()
        cameraNode.camera?.zNear = 0.1
        cameraNode.camera?.zFar = 100
        cameraNode.camera?.fieldOfView = 50

        if let transform = sceneDelegate.lastCameraTransform {
            cameraNode.transform = transform
        } else {
            // Initial camera position: looking at origin (0,0,0), slightly elevated, positive Z.
            cameraNode.position = SCNVector3(x: 0, y: 0.3, z: 2.8) // Adjusted Z for a bit more distance
            cameraNode.look(at: SCNVector3(0, 0, 0))
        }
        scene.rootNode.addChildNode(cameraNode)
    }
    
    private func setupLighting(in scene: SCNScene) {
        // Ambient light for overall illumination
        let ambientLightNode = SCNNode()
        ambientLightNode.light = SCNLight()
        ambientLightNode.light!.type = .ambient
        ambientLightNode.light!.color = UIColor(white: 0.7, alpha: 1.0)
        scene.rootNode.addChildNode(ambientLightNode)
        
        // Directional light for highlights and shadows
        let directionalLightNode = SCNNode()
        directionalLightNode.light = SCNLight()
        directionalLightNode.light!.type = .directional
        directionalLightNode.light!.color = UIColor(white: 0.8, alpha: 1.0)
        directionalLightNode.light!.castsShadow = true
        directionalLightNode.light!.shadowMode = .deferred
        directionalLightNode.light!.shadowColor = UIColor.black.withAlphaComponent(0.4)
        directionalLightNode.light!.shadowSampleCount = 16
        directionalLightNode.light!.shadowRadius = 3.0
        directionalLightNode.position = SCNVector3(x: -1.5, y: 2.5, z: 2)
        directionalLightNode.look(at: SCNVector3(0,0,0))
        scene.rootNode.addChildNode(directionalLightNode)
    }

    // Sets up the floor with a checkerboard pattern to simulate a grid.
    private func setupFloorWithGrid(in scene: SCNScene) {
        let floorGeometry = SCNFloor()
        floorGeometry.reflectivity = 0.05
        
        // Create a checkerboard material for the floor
        let floorMaterial = SCNMaterial()
        if #available(iOS 13.0, *) { // Check for iOS 13 availability for CIFilter
            let checkerboard = CIFilter(name: "CICheckerboardGenerator")!
            checkerboard.setValue(CIColor.gray, forKey: "inputColor0") // Color for light squares
            checkerboard.setValue(CIColor.black, forKey: "inputColor1") // Color for dark squares
            checkerboard.setValue(80, forKey: "inputWidth") // Width of each square in the pattern
            checkerboard.setValue(CIVector(x: 0, y: 0), forKey: "inputCenter")
            if let ciImage = checkerboard.outputImage {
                floorMaterial.diffuse.contents = ciImage
                // Adjust texture wrapping and scaling if needed
                floorMaterial.diffuse.wrapS = .repeat
                floorMaterial.diffuse.wrapT = .repeat
                // Scale the texture to make the grid appear reasonably sized on the floor.
                // A transform of SCNMatrix4MakeScale(10, 10, 10) means the texture repeats 10 times.
                floorMaterial.diffuse.contentsTransform = SCNMatrix4MakeScale(20, 20, 1) // Repeat texture more for smaller grid cells
            } else {
                floorMaterial.diffuse.contents = UIColor.systemGray4 // Fallback color
            }
        } else {
            floorMaterial.diffuse.contents = UIColor.systemGray4 // Fallback for older iOS
        }
        floorMaterial.lightingModel = .physicallyBased
        floorGeometry.materials = [floorMaterial]
        
        let floorNode = SCNNode(geometry: floorGeometry)
        floorNode.position = SCNVector3(0, floorLevelY, 0)
        scene.rootNode.addChildNode(floorNode)
        
        // Note: For a more distinct line grid, you would typically use an image texture
        // with lines drawn on it, e.g., UIImage(named: "grid_texture.png").
        // The checkerboard provides a basic grid-like pattern.
    }
    
    // Adds a skeleton to the scene, adjusting its Y position to stand on the floor.
    private func addSkeletonToScene(pose: [String: SIMD3<Float>], scene: SCNScene, color: UIColor, baseOffset: SIMD3<Float>) {
        let skeletonRoot = SCNNode()
        skeletonRoot.name = skeletonNodeName
        
        var jointNodes: [String: SCNNode] = [:]
        var minYInSkeletonSpace: Float = Float.greatestFiniteMagnitude

        // Store transformed joint positions relative to the skeleton's own origin
        var relativeJointPositions: [String: SCNVector3] = [:]

        // First pass: Transform coordinates (including Y-inversion) and find the minimum Y.
        for (jointName, position3D) in pose {
            // Assuming position3D.x and position3D.y are normalized (0-1)
            // X: Center it if needed, e.g., (position3D.x - 0.5) * someXScale
            let transformedX = (position3D.x - 0.5) * skeletonHeightScale // Center X and scale
            
            // Y: Invert (0=top becomes 1=top_of_skeleton_height) and scale.
            // (1.0 - position3D.y) makes 0 bottom of normalized range, 1 top.
            // Then scale by skeletonHeightScale.
            let transformedY = (1.0 - position3D.y) * skeletonHeightScale
            
            let transformedZ = position3D.z * zScale
            
            let relativePos = SCNVector3(transformedX, transformedY, transformedZ)
            relativeJointPositions[jointName] = relativePos
            minYInSkeletonSpace = min(minYInSkeletonSpace, transformedY)
        }

        // If no valid minY was found (e.g., empty pose), default to 0 to avoid issues.
        if minYInSkeletonSpace == Float.greatestFiniteMagnitude {
            minYInSkeletonSpace = 0
        }
        
        // Calculate the Y adjustment needed to place the skeleton's lowest point (minYInSkeletonSpace) on the floorLevelY.
        let yShiftToFloor = floorLevelY - minYInSkeletonSpace
        
        // Set the final position for the skeleton root node.
        // The Y position includes the base offset (usually 0 for Y) and the calculated shift.
        skeletonRoot.position = SCNVector3(baseOffset.x, yShiftToFloor + baseOffset.y, baseOffset.z)
        scene.rootNode.addChildNode(skeletonRoot)

        // Second pass: Create SCNNode for each joint using its relative position and add to skeletonRoot.
        for (jointName, relativePos) in relativeJointPositions {
            let jointNode = createJointSphereNode(radius: 0.025, color: color)
            jointNode.position = relativePos // Position is relative to skeletonRoot
            skeletonRoot.addChildNode(jointNode)
            jointNodes[jointName] = jointNode // Store for bone creation
        }
        
        // Create cylinder nodes for bones, connecting the joint spheres.
        // These are also relative to skeletonRoot.
        for connection in bodyConnections {
            guard let fromNode = jointNodes[connection.from],
                  let toNode = jointNodes[connection.to] else {
                continue
            }
            let boneNode = createBoneCylinderNode(from: fromNode.position, to: toNode.position, radius: 0.012, color: color)
            skeletonRoot.addChildNode(boneNode)
        }
    }
    
    private func createJointSphereNode(radius: CGFloat, color: UIColor) -> SCNNode {
        let sphere = SCNSphere(radius: radius)
        let material = SCNMaterial()
        material.diffuse.contents = color
        material.lightingModel = .phong
        sphere.materials = [material]
        return SCNNode(geometry: sphere)
    }
    
    private func createBoneCylinderNode(from startPoint: SCNVector3, to endPoint: SCNVector3, radius: CGFloat, color: UIColor) -> SCNNode {
        let height = SCNVector3.distance(vectorStart: startPoint, vectorEnd: endPoint)
        guard height > 0.001 else { return SCNNode() }

        let cylinder = SCNCylinder(radius: radius, height: CGFloat(height))
        let material = SCNMaterial()
        material.diffuse.contents = color
        material.lightingModel = .phong
        cylinder.materials = [material]
        
        let boneNode = SCNNode(geometry: cylinder)
        
        boneNode.position = SCNVector3(
            (startPoint.x + endPoint.x) / 2,
            (startPoint.y + endPoint.y) / 2,
            (startPoint.z + endPoint.z) / 2
        )
        
        let directionVector = endPoint - startPoint
        let yAxis = SCNVector3(0, 1, 0)
        
        let rotationAxis = yAxis.cross(directionVector).normalized()
        var angle = acos(yAxis.dot(directionVector) / (yAxis.length() * directionVector.length()))

        if rotationAxis.length() < 0.001 {
            if directionVector.y < 0 {
                angle = .pi
                 boneNode.rotation = SCNVector4(1, 0, 0, Float.pi)
            } else {
                boneNode.rotation = SCNVector4(0,0,0,0)
            }
        } else if !rotationAxis.x.isNaN && !rotationAxis.y.isNaN && !rotationAxis.z.isNaN && !angle.isNaN {
             boneNode.rotation = SCNVector4(rotationAxis.x, rotationAxis.y, rotationAxis.z, angle)
        }
        return boneNode
    }
}

// Helper SCNVector3 extensions
extension SCNVector3 {
    static func distance(vectorStart: SCNVector3, vectorEnd: SCNVector3) -> Float {
        let dx = vectorEnd.x - vectorStart.x; let dy = vectorEnd.y - vectorStart.y; let dz = vectorEnd.z - vectorStart.z
        return sqrt(dx*dx + dy*dy + dz*dz)
    }
    func length() -> Float { return sqrt(x*x + y*y + z*z) }
    func normalized() -> SCNVector3 {
        let len = length(); if len == 0 { return SCNVector3(0,0,0) }
        return SCNVector3(x/len, y/len, z/len)
    }
    func cross(_ vector: SCNVector3) -> SCNVector3 {
        return SCNVector3(y * vector.z - z * vector.y, z * vector.x - x * vector.z, x * vector.y - y * vector.x)
    }
    func dot(_ vector: SCNVector3) -> Float { return x * vector.x + y * vector.y + z * vector.z }
}
func -(left: SCNVector3, right: SCNVector3) -> SCNVector3 {
    return SCNVector3Make(left.x - right.x, left.y - right.y, left.z - right.z)
}
</file>

<file path="MoveInsight/TechniqueAnalysisService.swift">
// MoveInsight/TechniqueAnalysisService.swift
import Foundation
import Combine
import AVFoundation // For URL

// MARK: - Data Structures for Server Communication

struct ServerJointData: Codable {
    let x: Double
    let y: Double
    let z: Double? // Added Z coordinate
    let confidence: Double?
}

struct ServerFrameData: Codable {
    let joints: [String: ServerJointData] // Key is joint name string
}

struct VideoAnalysisResponse: Codable {
    let totalFrames: Int
    let jointDataPerFrame: [ServerFrameData] // Will now contain 3D data
    let swingAnalysis: [String: Bool]? // Swing analysis remains 2D based

    enum CodingKeys: String, CodingKey {
        case totalFrames = "total_frames"
        case jointDataPerFrame = "joint_data_per_frame"
        case swingAnalysis = "swing_analysis"
    }
}

struct TechniqueComparisonRequestData: Codable {
    let userVideoFrames: [ServerFrameData] // Will now contain 3D data
    let modelVideoFrames: [ServerFrameData] // Will now contain 3D data
    let dominantSide: String

    enum CodingKeys: String, CodingKey {
        case userVideoFrames = "user_video_frames"
        case modelVideoFrames = "model_video_frames"
        case dominantSide = "dominant_side"
    }
}

// ComparisonResult struct is defined in TechniqueComparisonView.swift
// It's based on the 2D swing analysis from the server.
// struct ComparisonResult: Codable { ... }

class TechniqueAnalysisService {
    // Ensure this URL points to your server's correct IP/domain and port
    private let serverBaseURL = URL(string: "http://115.188.74.78:8000")!

    // Function to upload a single video and get its joint data (now 3D)
    func analyzeVideoByUploading(videoURL: URL, dominantSide: String) -> AnyPublisher<VideoAnalysisResponse, Error> {
        let endpoint = serverBaseURL.appendingPathComponent("/analyze/video_upload/")
        
        var request = URLRequest(url: endpoint)
        request.httpMethod = "POST"
        
        let boundary = "Boundary-\(UUID().uuidString)"
        request.setValue("multipart/form-data; boundary=\(boundary)", forHTTPHeaderField: "Content-Type")
        
        var httpBody = Data()
        
        // Add dominant_side part
        httpBody.append("--\(boundary)\r\n".data(using: .utf8)!)
        httpBody.append("Content-Disposition: form-data; name=\"dominant_side\"\r\n\r\n".data(using: .utf8)!)
        httpBody.append("\(dominantSide)\r\n".data(using: .utf8)!)
        
        // Add video file part
        do {
            let videoData = try Data(contentsOf: videoURL)
            let filename = videoURL.lastPathComponent
            // Mimetype for mp4. Consider making this more dynamic if other formats are supported.
            let mimetype = "video/mp4"
            
            httpBody.append("--\(boundary)\r\n".data(using: .utf8)!)
            httpBody.append("Content-Disposition: form-data; name=\"file\"; filename=\"\(filename)\"\r\n".data(using: .utf8)!)
            httpBody.append("Content-Type: \(mimetype)\r\n\r\n".data(using: .utf8)!)
            httpBody.append(videoData)
            httpBody.append("\r\n".data(using: .utf8)!)
        } catch {
            print("Error reading video data for upload: \(error)")
            return Fail(error: error).eraseToAnyPublisher()
        }
        
        httpBody.append("--\(boundary)--\r\n".data(using: .utf8)!)
        request.httpBody = httpBody
        
        print("Uploading video for 3D analysis to \(endpoint)... with dominant side: \(dominantSide)")

        return URLSession.shared.dataTaskPublisher(for: request)
            .tryMap { output in
                guard let httpResponse = output.response as? HTTPURLResponse else {
                    print("Invalid server response (not HTTPURLResponse)")
                    throw URLError(.badServerResponse)
                }
                print("Server response status code for 3D video upload: \(httpResponse.statusCode)")
                if !(200...299).contains(httpResponse.statusCode) {
                    if let responseString = String(data: output.data, encoding: .utf8) {
                        print("Server error response (3D upload) [Status: \(httpResponse.statusCode)]: \(responseString)")
                    } else {
                        print("Server error response (3D upload) [Status: \(httpResponse.statusCode)]: No parsable error body.")
                    }
                    throw URLError(.init(rawValue: httpResponse.statusCode), userInfo: [NSLocalizedDescriptionKey: "Server returned status \(httpResponse.statusCode) for 3D video upload"])
                }
                // For debugging the raw response:
                // if let jsonString = String(data: output.data, encoding: .utf8) {
                //    print("Raw server response (3D upload): \(jsonString.prefix(2000))") // Log more characters
                // }
                return output.data
            }
            .decode(type: VideoAnalysisResponse.self, decoder: JSONDecoder())
            .receive(on: DispatchQueue.main)
            .eraseToAnyPublisher()
    }

    // Function to send two sets of 3D joint data to the server for comparison.
    // The server will perform 2D swing analysis on this 3D data.
    func requestTechniqueComparison(
        userFrames: [ServerFrameData], // These now contain 3D data
        modelFrames: [ServerFrameData], // These now contain 3D data
        dominantSide: String
    ) -> AnyPublisher<ComparisonResult, Error> { // ComparisonResult is still based on 2D analysis
        
        let endpoint = serverBaseURL.appendingPathComponent("/analyze/technique_comparison/")
        print("Requesting technique comparison (with 3D frame data input) from: \(endpoint)")

        var request = URLRequest(url: endpoint)
        request.httpMethod = "POST"
        request.setValue("application/json", forHTTPHeaderField: "Content-Type")

        let requestData = TechniqueComparisonRequestData(
            userVideoFrames: userFrames,
            modelVideoFrames: modelFrames,
            dominantSide: dominantSide
        )

        do {
            request.httpBody = try JSONEncoder().encode(requestData)
            // For debugging the request payload:
            // if let jsonString = String(data: request.httpBody!, encoding: .utf8) {
            //      print("Sending 3D comparison request JSON: \(jsonString.prefix(2000))")
            // }
        } catch {
            print("Error encoding 3D comparison request data: \(error)")
            return Fail(error: error).eraseToAnyPublisher()
        }

        return URLSession.shared.dataTaskPublisher(for: request)
            .tryMap { output in
                guard let httpResponse = output.response as? HTTPURLResponse else {
                    print("Invalid server response (not HTTPURLResponse) for comparison.")
                    throw URLError(.badServerResponse)
                }
                print("Server response status code for 3D input comparison: \(httpResponse.statusCode)")
                if !(200...299).contains(httpResponse.statusCode) {
                    if let responseString = String(data: output.data, encoding: .utf8) {
                        print("Server error response (3D input comparison) [Status: \(httpResponse.statusCode)]: \(responseString)")
                    } else {
                         print("Server error response (3D input comparison) [Status: \(httpResponse.statusCode)]: No parsable error body.")
                    }
                    throw URLError(.init(rawValue: httpResponse.statusCode), userInfo: [NSLocalizedDescriptionKey: "Server returned status \(httpResponse.statusCode) for 3D input comparison"])
                }
                // For debugging raw comparison response:
                // if let jsonString = String(data: output.data, encoding: .utf8) {
                //     print("Raw server response JSON (3D input comparison): \(jsonString)")
                // }
                return output.data
            }
            .decode(type: ComparisonResult.self, decoder: JSONDecoder()) // ComparisonResult structure itself doesn't change yet
            .receive(on: DispatchQueue.main)
            .eraseToAnyPublisher()
    }
}

// Helper extension for appending string to Data
extension Data {
    mutating func append(_ string: String) {
        if let data = string.data(using: .utf8) {
            append(data)
        }
    }
}
</file>

<file path="MoveInsight/TechniqueDetailView.swift">
// MoveInsight/TechniqueDetailView.swift
import SwiftUI
import AVKit
import Combine
import simd // Required for SIMD3 if used by PoseOverlayView indirectly via ViewModel

// MARK: - Pose Overlay View (Adapted for 3D Poses from ViewModel, Renders 2D Projection)
// This view displays a 2D projection of the 3D pose data onto the video.
struct PoseOverlayView: View {
    @ObservedObject var viewModel: VideoPlayerViewModel // viewModel.poses is now [String: SIMD3<Float>]
    let videoActualRect: CGRect // The actual rectangle of the video content within the player view

    var body: some View {
        Canvas { context, size in
            // Ensure poses are available and the video rectangle is valid.
            guard !viewModel.poses.isEmpty,
                  videoActualRect.size.width > 0,
                  videoActualRect.size.height > 0 else {
                // print("PoseOverlayView: No poses or invalid videoActualRect \(videoActualRect)")
                return
            }
            let currentPoses3D = viewModel.poses // These are [String: SIMD3<Float>]

            // Draw connections using the x and y components of the 3D poses.
            for connection in viewModel.bodyConnections { // bodyConnections are StringBodyConnection for 2D
                guard let fromPose3D = currentPoses3D[connection.from],
                      let toPose3D = currentPoses3D[connection.to] else {
                    // print("PoseOverlayView: Missing joint for connection \(connection.from) -> \(connection.to)")
                    continue
                }
                
                // Extract 2D points (normalized x, y) from the 3D pose data.
                // Server provides x,y as normalized (0-1) screen coordinates.
                let fromPointNorm = CGPoint(x: CGFloat(fromPose3D.x), y: CGFloat(fromPose3D.y))
                let toPointNorm = CGPoint(x: CGFloat(toPose3D.x), y: CGFloat(toPose3D.y))
                
                // Scale these normalized points to the actual video rectangle's coordinate system.
                let fromCanvasPoint = CGPoint(
                    x: videoActualRect.origin.x + (fromPointNorm.x * videoActualRect.size.width),
                    y: videoActualRect.origin.y + (fromPointNorm.y * videoActualRect.size.height)
                )
                let toCanvasPoint = CGPoint(
                    x: videoActualRect.origin.x + (toPointNorm.x * videoActualRect.size.width),
                    y: videoActualRect.origin.y + (toPointNorm.y * videoActualRect.size.height)
                )
                
                var path = Path()
                path.move(to: fromCanvasPoint)
                path.addLine(to: toCanvasPoint)
                context.stroke(path, with: .color(ColorManager.accentColor.opacity(0.8)), lineWidth: 3)
            }

            // Draw joints using the x and y components of the 3D poses.
            for (_, jointPose3D) in currentPoses3D {
                let jointPointNorm = CGPoint(x: CGFloat(jointPose3D.x), y: CGFloat(jointPose3D.y))
                let jointCanvasPoint = CGPoint(
                    x: videoActualRect.origin.x + (jointPointNorm.x * videoActualRect.size.width),
                    y: videoActualRect.origin.y + (jointPointNorm.y * videoActualRect.size.height)
                )
                let rect = CGRect(x: jointCanvasPoint.x - 4, y: jointCanvasPoint.y - 4, width: 8, height: 8)
                context.fill(Path(ellipseIn: rect), with: .color(Color.red.opacity(0.8)))
            }
        }
        .opacity(0.7) // Keep overlay slightly transparent.
        // .drawingGroup() // Consider for performance with very complex drawings, test if needed.
    }
}

// KeyPointRow struct remains unchanged.
struct KeyPointRow: View {
    let icon: String
    let text: String
    var body: some View { /* ... as before ... */
        HStack(spacing: 12) {
            Image(systemName: icon)
                .font(.system(size: 18))
                .foregroundColor(ColorManager.accentColor)
                .frame(width: 24, height: 24)
            Text(LocalizedStringKey(text))
                .font(.subheadline)
                .foregroundColor(ColorManager.textPrimary)
            Spacer()
        }
    }
}

// MARK: - Main Detail View (TechniqueDetailView)
struct TechniqueDetailView: View {
    let technique: BadmintonTechnique
    
    // StateObjects to manage VideoPlayerViewModels for primary and comparison videos.
    // VideoPlayerViewModel now handles 3D pose data internally.
    @StateObject private var videoVMContainer = VideoVMContainer()
    @StateObject private var comparisonVideoVMContainer = VideoVMContainer()
    
    // UI state variables
    @State private var showUploadOptions = false
    @State private var isVideoBeingUploadedOrProcessed = false
    @State private var processingStatusMessage = NSLocalizedString("Processing video...", comment: "")
    @State private var analysisError: String? = nil
    
    @State private var isUploadingForComparison = false
    @State private var navigateToComparisonView = false // For programmatic navigation
    
    @State private var shouldShowProcessedPrimaryVideo = false

    // State to hold server frame data (which now includes 3D joint info).
    @State private var primaryVideoServerFrames: [ServerFrameData]? = nil
    @State private var comparisonVideoServerFrames: [ServerFrameData]? = nil
    // ComparisonResult is still based on 2D swing analysis from the server.
    @State private var comparisonAnalysisResult: ComparisonResult? = nil
    @State private var isFetchingComparisonAnalysis = false

    // State to hold the actual video rectangle for the primary video player's 2D overlay.
    @State private var primaryVideoActualRect: CGRect = .zero

    @State private var cancellables = Set<AnyCancellable>()
    private let analysisService = TechniqueAnalysisService() // Service handles 3D data communication.
    
    var body: some View {
        ZStack {
            ColorManager.background.ignoresSafeArea()
            
            ScrollView {
                VStack(spacing: 24) {
                    techniqueHeader // Displays technique name, icon, description.
                    
                    // Conditional content based on processing state:
                    if isVideoBeingUploadedOrProcessed {
                        processingIndicator // Shows progress view and status message.
                    } else if isFetchingComparisonAnalysis {
                        // Indicator for when comparison analysis is being fetched.
                        VStack(spacing: 16) {
                            ProgressView().progressViewStyle(CircularProgressViewStyle(tint: ColorManager.accentColor)).scaleEffect(1.5)
                            Text(LocalizedStringKey("Comparing techniques..."))
                                .foregroundColor(ColorManager.textPrimary)
                        }
                        .frame(height: 300).frame(maxWidth: .infinity).background(Color.black.opacity(0.05)).cornerRadius(12).padding(.horizontal)
                    } else if let error = analysisError {
                        errorView(error) // Displays error message and retry option.
                    } else if shouldShowProcessedPrimaryVideo, let primaryVM = videoVMContainer.viewModel {
                        // Display processed primary video with 2D overlay and comparison options.
                        processedVideoView(viewModel: primaryVM, videoActualRect: $primaryVideoActualRect)
                        comparisonOptionsView(primaryVM: primaryVM)
                    } else {
                        // Initial state: show upload button for primary video.
                        uploadButton(isComparisonUpload: false)
                    }
                    
                    keyPointsSection // Displays general key points for the technique.
                }
                .padding(.bottom, 32)
            }
            // Hidden NavigationLink for programmatic navigation to TechniqueComparisonView.
            .background(
                NavigationLink(
                    destination: navigationDestinationView(),
                    isActive: $navigateToComparisonView
                ) { EmptyView() }
            )
        }
        .navigationTitle(technique.name)
        .navigationBarTitleDisplayMode(.inline)
        // Sheet for presenting the video upload view.
        .sheet(isPresented: $showUploadOptions) {
            TechniqueVideoUploadView(technique: technique, isComparison: isUploadingForComparison) { videoURL in
                self.showUploadOptions = false // Dismiss sheet.
                if let url = videoURL { // If a video URL was selected.
                    DispatchQueue.main.asyncAfter(deadline: .now() + 0.1) { // Slight delay for UI.
                        if self.isUploadingForComparison {
                            handleComparisonVideoSelected(url)
                        } else {
                            handlePrimaryVideoSelected(url)
                        }
                    }
                }
            }
        }
    }

    // MARK: - Subviews (Header, Indicators, Error, Upload Button, Processed Video, Options, Key Points)
    // These subviews' implementations are largely the same as before, focusing on UI presentation.
    // The `processedVideoView` is notable as it uses `PoseOverlayView` which now handles 3D data for 2D projection.

    private var techniqueHeader: some View { /* ... as before ... */
        VStack(spacing: 12) {
            ZStack {
                Circle().fill(ColorManager.accentColor.opacity(0.2)).frame(width: 80, height: 80)
                Image(systemName: technique.iconName).font(.system(size: 36)).foregroundColor(ColorManager.accentColor)
            }
            Text(technique.name).font(.title).foregroundColor(ColorManager.textPrimary).multilineTextAlignment(.center)
            Text(technique.description).font(.body).foregroundColor(ColorManager.textSecondary).multilineTextAlignment(.center).padding(.horizontal, 24)
        }.padding(.top, 24)
    }

    private var processingIndicator: some View { /* ... as before ... */
        VStack(spacing: 16) {
            ProgressView().progressViewStyle(CircularProgressViewStyle(tint: ColorManager.accentColor)).scaleEffect(1.5)
            Text(LocalizedStringKey(processingStatusMessage))
                .foregroundColor(ColorManager.textPrimary)
        }
        .frame(height: 300).frame(maxWidth: .infinity).background(Color.black.opacity(0.05)).cornerRadius(12).padding(.horizontal)
    }

    private func errorView(_ errorMessage: String) -> some View { /* ... as before ... */
        VStack(spacing: 16) {
            Image(systemName: "exclamationmark.triangle.fill").font(.largeTitle).foregroundColor(.red)
            Text(LocalizedStringKey("Error")).font(.title2).foregroundColor(ColorManager.textPrimary)
            Text(errorMessage).font(.body).foregroundColor(ColorManager.textSecondary).multilineTextAlignment(.center).padding(.horizontal)
            Button(LocalizedStringKey("Try Upload Again")) {
                resetToInitialUploadState()
                showUploadOptions = true
            }
            .padding().background(ColorManager.accentColor).foregroundColor(.white).cornerRadius(10)
        }
        .padding().frame(maxWidth: .infinity).background(ColorManager.cardBackground.opacity(0.7)).cornerRadius(12).padding(.horizontal)
    }
    
    private func resetToInitialUploadState() { /* ... as before, ensures primaryVideoActualRect is reset ... */
        analysisError = nil
        isVideoBeingUploadedOrProcessed = false
        isFetchingComparisonAnalysis = false
        
        videoVMContainer.viewModel?.cleanup()
        videoVMContainer.viewModel = nil
        primaryVideoServerFrames = nil
        primaryVideoActualRect = .zero // Reset video rect crucial for new uploads
        
        comparisonVideoVMContainer.viewModel?.cleanup()
        comparisonVideoVMContainer.viewModel = nil
        comparisonVideoServerFrames = nil
        
        comparisonAnalysisResult = nil
        isUploadingForComparison = false
        shouldShowProcessedPrimaryVideo = false
        navigateToComparisonView = false
    }

    private func uploadButton(isComparisonUpload: Bool) -> some View { /* ... as before ... */
        UploadButton(
            title: isComparisonUpload ?
                LocalizedStringKey("Upload Second Video for Comparison") :
                LocalizedStringKey(String(format: NSLocalizedString("Upload Your %@ Video", comment: ""), technique.name)),
            iconName: "video.badge.plus"
        ) {
            analysisError = nil
            self.isUploadingForComparison = isComparisonUpload
            if !isComparisonUpload {
                comparisonVideoVMContainer.viewModel?.cleanup(); comparisonVideoVMContainer.viewModel = nil
                comparisonVideoServerFrames = nil; comparisonAnalysisResult = nil
                primaryVideoActualRect = .zero // Reset for new primary video
            }
            showUploadOptions = true
        }
        .padding(.top, 20)
    }
    
    // `processedVideoView` now uses `PoseOverlayView` which takes 3D poses from `viewModel`
    // and the `videoActualRect` for correct 2D projection.
    private func processedVideoView(viewModel: VideoPlayerViewModel, videoActualRect: Binding<CGRect>) -> some View {
        VStack(spacing: 12) {
            Text(LocalizedStringKey("Your Analyzed Video"))
                .font(.headline).foregroundColor(ColorManager.textPrimary)
            
            VideoPlayerRepresentable(player: viewModel.player, videoRect: videoActualRect)
                .frame(height: 300).cornerRadius(12)
                // PoseOverlayView now gets 3D poses from viewModel but renders them in 2D using videoActualRect.
                .overlay(PoseOverlayView(viewModel: viewModel, videoActualRect: videoActualRect.wrappedValue))
                .background(Color.black) // Ensures black bars if video aspect ratio differs from frame.
                .onAppear { viewModel.play() }
                .onDisappear { viewModel.pause() }
            
            HStack { // Playback controls
                Button(action: { viewModel.restart() }) { Image(systemName: "arrow.clockwise.circle.fill") }
            }.font(.title).padding().foregroundColor(ColorManager.accentColor)
        }
        .padding(.horizontal)
    }

    private func comparisonOptionsView(primaryVM: VideoPlayerViewModel) -> some View { /* ... as before ... */
        VStack(spacing: 16) {
            Text(LocalizedStringKey("Next Steps"))
                .font(.title3).fontWeight(.semibold).foregroundColor(ColorManager.textPrimary)

            uploadButton(isComparisonUpload: true) // Button to upload a comparison video

            if technique.hasModelVideo { // If a model video is available for this technique
                Button {
                    // Ensure primary video's 3D frame data is available before comparing with model
                    guard primaryVideoServerFrames != nil else {
                        analysisError = "Primary video 3D data not available for model comparison."
                        return
                    }
                    handleCompareWithModelVideo(primaryUserVM: primaryVM)
                } label: {
                    Label(LocalizedStringKey("Compare with Model Video"), systemImage: "person.crop.square.filled.and.at.rectangle")
                        .font(.headline).padding().frame(maxWidth: .infinity)
                }
                .buttonStyle(.bordered).tint(ColorManager.accentColor)
            }
            
            Button(LocalizedStringKey("Re-upload Primary Video")) {
                resetToInitialUploadState()
                showUploadOptions = true
            }
            .padding(.top, 10)
            .foregroundColor(ColorManager.accentColor)
        }
        .padding(.horizontal)
    }

    private var keyPointsSection: some View { /* ... as before ... */
        VStack(alignment: .leading, spacing: 16) {
            Text(LocalizedStringKey(String(format: NSLocalizedString("Key Points for %@", comment: ""), technique.name)))
                .font(.headline).foregroundColor(ColorManager.textPrimary)
            KeyPointRow(icon: "figure.play", text: "Start with proper stance, feet shoulder-width apart.")
            KeyPointRow(icon: "hand.raised", text: "Grip the racket with a relaxed, comfortable hold.")
            KeyPointRow(icon: "arrow.up.and.down.and.arrow.left.and.right", text: "Maintain balance throughout the motion.")
            KeyPointRow(icon: "eye", text: "Keep your eye on the shuttle at all times.")
            KeyPointRow(icon: "figure.walk", text: "Follow through with your swing for better control.")
        }
        .padding().background(RoundedRectangle(cornerRadius: 12).fill(ColorManager.cardBackground)).padding(.horizontal)
    }

    // Navigation destination for comparison view.
    // TechniqueComparisonView will receive ViewModels that now manage 3D pose data.
    @ViewBuilder
    private func navigationDestinationView() -> some View {
        if let userVM = videoVMContainer.viewModel,
           let compVM = comparisonVideoVMContainer.viewModel,
           userVM.isVideoReady, compVM.isVideoReady,
           let userFrames = primaryVideoServerFrames, // These are [ServerFrameData] with 3D joints
           let compFrames = comparisonVideoServerFrames { // These are [ServerFrameData] with 3D joints
            
            // Pass the ViewModels (which contain 3D poses) and the 2D analysis result.
            TechniqueComparisonView(
                technique: technique,
                userVideoViewModel: userVM,   // ViewModel now has 3D poses in .poses
                modelVideoViewModel: compVM,  // ViewModel now has 3D poses in .poses
                analysisResult: comparisonAnalysisResult // This is still the 2D swing analysis
            )
        } else {
            ProgressView(LocalizedStringKey("Preparing comparison view..."))
        }
    }

    // MARK: - Video Handling & Analysis Logic
    // These functions now trigger analysis that returns 3D pose data.
    // The VideoPlayerViewModel's `setServerProcessedJoints` method is updated to handle this 3D data.

    private func handlePrimaryVideoSelected(_ url: URL) {
        isVideoBeingUploadedOrProcessed = true
        processingStatusMessage = NSLocalizedString("Uploading & Analyzing Primary Video (3D)...", comment: "") // Updated message
        analysisError = nil
        shouldShowProcessedPrimaryVideo = false
        primaryVideoActualRect = .zero // Reset rect for the new video
        
        videoVMContainer.viewModel?.cleanup() // Clean up previous VM
        videoVMContainer.viewModel = nil
        primaryVideoServerFrames = nil

        // Call analysis service; it now returns VideoAnalysisResponse with 3D jointDataPerFrame.
        analysisService.analyzeVideoByUploading(videoURL: url, dominantSide: "Right")
            .sink(receiveCompletion: { completion in
                self.isVideoBeingUploadedOrProcessed = false
                if case let .failure(error) = completion {
                    self.analysisError = "Failed to analyze primary video (3D): \(error.localizedDescription)"
                    self.shouldShowProcessedPrimaryVideo = false
                }
            }, receiveValue: { response in // response.jointDataPerFrame contains 3D data
                print("Primary video 3D analysis successful. Received \(response.totalFrames) frames.")
                self.primaryVideoServerFrames = response.jointDataPerFrame // Store 3D server frames
                
                let newVM = VideoPlayerViewModel(videoURL: url, videoSource: .primary)
                newVM.setServerProcessedJoints(response.jointDataPerFrame) // VM processes 3D data
                self.videoVMContainer.viewModel = newVM
                
                self.listenForVMReadyAndSetShowFlag(vm: newVM, isPrimary: true)
            })
            .store(in: &cancellables)
    }

    private func handleComparisonVideoSelected(_ url: URL) {
        // Similar to handlePrimaryVideoSelected, but for the comparison video.
        isVideoBeingUploadedOrProcessed = true
        processingStatusMessage = NSLocalizedString("Uploading & Analyzing Comparison Video (3D)...", comment: "")
        analysisError = nil
        
        comparisonVideoVMContainer.viewModel?.cleanup()
        comparisonVideoVMContainer.viewModel = nil
        comparisonVideoServerFrames = nil

        analysisService.analyzeVideoByUploading(videoURL: url, dominantSide: "Right")
            .sink(receiveCompletion: { completion in
                self.isVideoBeingUploadedOrProcessed = false
                if case let .failure(error) = completion {
                    self.analysisError = "Failed to analyze comparison video (3D): \(error.localizedDescription)"
                }
            }, receiveValue: { response in
                print("Comparison video 3D analysis successful. Received \(response.totalFrames) frames.")
                self.comparisonVideoServerFrames = response.jointDataPerFrame
                
                let newCompVM = VideoPlayerViewModel(videoURL: url, videoSource: .secondary)
                newCompVM.setServerProcessedJoints(response.jointDataPerFrame)
                self.comparisonVideoVMContainer.viewModel = newCompVM

                self.listenForVMReadyAndSetShowFlag(vm: newCompVM, isPrimary: false)
            })
            .store(in: &cancellables)
    }
    
    private func handleCompareWithModelVideo(primaryUserVM: VideoPlayerViewModel) {
        // Similar logic, but loads and analyzes the pre-defined model video.
        guard let modelVideoURL = ModelVideoLoader.shared.getModelVideoURL(for: technique.name) else {
            analysisError = "Model video for \(technique.name) not found."
            return
        }
        // Ensure primary user video's 3D data is available.
        guard self.primaryVideoServerFrames != nil else {
            analysisError = "Primary video 3D data is missing for model comparison."
            return
        }
        
        isVideoBeingUploadedOrProcessed = true
        processingStatusMessage = NSLocalizedString("Processing Model Video (3D)...", comment: "")
        analysisError = nil

        comparisonVideoVMContainer.viewModel?.cleanup()
        comparisonVideoVMContainer.viewModel = nil
        comparisonVideoServerFrames = nil
        
        analysisService.analyzeVideoByUploading(videoURL: modelVideoURL, dominantSide: "Right")
            .sink(receiveCompletion: { completion in
                self.isVideoBeingUploadedOrProcessed = false
                if case let .failure(error) = completion {
                    self.analysisError = "Failed to analyze model video (3D): \(error.localizedDescription)"
                }
            }, receiveValue: { response in
                print("Model video 3D analysis successful. Received \(response.totalFrames) frames.")
                self.comparisonVideoServerFrames = response.jointDataPerFrame
                
                let newModelVM = VideoPlayerViewModel(videoURL: modelVideoURL, videoSource: .secondary)
                newModelVM.setServerProcessedJoints(response.jointDataPerFrame)
                self.comparisonVideoVMContainer.viewModel = newModelVM
                
                self.listenForVMReadyAndSetShowFlag(vm: newModelVM, isPrimary: false)
            })
            .store(in: &cancellables)
    }

    // `listenForVMReadyAndSetShowFlag` remains largely the same.
    // It waits for VideoPlayerViewModel to be ready and have poses (which are now 3D).
    private func listenForVMReadyAndSetShowFlag(vm: VideoPlayerViewModel, isPrimary: Bool) {
        var readyCancellable: AnyCancellable?
        readyCancellable = vm.$isVideoReady
            .combineLatest(vm.$accumulatedPoses.map { !$0.isEmpty }) // accumulatedPoses is now 3D
            .filter { $0.0 && $0.1 } // Video ready AND poses available
            .first() // Only take the first time this condition is met
            .sink { [weak vmInstance = vm] _ in // Capture vm weakly
                guard vmInstance != nil else {
                    readyCancellable?.cancel()
                    return
                }
                if isPrimary {
                    print("Primary VM (handling 3D data) is ready and poses are set.")
                    self.shouldShowProcessedPrimaryVideo = true
                } else { // This is for comparison or model video
                    print("Comparison/Model VM (handling 3D data) is ready and poses are set.")
                    // Slight delay before triggering comparison to ensure UI updates settle.
                    DispatchQueue.main.asyncAfter(deadline: .now() + 0.2) {
                        self.triggerTechniqueComparison()
                    }
                }
                readyCancellable?.cancel() // Clean up the cancellable
            }
        if let rc = readyCancellable { self.cancellables.insert(rc) }
    }

    // `triggerTechniqueComparison` sends 3D frame data to the server.
    // The server performs 2D swing analysis and returns ComparisonResult.
    private func triggerTechniqueComparison() {
        guard let userFrames = primaryVideoServerFrames, // These are [ServerFrameData] with 3D joints
              let modelFrames = comparisonVideoServerFrames else { // These are [ServerFrameData] with 3D joints
            analysisError = "One or both 3D video data sets are missing for comparison."
            isFetchingComparisonAnalysis = false; return
        }
        // Ensure ViewModels are ready.
        guard let userVM = videoVMContainer.viewModel, userVM.isVideoReady,
              let modelVM = comparisonVideoVMContainer.viewModel, modelVM.isVideoReady else {
            analysisError = "Video players not ready for comparison (using 3D data).";
            isFetchingComparisonAnalysis = false; return
        }

        isFetchingComparisonAnalysis = true; analysisError = nil
        
        // Call service: it sends 3D frames, but server's swing analysis logic is still 2D.
        analysisService.requestTechniqueComparison(
            userFrames: userFrames,
            modelFrames: modelFrames,
            dominantSide: "Right"
        )
        .sink(receiveCompletion: { completion in
            self.isFetchingComparisonAnalysis = false
            if case let .failure(error) = completion {
                self.analysisError = "Technique comparison (from 3D data input) failed: \(error.localizedDescription)"
                 print("Comparison request error: \(error)")
            }
        }, receiveValue: { result in // `result` is ComparisonResult based on 2D server-side analysis
            self.comparisonAnalysisResult = result
            self.navigateToComparisonView = true // Trigger navigation
            print("Technique comparison successful (from 3D data input). Navigating.")
        })
        .store(in: &cancellables)
    }
}

class VideoVMContainer: ObservableObject { @Published var viewModel: VideoPlayerViewModel? }
</file>

<file path="MoveInsight/TechniquesListView.swift">
import SwiftUI
import AVKit

// Define a structure for badminton techniques
struct BadmintonTechnique: Identifiable {
    let id = UUID()
    let name: String
    let description: String
    let iconName: String
    
    // Flag to indicate if we have a model video for this technique
    let hasModelVideo: Bool
}

struct TechniquesListView: View {
    // List of all badminton techniques
    let techniques = [
        BadmintonTechnique(
            name: NSLocalizedString("Backhand Clear", comment: "Technique name"),
            description: NSLocalizedString("A clear shot played with the back of the hand facing forward.", comment: "Technique description"),
            iconName: "arrow.left.arrow.right",
            hasModelVideo: ModelVideoLoader.shared.hasModelVideo(for: "Backhand Clear")
        ),
        BadmintonTechnique(
            name: NSLocalizedString("Underhand Clear", comment: "Technique name"),
            description: NSLocalizedString("A defensive shot played from below waist height, sending the shuttle high to the back of the opponent's court.", comment: "Technique description"),
            iconName: "arrow.up.forward",
            hasModelVideo: ModelVideoLoader.shared.hasModelVideo(for: "Underhand Clear")
        ),
        BadmintonTechnique(
            name: NSLocalizedString("Overhead Clear", comment: "Technique name"),
            description: NSLocalizedString("A powerful shot played from above the head, sending the shuttle to the back of the opponent's court.", comment: "Technique description"),
            iconName: "arrow.down.forward",
            hasModelVideo: ModelVideoLoader.shared.hasModelVideo(for: "Overhead Clear")
        ),
        BadmintonTechnique(
            name: NSLocalizedString("Drop Shot", comment: "Technique name"),
            description: NSLocalizedString("A gentle shot that just clears the net and drops sharply on the other side.", comment: "Technique description"),
            iconName: "arrow.down",
            hasModelVideo: ModelVideoLoader.shared.hasModelVideo(for: "Drop Shot")
        ),
        BadmintonTechnique(
            name: NSLocalizedString("Smash", comment: "Technique name"),
            description: NSLocalizedString("A powerful overhead shot hit steeply downward into the opponent's court.", comment: "Technique description"),
            iconName: "bolt.fill",
            hasModelVideo: ModelVideoLoader.shared.hasModelVideo(for: "Smash")
        ),
        BadmintonTechnique(
            name: NSLocalizedString("Net Shot", comment: "Technique name"),
            description: NSLocalizedString("A soft shot played near the net that just clears it and falls close to the net on the other side.", comment: "Technique description"),
            iconName: "power.dotted",
            hasModelVideo: ModelVideoLoader.shared.hasModelVideo(for: "Net Shot")
        )
    ]
    
    var body: some View {
        ZStack {
            ColorManager.background.ignoresSafeArea()
            
            ScrollView {
                VStack(spacing: 16) {
                    Text(LocalizedStringKey("Badminton Techniques"))
                        .font(.title)
                        .foregroundColor(ColorManager.textPrimary)
                        .padding(.top, 24)
                    
                    Text(LocalizedStringKey("Select a technique to upload and analyze your form"))
                        .font(.subheadline)
                        .foregroundColor(ColorManager.textSecondary)
                        .multilineTextAlignment(.center)
                        .padding(.horizontal)
                        .padding(.bottom, 12)
                    
                    LazyVStack(spacing: 16) {
                        ForEach(techniques) { technique in
                            NavigationLink(destination: TechniqueDetailView(technique: technique)) {
                                TechniqueCard(technique: technique)
                            }
                            .buttonStyle(PlainButtonStyle())
                        }
                    }
                    .padding(.horizontal, 16)
                    .padding(.bottom, 24)
                }
            }
        }
        .navigationTitle(LocalizedStringKey("Techniques"))
        .navigationBarTitleDisplayMode(.inline)
    }
}

struct TechniqueCard: View {
    let technique: BadmintonTechnique
    
    var body: some View {
        HStack(spacing: 16) {
            // Technique icon
            ZStack {
                Circle()
                    .fill(ColorManager.accentColor.opacity(0.2))
                    .frame(width: 60, height: 60)
                
                Image(systemName: technique.iconName)
                    .font(.system(size: 24))
                    .foregroundColor(ColorManager.accentColor)
            }
            
            VStack(alignment: .leading, spacing: 4) {
                Text(technique.name)
                    .font(.headline)
                    .foregroundColor(ColorManager.textPrimary)
                
                Text(technique.description)
                    .font(.subheadline)
                    .foregroundColor(ColorManager.textSecondary)
                    .lineLimit(2)
            }
            
            Spacer()
            
            Image(systemName: "chevron.right")
                .foregroundColor(ColorManager.textSecondary)
        }
        .padding(16)
        .background(
            RoundedRectangle(cornerRadius: 12)
                .fill(ColorManager.cardBackground)
        )
    }
}
</file>

<file path="MoveInsight/TechniqueVideoUploadView.swift">
import SwiftUI
import PhotosUI
import AVKit

struct TechniqueVideoUploadView: View {
    let technique: BadmintonTechnique
    let isComparison: Bool 
    let onVideoSelected: (URL?) -> Void
    
    @State private var selectedItem: PhotosPickerItem?
    @State private var showPhotoPicker = false
    @State private var showPermissionAlert = false
    @State private var loadingMessage: String? = nil

    @Environment(\.presentationMode) var presentationMode
    
    var body: some View {
        ZStack {
            ColorManager.background.ignoresSafeArea()
            
            VStack(spacing: 24) {
                // Header
                VStack(spacing: 12) {
                    Text(isComparison ? 
                         LocalizedStringKey("Upload Comparison Video") : 
                         LocalizedStringKey(String(format: NSLocalizedString("Upload %@ Video", comment: ""), technique.name)))
                        .font(.title2)
                        .foregroundColor(ColorManager.textPrimary)
                        .padding(.top, 16)
                    
                    Text(LocalizedStringKey(String(format: NSLocalizedString("Select a video of yourself performing the %@ technique.", comment: ""), technique.name)))
                        .font(.subheadline)
                        .foregroundColor(ColorManager.textSecondary)
                        .multilineTextAlignment(.center)
                        .padding(.horizontal)
                }
                
                Spacer()
                
                if let message = loadingMessage {
                    VStack {
                        ProgressView()
                            .progressViewStyle(CircularProgressViewStyle(tint: ColorManager.accentColor))
                        Text(LocalizedStringKey(message))
                            .foregroundColor(ColorManager.textPrimary)
                            .padding(.top)
                    }
                } else {
                    // Upload button
                    Button(action: {
                        checkPermissionThenPick()
                    }) {
                        VStack(spacing: 16) {
                            Image(systemName: "video.badge.plus")
                                .font(.system(size: 40))
                                .foregroundColor(ColorManager.accentColor)
                            
                            Text(LocalizedStringKey("Select Video from Library"))
                                .font(.headline)
                                .foregroundColor(ColorManager.textPrimary)
                        }
                        .frame(maxWidth: .infinity)
                        .padding(48)
                        .background(
                            RoundedRectangle(cornerRadius: 12)
                                .stroke(ColorManager.accentColor, lineWidth: 2)
                                .background(ColorManager.cardBackground.cornerRadius(12))
                        )
                        .padding(.horizontal, 32)
                    }
                    
                    Text(LocalizedStringKey("For best results, ensure your entire body is visible, and you are performing the technique from start to finish."))
                        .font(.caption)
                        .foregroundColor(ColorManager.textSecondary)
                        .multilineTextAlignment(.center)
                        .padding(.horizontal, 32)
                }
                
                Spacer()
                
                // Bottom buttons
                Button(LocalizedStringKey("Cancel")) {
                    onVideoSelected(nil)
                    presentationMode.wrappedValue.dismiss()
                }
                .padding()
                .foregroundColor(ColorManager.textSecondary)
            }
            .padding(.horizontal, 16)
            .padding(.bottom, 16)
        }
        .photosPicker(
            isPresented: $showPhotoPicker,
            selection: $selectedItem,
            matching: .videos
        )
        .onChange(of: selectedItem) { newItem in
            guard let item = newItem else { return }
            loadingMessage = NSLocalizedString("Loading Video...", comment: "")
            
            // Load the video file URL
            item.loadTransferable(type: VideoItem.self) { result in
                DispatchQueue.main.async {
                    self.loadingMessage = nil 
                    switch result {
                    case .success(let videoItem?):
                        print("Video selected: \(videoItem.url)")
                        onVideoSelected(videoItem.url)
                    case .success(nil):
                        print("Video selection failed: No item returned.")
                        onVideoSelected(nil)
                    case .failure(let error):
                        print("Video selection failed with error: \(error.localizedDescription)")
                        onVideoSelected(nil)
                    }
                }
            }
        }
        .alert(LocalizedStringKey("Photo Library Access Required"), isPresented: $showPermissionAlert) {
            Button(LocalizedStringKey("Go to Settings")) {
                UIApplication.shared.open(URL(string: UIApplication.openSettingsURLString)!)
            }
            Button(LocalizedStringKey("Cancel"), role: .cancel) { }
        }
    }
    
    // MARK: - Permissions
    private func checkPermissionThenPick() {
        let status = PHPhotoLibrary.authorizationStatus(for: .readWrite)
        switch status {
        case .authorized, .limited:
            showPhotoPicker = true
        case .notDetermined:
            PHPhotoLibrary.requestAuthorization(for: .readWrite) { newStatus in
                DispatchQueue.main.async {
                    if newStatus == .authorized || newStatus == .limited {
                        showPhotoPicker = true
                    } else {
                        showPermissionAlert = true
                    }
                }
            }
        default:
            showPermissionAlert = true
        }
    }
}
</file>

<file path="MoveInsightServer/analysis_server.py">
# MoveInsightServer/analysis_server.py
from fastapi import FastAPI, HTTPException, Request, File, UploadFile, Form
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Dict, List, Any, Optional
import numpy as np
import logging
import time
import traceback
import cv2
import mediapipe as mp
import tempfile
import os

# Import functions from swing_diagnose.py
from swing_diagnose import evaluate_swing_rules, align_keypoints_with_interpolation

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("analysis_server")

# --- FastAPI App Setup ---
app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"],
)

@app.middleware("http")
async def log_requests(request: Request, call_next):
    start_time = time.time()
    client_host = request.client.host if request.client else "unknown"
    logger.info(f"Request started: {request.method} {request.url.path} - Client: {client_host}")
    try:
        response = await call_next(request)
        process_time = time.time() - start_time
        logger.info(f"Request completed: {request.method} {request.url.path} - {response.status_code} in {process_time:.2f}s")
        return response
    except Exception as e:
        logger.error(f"Request failed: {request.method} {request.url.path} - Error: {str(e)} - Traceback: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"Internal Server Error: {str(e)}")

# --- MediaPipe Pose Estimation Setup ---
mp_pose = mp.solutions.pose
JOINT_MAP = {
    0: 'Nose', 11: 'LeftShoulder', 12: 'RightShoulder', 13: 'LeftElbow', 14: 'RightElbow',
    15: 'LeftWrist', 16: 'RightWrist', 23: 'LeftHip', 24: 'RightHip', 25: 'LeftKnee',
    26: 'RightKnee', 27: 'LeftAnkle', 28: 'RightAnkle', 29: 'LeftHeel', 30: 'RightHeel',
    31: 'LeftFootIndex', 32: 'RightFootIndex'
}
JOINT_NAME_REMAPPING = { 'LeftFootIndex': 'LeftToe', 'RightFootIndex': 'RightToe' }

# --- Pydantic Models ---
class JointDataItem(BaseModel):
    x: float
    y: float
    z: Optional[float] = None # Added Z coordinate
    confidence: Optional[float] = None

class FrameDataItem(BaseModel):
    joints: Dict[str, JointDataItem]

class VideoAnalysisResponseModel(BaseModel):
    total_frames: int
    joint_data_per_frame: List[FrameDataItem]
    swing_analysis: Optional[Dict[str, bool]] = None

class TechniqueComparisonRequestDataModel(BaseModel):
    user_video_frames: List[FrameDataItem] # Will now contain 3D data
    model_video_frames: List[FrameDataItem] # Will now contain 3D data
    dominant_side: str

class ComparisonResultModel(BaseModel):
    user_score: float
    reference_score: float
    similarity: Dict[str, bool]
    user_details: Dict[str, bool]
    reference_details: Dict[str, bool]

# --- Helper Functions ---
def transform_pydantic_to_numpy(joint_data_per_frame_pydantic: List[FrameDataItem]) -> Dict[str, List[List[float]]]:
    """
    Transform pydantic models to format needed for align_keypoints_with_interpolation.
    Output format: {'joint_name': [[x1,y1,z1], [x2,y2,z2], ...]} 
    The inner lists can have varying lengths if a joint is not detected in all frames.
    """
    joint_data_lists: Dict[str, List[List[float]]] = {}
    if not joint_data_per_frame_pydantic:
        return joint_data_lists
    
    # Initialize lists for all possible joint names that might appear
    all_possible_joint_names = set(JOINT_MAP.values())
    all_possible_joint_names.update(JOINT_NAME_REMAPPING.values())

    for name in all_possible_joint_names:
        joint_data_lists[name] = []
    
    for frame_data in joint_data_per_frame_pydantic:
        for joint_name_in_frame, joint_info in frame_data.joints.items():
            # Ensure z is not None, default to 0.0 if it is
            z_val = joint_info.z if joint_info.z is not None else 0.0
            # Append to the list for this joint_name
            # If joint_name_in_frame is not pre-initialized (e.g. unexpected name), it will error.
            # This assumes joint_name_in_frame will always be in all_possible_joint_names.
            if joint_name_in_frame in joint_data_lists:
                 joint_data_lists[joint_name_in_frame].append([joint_info.x, joint_info.y, z_val])
            else:
                logger.warning(f"Unexpected joint name '{joint_name_in_frame}' encountered in pydantic transform.")
    
    return joint_data_lists

def transform_numpy_to_pydantic(keypoints: Dict[str, np.ndarray], frame_count: int) -> List[FrameDataItem]:
    """
    Transform numpy arrays (shape (T,3)) back to pydantic models for response
    """
    result: List[FrameDataItem] = []
    for frame_idx in range(frame_count):
        frame_joints: Dict[str, JointDataItem] = {}
        for joint_name, points_array in keypoints.items():
            if frame_idx < points_array.shape[0] and not np.isnan(points_array[frame_idx]).any(): # Check if frame_idx is within bounds and not all NaN
                point = points_array[frame_idx]
                if points_array.shape[1] == 3: # Ensure it's 3D data
                    frame_joints[joint_name] = JointDataItem(
                        x=float(point[0]), 
                        y=float(point[1]),
                        z=float(point[2]), # Add Z
                        confidence=0.9  # Default confidence; consider passing actual confidence if available
                    )
                # No fallback for 2D, expect 3D from align_keypoints
        if frame_joints: # Only add frame if it has any valid joints
            result.append(FrameDataItem(joints=frame_joints))
    return result

# --- API Endpoints ---
@app.post("/analyze/video_upload/", response_model=VideoAnalysisResponseModel)
async def analyze_video_upload(
    file: UploadFile = File(...),
    dominant_side: str = Form("Right")
):
    logger.info(f"Received video upload for 3D analysis: {file.filename}, dominant side: {dominant_side}")
    temp_video_path = ""
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as tmp:
            tmp.write(await file.read())
            temp_video_path = tmp.name
        logger.info(f"Video saved temporarily to: {temp_video_path}")

        # Store raw detected points per joint: {'joint_name': [[x,y,z], [x,y,z], ...]}
        # The inner lists will only contain data for frames where the joint was detected.
        joint_data_raw_detections: Dict[str, List[List[float]]] = {}
        
        # Initialize with all possible joint names that MediaPipe might output via our mapping
        all_expected_client_joint_names = set(JOINT_MAP.values())
        all_expected_client_joint_names.update(JOINT_NAME_REMAPPING.values())
        for name in all_expected_client_joint_names:
            joint_data_raw_detections[name] = []

        frame_count = 0
        
        with mp_pose.Pose(static_image_mode=False, model_complexity=1, min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose_estimator:
            cap = cv2.VideoCapture(temp_video_path)
            if not cap.isOpened():
                logger.error(f"Could not open video file: {temp_video_path}")
                raise HTTPException(status_code=400, detail="Could not open video file.")

            while cap.isOpened():
                success, frame = cap.read()
                if not success: break
                
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                results = pose_estimator.process(frame_rgb)
                
                if results.pose_landmarks:
                    for idx, lm in enumerate(results.pose_landmarks.landmark):
                        if idx in JOINT_MAP:
                            original_joint_name = JOINT_MAP[idx]
                            # Use the remapped name if it exists, otherwise the original name
                            joint_name_for_client = JOINT_NAME_REMAPPING.get(original_joint_name, original_joint_name)
                            
                            # lm.visibility is also available if needed for confidence
                            if joint_name_for_client in joint_data_raw_detections: # Ensure key exists
                                joint_data_raw_detections[joint_name_for_client].append([lm.x, lm.y, lm.z])
                            else:
                                # This case should ideally not happen if all_expected_client_joint_names is comprehensive
                                logger.warning(f"Detected joint '{joint_name_for_client}' not in pre-initialized keys. Adding it.")
                                joint_data_raw_detections[joint_name_for_client] = [[lm.x, lm.y, lm.z]]
                
                frame_count += 1
            cap.release()
        
        logger.info(f"Processed {frame_count} frames from video for 3D data.")

        if frame_count == 0:
            logger.warning("No frames detected in the video.")
            return VideoAnalysisResponseModel(total_frames=0, joint_data_per_frame=[], swing_analysis=None)

        # Align and interpolate, now expecting 3D data
        # align_keypoints_with_interpolation will fill missing frames with NaNs or interpolated values
        keypoints_3d_aligned = align_keypoints_with_interpolation(joint_data_raw_detections, frame_count)
        
        # Convert aligned 3D keypoints to pydantic models for response
        joint_data_per_frame_pydantic = transform_numpy_to_pydantic(keypoints_3d_aligned, frame_count)
        
        # --- Swing Analysis (using 2D slice) ---
        swing_analysis_results = None
        # Define keys required by evaluate_swing_rules
        required_keys_for_swing = [
            'RightShoulder', 'LeftShoulder', 'RightElbow', 'LeftElbow', 'RightWrist',
            'RightHip', 'LeftHip', 'RightHeel', 'RightToe', 'LeftHeel', 'LeftToe'
        ]
        default_swing_rules_keys = [ # Must match ComparisonResultModel.user_details keys
            'shoulder_abduction', 'elbow_flexion', 'elbow_lower', 
            'foot_direction_aligned', 'proximal_to_distal_sequence', 
            'hip_forward_shift', 'trunk_rotation_completed'
        ]

        keypoints_2d_for_swing: Dict[str, np.ndarray] = {}
        all_required_present_and_valid_for_swing = True

        for k in required_keys_for_swing:
            if k in keypoints_3d_aligned and keypoints_3d_aligned[k].shape[0] == frame_count and not np.all(np.isnan(keypoints_3d_aligned[k][:, :2])):
                keypoints_2d_for_swing[k] = keypoints_3d_aligned[k][:, :2] # Take x, y
            else:
                logger.warning(f"Keypoint '{k}' missing, has insufficient frames ({keypoints_3d_aligned.get(k, np.array([])).shape[0]}/{frame_count}), or is all NaNs for swing analysis.")
                all_required_present_and_valid_for_swing = False
                break
        
        if all_required_present_and_valid_for_swing:
            try:
                swing_analysis_results = evaluate_swing_rules(keypoints_2d_for_swing, dominant_side=dominant_side)
                logger.info(f"Swing analysis (2D) for dominant side {dominant_side}: {swing_analysis_results}")
            except Exception as e:
                logger.error(f"Error in evaluate_swing_rules: {str(e)} - Traceback: {traceback.format_exc()}")
                swing_analysis_results = {rule_key: False for rule_key in default_swing_rules_keys}
        else:
            logger.warning("Missing or incomplete required key points for swing evaluation. Defaulting swing analysis to False.")
            swing_analysis_results = {rule_key: False for rule_key in default_swing_rules_keys}


        logger.info(f"Returning {len(joint_data_per_frame_pydantic)} frames of 3D joint data and 2D swing analysis.")
        return VideoAnalysisResponseModel(
            total_frames=frame_count,
            joint_data_per_frame=joint_data_per_frame_pydantic,
            swing_analysis=swing_analysis_results
        )
    finally:
        if temp_video_path and os.path.exists(temp_video_path):
            try:
                os.unlink(temp_video_path)
                logger.info(f"Temporary video file {temp_video_path} deleted.")
            except Exception as e:
                logger.error(f"Error deleting temporary file {temp_video_path}: {e}")


@app.post("/analyze/technique_comparison/", response_model=ComparisonResultModel)
async def analyze_technique_comparison(data: TechniqueComparisonRequestDataModel):
    logger.info(f"Received 3D technique comparison request for dominant side: {data.dominant_side}")

    user_joint_data_raw_lists = transform_pydantic_to_numpy(data.user_video_frames)
    model_joint_data_raw_lists = transform_pydantic_to_numpy(data.model_video_frames)
    
    user_frame_count = len(data.user_video_frames)
    model_frame_count = len(data.model_video_frames)
    
    logger.info(f"Processing {user_frame_count} user video frames and {model_frame_count} model video frames for 3D comparison.")
    
    user_keypoints_3d_aligned = align_keypoints_with_interpolation(user_joint_data_raw_lists, user_frame_count)
    model_keypoints_3d_aligned = align_keypoints_with_interpolation(model_joint_data_raw_lists, model_frame_count)
    
    # --- Swing Analysis (using 2D slice) ---
    required_keys_for_swing = [ # Must match evaluate_swing_rules expectations
            'RightShoulder', 'LeftShoulder', 'RightElbow', 'LeftElbow', 'RightWrist',
            'RightHip', 'LeftHip', 'RightHeel', 'RightToe', 'LeftHeel', 'LeftToe'
    ]
    default_swing_rules_keys = list(ComparisonResultModel.model_fields['user_details'].default.keys() if ComparisonResultModel.model_fields['user_details'].default else [
            'shoulder_abduction', 'elbow_flexion', 'elbow_lower', 
            'foot_direction_aligned', 'proximal_to_distal_sequence', 
            'hip_forward_shift', 'trunk_rotation_completed'
        ])


    def get_2d_slice_for_swing(kp_3d_aligned: Dict[str, np.ndarray], frame_count_val: int, req_keys: List[str]) -> Tuple[Dict[str, np.ndarray], bool]:
        kp_2d: Dict[str, np.ndarray] = {}
        all_valid = True
        for k in req_keys:
            if k in kp_3d_aligned and kp_3d_aligned[k].shape[0] == frame_count_val and not np.all(np.isnan(kp_3d_aligned[k][:,:2])):
                kp_2d[k] = kp_3d_aligned[k][:, :2] # Slice X, Y
            else:
                logger.warning(f"Keypoint '{k}' missing or invalid for 2D slicing in comparison. Frames: {kp_3d_aligned.get(k, np.array([])).shape[0]}/{frame_count_val}")
                all_valid = False
                break
        return kp_2d, all_valid

    user_keypoints_2d, user_swing_data_valid = get_2d_slice_for_swing(user_keypoints_3d_aligned, user_frame_count, required_keys_for_swing)
    model_keypoints_2d, model_swing_data_valid = get_2d_slice_for_swing(model_keypoints_3d_aligned, model_frame_count, required_keys_for_swing)

    default_swing_details = {key: False for key in default_swing_rules_keys}

    user_swing_details = evaluate_swing_rules(user_keypoints_2d, data.dominant_side) if user_swing_data_valid else default_swing_details.copy()
    model_swing_details = evaluate_swing_rules(model_keypoints_2d, data.dominant_side) if model_swing_data_valid else default_swing_details.copy()
    
    num_criteria = len(default_swing_rules_keys) # Use the definitive list of rules
    
    user_correct_criteria = sum(1 for rule_key in default_swing_rules_keys if user_swing_details.get(rule_key, False))
    user_score = (user_correct_criteria / num_criteria) * 100.0 if num_criteria > 0 else 0.0

    model_correct_criteria = sum(1 for rule_key in default_swing_rules_keys if model_swing_details.get(rule_key, False))
    reference_score = (model_correct_criteria / num_criteria) * 100.0 if num_criteria > 0 else 0.0

    similarity = {}
    for rule_name in default_swing_rules_keys:
        similarity[rule_name] = (user_swing_details.get(rule_name, False) == model_swing_details.get(rule_name, False))
    
    logger.info(f"Comparison complete (using 2D for swing rules): User Score {user_score}, Reference Score {reference_score}")

    return ComparisonResultModel(
        user_score=user_score,
        reference_score=reference_score,
        similarity=similarity,
        user_details=user_swing_details, # Ensure this dict contains all keys from default_swing_rules_keys
        reference_details=model_swing_details # Ensure this dict contains all keys from default_swing_rules_keys
    )

@app.get("/")
async def root():
    return {"status": "MoveInsight Analysis Server is running", "version": "1.3.1 - 3D Pose Update with Robustness"}

if __name__ == "__main__":
    import uvicorn
    logger.info("Starting MoveInsight Analysis Server with 3D pose capabilities (v1.3.1)...")
    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")
</file>

<file path="MoveInsight/BodyPoseTypes.swift">
// MoveInsight/BodyPoseTypes.swift
import SwiftUI
import simd // For SIMD3<Float>

// MARK: - 2D Body Connection Structure (for 2D Overlays)
// Defines a connection between two body joints for drawing the 2D skeleton overlay.
// Uses String joint names that correspond to server/MediaPipe output.
struct StringBodyConnection: Identifiable {
    let id = UUID()
    let from: String // Joint name (e.g., "LeftShoulder")
    let to: String   // Joint name (e.g., "LeftElbow")
}

// MARK: - 3D Body Connection Structure (for SceneKit Rendering)
// Defines a connection between two body joints for drawing the 3D skeleton.
// Uses String joint names.
struct BodyConnection3D: Identifiable {
    let id = UUID()
    let from: String // Joint name (e.g., "LeftShoulder")
    let to: String   // Joint name (e.g., "LeftElbow")
}

// Standard set of connections for rendering a 3D skeleton.
let HumanBodyJoints: [BodyConnection3D] = [
    // Torso
    BodyConnection3D(from: "LeftShoulder", to: "RightShoulder"),
    BodyConnection3D(from: "LeftHip", to: "RightHip"),
    BodyConnection3D(from: "LeftShoulder", to: "LeftHip"),
    BodyConnection3D(from: "RightShoulder", to: "RightHip"),
    
    // Optional: Connect nose to a central point like a "Neck" if available,
    // or to mid-point of shoulders for a basic head representation.
    // BodyConnection3D(from: "Nose", to: "LeftShoulder"), // Example, adjust as needed
    // BodyConnection3D(from: "Nose", to: "RightShoulder"), // Example, adjust as needed

    // Left Arm
    BodyConnection3D(from: "LeftShoulder", to: "LeftElbow"),
    BodyConnection3D(from: "LeftElbow", to: "LeftWrist"),
    
    // Right Arm
    BodyConnection3D(from: "RightShoulder", to: "RightElbow"),
    BodyConnection3D(from: "RightElbow", to: "RightWrist"),

    // Left Leg
    BodyConnection3D(from: "LeftHip", to: "LeftKnee"),
    BodyConnection3D(from: "LeftKnee", to: "LeftAnkle"),
    // Optional finer details for feet:
    // BodyConnection3D(from: "LeftAnkle", to: "LeftHeel"),
    // BodyConnection3D(from: "LeftHeel", to: "LeftToe"), // "LeftToe" is often "LeftFootIndex" from MediaPipe

    // Right Leg
    BodyConnection3D(from: "RightHip", to: "RightKnee"),
    BodyConnection3D(from: "RightKnee", to: "RightAnkle"),
    // Optional finer details for feet:
    // BodyConnection3D(from: "RightAnkle", to: "RightHeel"),
    // BodyConnection3D(from: "RightHeel", to: "RightToe") // "RightToe" is often "RightFootIndex"
]


// MARK: - 3D Pose Point (String-based joint name)
// Represents a single 3D point for a joint.
struct Pose3DPoint: Identifiable { // Identifiable for use in SwiftUI lists if needed.
    let id = UUID()
    let jointName: String        // The name of the joint (e.g., "Nose", "LeftElbow")
    let position: SIMD3<Float>   // The 3D coordinates (x, y, z) of the joint

    init(jointName: String, position: SIMD3<Float>) {
        self.jointName = jointName
        self.position = position
    }
}

// MARK: - 3D Pose Body (String-based joints)
// Represents a complete 3D human pose for a single person/video source at a specific frame.
// This structure holds all detected 3D joint positions for one skeleton.
struct Pose3DBody: Identifiable {
    let id = UUID()
    // A dictionary mapping joint names (Strings) to their 3D positions (SIMD3<Float>).
    let joints: [String: SIMD3<Float>]
    // Indicates the source of this pose data (e.g., user's primary video or model/secondary video).
    let videoSource: VideoSource
    
    // Enum to differentiate between video sources, useful for coloring or offsetting skeletons.
    enum VideoSource {
        case primary   // Typically the user's video
        case secondary // Typically the model or comparison video
    }
    
    init(joints: [String: SIMD3<Float>], videoSource: VideoSource) {
        self.joints = joints
        self.videoSource = videoSource
    }
}
</file>

<file path="MoveInsight/TechniqueComparisonView.swift">
// MoveInsight/TechniqueComparisonView.swift
import SwiftUI
import AVKit
import Combine
import SceneKit // Import SceneKit for SceneView3D
import simd     // For SIMD3 used in poses

// Define ComparisonResult struct to match server's JSON response.
// This struct holds the results of the 2D swing analysis performed by the server.
struct ComparisonResult: Codable, Identifiable {
    // Add Identifiable conformance if needed, e.g., for use in ForEach directly with objects.
    // If not directly used in a ForEach that requires Identifiable on the struct itself,
    // `id` can be omitted, but ensure keys in ForEach loops are handled appropriately.
    let id = UUID() // Provides a unique ID, useful if these results are part of a list.
    
    let userScore: Double         // The user's overall technique score.
    let referenceScore: Double    // The model/reference performer's overall score.
    let similarity: [String: Bool] // Dictionary indicating if user matched model on specific criteria.
    let userDetails: [String: Bool]    // Detailed breakdown of user's performance on each criterion.
    let referenceDetails: [String: Bool] // Detailed breakdown of model's performance.

    // CodingKeys to map Swift property names to JSON keys from the server.
    enum CodingKeys: String, CodingKey {
        case userScore = "user_score"           // Maps "user_score" JSON key to userScore.
        case referenceScore = "reference_score" // Maps "reference_score" JSON key to referenceScore.
        case similarity                         // Assumes JSON key is "similarity".
        case userDetails = "user_details"       // Maps "user_details" JSON key to userDetails.
        case referenceDetails = "reference_details" // Maps "reference_details" JSON key to referenceDetails.
    }
}


// Example FeedbackItem if not defined elsewhere (for compilation):
 struct FeedbackItem: View {
     let title: String
     let description: String
     let score: Int // Represents a visual score (e.g., 0-100) for the item color

     var body: some View {
         HStack(alignment: .center, spacing: 16) {
             ZStack { // Score circle
                 Circle().fill(scoreColor.opacity(0.2)).frame(width: 40, height: 40)
                 Text("\(score)%").font(.system(size: 12, weight: .bold)).foregroundColor(scoreColor)
             }
             VStack(alignment: .leading, spacing: 4) { // Feedback text
                 Text(title).font(.subheadline).foregroundColor(ColorManager.textPrimary)
                 Text(description).font(.caption).foregroundColor(ColorManager.textSecondary).lineLimit(nil)
             }
             Spacer()
         }
     }
     private var scoreColor: Color { // Color based on score
         if score >= 90 { return .green }
         else if score >= 75 { return .yellow }
         else if score >= 60 { return .orange }
         else { return .red }
     }
 }


// MARK: - Main Comparison View
struct TechniqueComparisonView: View {
    let technique: BadmintonTechnique
    // ViewModels now manage 3D pose data internally.
    @ObservedObject var userVideoViewModel: VideoPlayerViewModel
    @ObservedObject var modelVideoViewModel: VideoPlayerViewModel
    
    // UI State
    @State private var comparisonMode: ComparisonMode = .sideBySide // Default comparison mode
    @State private var selectedReportTab: ReportTab = .overview     // Default report tab
    
    // Analysis Data (ComparisonResult is based on 2D swing analysis from server)
    @State var analysisResult: ComparisonResult?
    @State private var isAnalyzing = false // For local loading indicators if re-fetching
    @State private var analysisError: String? = nil
    @State private var cancellables = Set<AnyCancellable>()

    // State for video rectangles, used by the 2D side-by-side overlay.
    @State private var userVideoActualRect: CGRect = .zero
    @State private var modelVideoActualRect: CGRect = .zero

    // Delegate for SceneView3D to manage and persist camera state.
    @StateObject private var sceneDelegate = SceneKitViewDelegate()
    
    // Enum for switching between comparison modes.
    enum ComparisonMode {
        case sideBySide  // Shows 2D video players with 2D pose overlays.
        case overlay3D   // Shows 3D skeletons using SceneView3D.
    }
    
    // Enum for switching between report tabs.
    enum ReportTab {
        case overview
        case technical
    }
    
    var body: some View {
        ZStack {
            ColorManager.background.ignoresSafeArea() // Apply background color.
            
            ScrollView {
                VStack(spacing: 20) { // Adjusted spacing
                    // Header text for the technique being analyzed.
                    Text(LocalizedStringKey(String(format: NSLocalizedString("%@ Analysis", comment: ""), technique.name)))
                        .font(.title2).bold().foregroundColor(ColorManager.textPrimary).padding(.top, 16)
                    
                    // Picker to switch between "Side by Side" (2D) and "3D Overlay" modes.
                    Picker("Comparison Mode", selection: $comparisonMode) {
                        Text(LocalizedStringKey("Side by Side")).tag(ComparisonMode.sideBySide)
                        Text(LocalizedStringKey("3D Overlay")).tag(ComparisonMode.overlay3D)
                    }
                    .pickerStyle(SegmentedPickerStyle()).padding(.horizontal) // Use standard padding
                    
                    // Conditional view based on the selected comparison mode.
                    if comparisonMode == .sideBySide {
                        sideBySideComparisonView // Displays 2D video players with overlays.
                    } else { // .overlay3D
                        // Display 3D skeletons using SceneView3D.
                        SceneView3D(
                            userPose3D: userVideoViewModel.poses, // Pass current 3D pose from ViewModel
                            modelPose3D: modelVideoViewModel.poses, // Pass current 3D pose from ViewModel
                            bodyConnections: HumanBodyJoints, // Defined in BodyPoseTypes.swift
                            sceneDelegate: sceneDelegate        // Pass delegate for camera state
                        )
                        .frame(height: 400) // Define a suitable frame for the 3D view.
                        .cornerRadius(12)
                        .overlay(RoundedRectangle(cornerRadius: 12).stroke(ColorManager.accentColor.opacity(0.5), lineWidth: 1)) // Subtle border
                        .padding(.horizontal)
                        .onAppear { // Ensure videos are playing for SceneView3D to receive pose updates.
                            if !userVideoViewModel.isPlaying { userVideoViewModel.play() }
                            if !modelVideoViewModel.isPlaying { modelVideoViewModel.play() }
                        }
                        // Playback controls are shared for both modes now.
                        playbackControls().padding(.top, 8)
                    }
                    
                    // Picker for switching between "Overview" and "Technical" report tabs.
                    Picker("Report Type", selection: $selectedReportTab) {
                        Text(LocalizedStringKey("Overview")).tag(ReportTab.overview)
                        Text(LocalizedStringKey("Technical")).tag(ReportTab.technical)
                    }
                    .pickerStyle(SegmentedPickerStyle()).padding(.horizontal).padding(.top, 10) // Added top padding
                    
                    // Display content based on the selected report tab.
                    switch selectedReportTab {
                    case .overview: overviewAnalysisSection
                    case .technical: technicalAnalysisSection
                    }
                }
                .padding(.bottom, 32)
            }
        }
        .navigationTitle(LocalizedStringKey("Technique Analysis"))
        .navigationBarTitleDisplayMode(.inline)
        .onAppear {
            // Mute the model/secondary video by default. User's video is unmuted.
            modelVideoViewModel.player.isMuted = true
            userVideoViewModel.player.isMuted = false
            
            // Auto-play videos when the view appears.
            userVideoViewModel.play()
            modelVideoViewModel.play()

            // Log if analysisResult is nil (it should be passed from TechniqueDetailView).
            if analysisResult == nil && !isAnalyzing {
                 print("TechniqueComparisonView: analysisResult is nil on appear. Ensure it's passed from parent.")
            }
        }
        .onDisappear {
            // Pause videos and clean up Combine cancellables when the view disappears.
            userVideoViewModel.pause()
            modelVideoViewModel.pause()
            cancellables.forEach { $0.cancel() }
            cancellables.removeAll()
        }
    }
    
    // View for side-by-side 2D video comparison with overlays.
    private var sideBySideComparisonView: some View {
        VStack(spacing: 12) { // Adjusted spacing
            HStack(spacing: 8) {
                // User's video player card.
                videoPlayerCard(
                    title: LocalizedStringKey("Your Technique"),
                    viewModel: userVideoViewModel,
                    borderColor: .blue,
                    videoActualRectBinding: $userVideoActualRect // Binding for video rect
                )
                // Model's video player card.
                videoPlayerCard(
                    title: LocalizedStringKey("Model Technique"),
                    viewModel: modelVideoViewModel,
                    borderColor: .red,
                    videoActualRectBinding: $modelVideoActualRect // Binding for video rect
                )
            }
            .padding(.horizontal) // Use standard padding
            playbackControls().padding(.top, 8) // Shared playback controls
        }
    }

    // Helper to create a single video player card with its 2D overlay.
    private func videoPlayerCard(
        title: LocalizedStringKey,
        viewModel: VideoPlayerViewModel, // ViewModel now contains 3D poses
        borderColor: Color,
        videoActualRectBinding: Binding<CGRect> // Binding for the video's actual rect
    ) -> some View {
        VStack(spacing: 4) { // Reduced spacing
            Text(title).font(.caption).padding(.bottom, 2).foregroundColor(ColorManager.textPrimary)
            VideoPlayerRepresentable(player: viewModel.player, videoRect: videoActualRectBinding)
                .frame(height: 250) // Adjusted height
                .cornerRadius(10) // Slightly less corner radius
                .overlay(
                    // PoseOverlayView takes 3D poses from viewModel and renders a 2D projection
                    // using the videoActualRect for correct scaling and positioning.
                    PoseOverlayView(viewModel: viewModel, videoActualRect: videoActualRectBinding.wrappedValue)
                )
                .background(Color.black) // Ensure black bars for letter/pillarboxing.
                .overlay(RoundedRectangle(cornerRadius: 10).stroke(borderColor, lineWidth: 1.5)) // Adjusted border
        }
        .frame(maxWidth: .infinity)
    }

    // Shared playback controls for both 2D and 3D modes.
    private func playbackControls() -> some View {
        HStack(spacing: 25) { // Increased spacing for better touch targets
            // Restart button: restarts both videos and plays them.
            Button(action: {
                userVideoViewModel.restart(); modelVideoViewModel.restart()
                userVideoViewModel.play(); modelVideoViewModel.play()
            }) {
                Image(systemName: "backward.end.fill")
                    .imageScale(.large) // Make icon larger
            }
            
            // Play/Pause button: toggles playback for both videos.
            Button(action: {
                if userVideoViewModel.isPlaying {
                    userVideoViewModel.pause(); modelVideoViewModel.pause()
                } else {
                    userVideoViewModel.play(); modelVideoViewModel.play()
                }
            }) {
                Image(systemName: userVideoViewModel.isPlaying ? "pause.fill" : "play.fill")
                    .imageScale(.large) // Make icon larger
            }
        }
        .font(.title2) // Apply font to the HStack for consistent icon sizing if not overridden by imageScale
        .padding(.vertical, 8) // Add some vertical padding
        .foregroundColor(ColorManager.accentColor)
    }
    
    // MARK: - Analysis Display Sections
    // These sections display the 2D swing analysis results from `analysisResult`.
    // Their structure and content remain largely the same as before the 3D update,
    // as `ComparisonResult` is still based on 2D analysis.

    private var overviewAnalysisSection: some View {
        VStack(alignment: .leading, spacing: 16) { // Adjusted spacing
            Text(LocalizedStringKey("Analysis & Feedback")).font(.headline).foregroundColor(ColorManager.textPrimary)
            
            if isAnalyzing { loadingView(message: LocalizedStringKey("Analyzing your technique...")) }
            else if let error = analysisError { errorDisplayView(error) }
            else if let result = analysisResult { // Now `result` is of type ComparisonResult
                techniqueScoreView(score: result.userScore, title: LocalizedStringKey("Overall Technique Score"), subtitle: LocalizedStringKey("Compared to model performance"))
                feedbackDetailsView(details: result.userDetails) // userDetails from ComparisonResult
            } else { noAnalysisDataView() }
        }.padding(.horizontal)
    }

    private var technicalAnalysisSection: some View {
        VStack(alignment: .leading, spacing: 16) { // Adjusted spacing
            Text(LocalizedStringKey("Technical Report")).font(.headline).foregroundColor(ColorManager.textPrimary)

            if isAnalyzing { loadingView(message: LocalizedStringKey("Loading technical report...")) }
            else if let error = analysisError { errorDisplayView(error) }
            else if let result = analysisResult { // Now `result` is of type ComparisonResult
                scoresComparisonView(userScore: result.userScore, referenceScore: result.referenceScore)
                technicalElementsView(similarity: result.similarity, userDetails: result.userDetails, referenceDetails: result.referenceDetails)
                improvementSuggestionsView(userDetails: result.userDetails)
            } else { noAnalysisDataView(message: LocalizedStringKey("Technical analysis data not available.")) }
        }.padding(.horizontal)
    }
    
    // MARK: - Helper Views for Analysis Display (Implementations assumed from previous context or standard UI)
    private func loadingView(message: LocalizedStringKey) -> some View {
        HStack { Spacer(); VStack(spacing: 16) {
            ProgressView().progressViewStyle(CircularProgressViewStyle(tint: ColorManager.accentColor)).scaleEffect(1.5)
            Text(message).font(.subheadline).foregroundColor(ColorManager.textPrimary)
        }.padding(.vertical, 40); Spacer() }
    }
    private func errorDisplayView(_ error: String) -> some View {
        VStack(alignment: .center, spacing: 16) {
            Image(systemName: "exclamationmark.triangle.fill").font(.system(size: 40)).foregroundColor(.orange)
            Text(LocalizedStringKey("Analysis Error")).font(.headline).foregroundColor(ColorManager.textPrimary)
            Text(error).font(.subheadline).foregroundColor(ColorManager.textSecondary).multilineTextAlignment(.center).padding(.horizontal)
            Button(LocalizedStringKey("Retry")) {
                print("Retry tapped. Implement re-fetch logic if this view is responsible.")
            }
            .padding().buttonStyle(.borderedProminent).tint(ColorManager.accentColor)
        }
        .frame(maxWidth: .infinity).padding(20).background(ColorManager.cardBackground.opacity(0.7)).cornerRadius(12).padding(.horizontal)
    }
    private func noAnalysisDataView(message: LocalizedStringKey = LocalizedStringKey("Analysis data not available. Please ensure the video was processed.")) -> some View {
        VStack(spacing: 16) {
            Image(systemName: "info.circle").font(.largeTitle).foregroundColor(ColorManager.textSecondary)
            Text(message).font(.subheadline).foregroundColor(ColorManager.textSecondary)
                .padding(.horizontal, 20).multilineTextAlignment(.center)
            Button(LocalizedStringKey("Refresh Analysis")) {
                print("Refresh Analysis Tapped. Implement re-fetch logic.")
            }
            .padding().buttonStyle(.bordered).tint(ColorManager.accentColor)
        }
        .padding(.vertical, 30).frame(maxWidth: .infinity)
    }
    private func techniqueScoreView(score: Double, title: LocalizedStringKey, subtitle: LocalizedStringKey) -> some View {
        HStack {
            VStack(alignment: .leading, spacing: 4) {
                Text(title).font(.subheadline).fontWeight(.medium).foregroundColor(ColorManager.textPrimary)
                Text(subtitle).font(.caption).foregroundColor(ColorManager.textSecondary)
            }
            Spacer()
            scoreRingView(score: score, color: ColorManager.accentColor, size: 70)
        }
        .padding().background(ColorManager.cardBackground.opacity(0.8)).cornerRadius(10)
    }
    private func feedbackDetailsView(details: [String: Bool]) -> some View {
        VStack(alignment: .leading, spacing: 12) {
            Text(LocalizedStringKey("Key Technique Elements")).font(.subheadline).fontWeight(.medium).foregroundColor(ColorManager.textPrimary)
            ForEach(Array(details.keys.sorted()), id: \.self) { key in
                let passed = details[key] ?? false
                FeedbackItem(title: formatRuleName(key), description: getDescription(for: key, passed: passed), score: passed ? 95 : 65 )
            }
        }
        .padding().background(ColorManager.cardBackground.opacity(0.8)).cornerRadius(10)
    }
    private func scoresComparisonView(userScore: Double, referenceScore: Double) -> some View {
        VStack(alignment: .leading, spacing: 12) {
            Text(LocalizedStringKey("Scores: You vs. Model")).font(.subheadline).fontWeight(.medium).foregroundColor(ColorManager.textPrimary)
            HStack(spacing: 16) {
                scoreRingView(title: LocalizedStringKey("Your Score"), score: userScore, color: .blue, size: 65)
                scoreRingView(title: LocalizedStringKey("Model Score"), score: referenceScore, color: .red, size: 65)
            }.frame(maxWidth: .infinity)
        }
        .padding().background(ColorManager.cardBackground.opacity(0.8)).cornerRadius(10)
    }
    private func scoreRingView(title: LocalizedStringKey? = nil, score: Double, color: Color, size: CGFloat) -> some View {
        VStack(alignment: .center, spacing: 6) {
            if let title = title { Text(title).font(.caption2).foregroundColor(ColorManager.textSecondary) }
            ZStack {
                Circle().stroke(color.opacity(0.25), lineWidth: size * 0.1).frame(width: size, height: size)
                Circle().trim(from: 0, to: CGFloat(score / 100.0))
                    .stroke(color, style: StrokeStyle(lineWidth: size * 0.1, lineCap: .round))
                    .frame(width: size, height: size).rotationEffect(.degrees(-90))
                Text("\(Int(score))").font(.system(size: size * 0.3, weight: .semibold)).foregroundColor(ColorManager.textPrimary)
            }
        }.frame(maxWidth: .infinity)
    }
    private func technicalElementsView(similarity: [String: Bool], userDetails: [String: Bool], referenceDetails: [String: Bool]) -> some View {
        VStack(alignment: .leading, spacing: 10) {
            Text(LocalizedStringKey("Technical Elements Breakdown")).font(.subheadline).fontWeight(.medium).foregroundColor(ColorManager.textPrimary)
            HStack {
                Text(LocalizedStringKey("Element")).font(.caption.weight(.semibold)).foregroundColor(ColorManager.textSecondary).frame(maxWidth: .infinity, alignment: .leading)
                Text(LocalizedStringKey("You")).font(.caption.weight(.semibold)).foregroundColor(ColorManager.textSecondary).frame(width: 50, alignment: .center)
                Text(LocalizedStringKey("Model")).font(.caption.weight(.semibold)).foregroundColor(ColorManager.textSecondary).frame(width: 50, alignment: .center)
            }.padding(.bottom, 2)
            ForEach(Array(similarity.keys.sorted()), id: \.self) { key in
                let userPassed = userDetails[key] ?? false; let modelPassed = referenceDetails[key] ?? false
                HStack {
                    Text(formatRuleName(key)).font(.caption).foregroundColor(ColorManager.textPrimary).frame(maxWidth: .infinity, alignment: .leading)
                    Image(systemName: userPassed ? "checkmark.circle.fill" : "xmark.circle.fill").foregroundColor(userPassed ? .green : .orange).frame(width: 50, alignment: .center)
                    Image(systemName: modelPassed ? "checkmark.circle.fill" : "xmark.circle.fill").foregroundColor(modelPassed ? .green : .orange).frame(width: 50, alignment: .center)
                }.padding(.vertical, 4).background((userPassed == modelPassed) ? Color.clear : Color.yellow.opacity(0.1)).cornerRadius(3)
            }
        }
        .padding().background(ColorManager.cardBackground.opacity(0.8)).cornerRadius(10)
    }
    private func improvementSuggestionsView(userDetails: [String: Bool]) -> some View {
        VStack(alignment: .leading, spacing: 8) {
            Text(LocalizedStringKey("Improvement Suggestions")).font(.subheadline).fontWeight(.medium).foregroundColor(ColorManager.textPrimary)
            let improvementAreas = userDetails.filter { !$0.value }.keys.sorted()
            if improvementAreas.isEmpty {
                Text(LocalizedStringKey("Excellent! All key technical elements are performed correctly.")).font(.caption).foregroundColor(.green).padding(.vertical, 4)
            } else {
                ForEach(improvementAreas, id: \.self) { key in
                    HStack(alignment: .top) {
                        Image(systemName: "exclamationmark.circle").foregroundColor(.orange).font(.caption)
                        Text(getDescription(for: key, passed: false)).font(.caption).foregroundColor(ColorManager.textPrimary)
                    }
                }
            }
        }
        .padding().background(ColorManager.cardBackground.opacity(0.8)).cornerRadius(10)
    }

    // Helper functions for formatting text.
    private func formatRuleName(_ rule: String) -> String {
        let localizedKey = NSLocalizedString(rule, comment: "Technical term from server (e.g., shoulder_abduction)")
        return localizedKey == rule ? rule.replacingOccurrences(of: "_", with: " ").capitalized : localizedKey
    }
    private func getDescription(for rule: String, passed: Bool) -> String {
        let baseMessage = formatRuleName(rule)
        let formatKey = passed ? "%@: Well done!" : "%@: Focus on improving this aspect. Check tutorials for guidance."
        return String(format: NSLocalizedString(formatKey, comment: "Feedback string format"), baseMessage)
    }
}
</file>

<file path="MoveInsight/UploadTabView.swift">
import SwiftUI
import PhotosUI
import AVKit

struct UploadTabView: View {
    @State private var navigateToTechniquesList = false
    
    var body: some View {
        NavigationView {
            ZStack {
                ColorManager.background.ignoresSafeArea()
                
                VStack(spacing: 30) {
                    // Header
                    Text(LocalizedStringKey("Upload Video"))
                        .font(.title)
                        .foregroundColor(ColorManager.textPrimary)
                        .padding(.top, 40)
                    
                    Text(LocalizedStringKey("Choose the type of video you want to upload"))
                        .font(.subheadline)
                        .foregroundColor(ColorManager.textSecondary)
                        .multilineTextAlignment(.center)
                        .padding(.horizontal, 32)
                    
                    Spacer()
                    
                    // Technique Video Option
                    NavigationLink(destination: TechniquesListView(), isActive: $navigateToTechniquesList) {
                        EmptyView()
                    }
                    
                    Button(action: {
                        navigateToTechniquesList = true
                    }) {
                        UploadOptionCard(
                            title: LocalizedStringKey("Upload Technique Video"),
                            description: LocalizedStringKey("Analyze and compare your badminton techniques with model performers"),
                            icon: "figure.badminton"
                        )
                    }
                    .buttonStyle(ScaleButtonStyle())
                    
                    // Match Video Option
                    Button(action: {
                        // Do nothing for now, as per requirements
                    }) {
                        UploadOptionCard(
                            title: LocalizedStringKey("Upload Match Video"),
                            description: LocalizedStringKey("Upload your match videos for performance analysis"),
                            icon: "sportscourt"
                        )
                    }
                    .buttonStyle(ScaleButtonStyle())
                    
                    Spacer()
                }
                .padding(.horizontal, 20)
            }
            .navigationBarHidden(true)
        }
    }
}

// Card view for upload options
struct UploadOptionCard: View {
    let title: LocalizedStringKey
    let description: LocalizedStringKey
    let icon: String
    
    var body: some View {
        HStack(spacing: 20) {
            // Icon
            ZStack {
                Circle()
                    .fill(ColorManager.accentColor.opacity(0.2))
                    .frame(width: 70, height: 70)
                
                Image(systemName: icon)
                    .font(.system(size: 30))
                    .foregroundColor(ColorManager.accentColor)
            }
            
            // Text content
            VStack(alignment: .leading, spacing: 8) {
                Text(title)
                    .font(.headline)
                    .foregroundColor(ColorManager.textPrimary)
                
                Text(description)
                    .font(.subheadline)
                    .foregroundColor(ColorManager.textSecondary)
                    .lineLimit(2)
            }
            
            Spacer()
            
            // Arrow
            Image(systemName: "chevron.right")
                .foregroundColor(ColorManager.textSecondary)
        }
        .padding(20)
        .background(
            RoundedRectangle(cornerRadius: 16)
                .fill(ColorManager.cardBackground)
        )
    }
}

struct ScaleButtonStyle: ButtonStyle {
    func makeBody(configuration: Configuration) -> some View {
        configuration.label
            .scaleEffect(configuration.isPressed ? 0.97 : 1.0)
            .animation(.easeInOut(duration: 0.2), value: configuration.isPressed)
    }
}
</file>

<file path="MoveInsight/VideoPlayerViewModel.swift">
// MoveInsight/VideoPlayerViewModel.swift
import SwiftUI
import AVKit
import Combine
import simd // For SIMD3<Float>

// MARK: - Video Player View Model
class VideoPlayerViewModel: ObservableObject {
    let videoURL: URL
    let player: AVPlayer
    let asset: AVAsset

    // Published properties to update the UI
    // Poses will now be [String: SIMD3<Float>] to store 3D coordinates from the server.
    @Published var poses: [String: SIMD3<Float>] = [:]
    // accumulatedPoses stores all frames' 3D poses after processing from server.
    @Published var accumulatedPoses: [[String: SIMD3<Float>]] = []
    
    // Store the original server frames for potential debugging or re-processing.
    @Published var originalServerFrames: [ServerFrameData] = []

    // bodyConnections are used for drawing the 2D overlay skeleton.
    @Published var bodyConnections: [StringBodyConnection] = []
    
    @Published var isVideoReady = false
    @Published var isPlaying = false
    @Published var videoOrientation: CGImagePropertyOrientation = .up // For potential orientation adjustments
    
    private(set) var videoSize: CGSize = .zero // Actual video dimensions
    let videoSource: Pose3DBody.VideoSource // To distinguish between user and model videos
    
    private var displayLink: CADisplayLink?
    private var playerItemStatusObserver: NSKeyValueObservation?
    private var didPlayToEndTimeObserver: NSObjectProtocol?
    private var cancellables = Set<AnyCancellable>()

    init(videoURL: URL, videoSource: Pose3DBody.VideoSource) {
        self.videoURL = videoURL
        self.asset = AVAsset(url: videoURL)
        self.player = AVPlayer() // Initialize player
        self.videoSource = videoSource
        print("VideoPlayerViewModel (3D enabled) initialized with URL: \(videoURL.lastPathComponent) for source: \(videoSource)")
        setupStringBodyConnectionsFor2DOverlay() // Setup connections for 2D overlay
        prepareVideoPlayback() // Asynchronously prepare video
    }
    
    // Called to populate the ViewModel with 3D pose data from the server.
    func setServerProcessedJoints(_ serverFrames: [ServerFrameData]) {
        self.originalServerFrames = serverFrames // Store the raw server response

        var allFramesPoses3D: [[String: SIMD3<Float>]] = []
        for serverFrameData in serverFrames {
            var singleFramePoses3D: [String: SIMD3<Float>] = [:]
            for (jointNameString, jointData) in serverFrameData.joints {
                // Convert server's x, y, z (Double?) to SIMD3<Float>.
                // Server's x,y are normalized (0-1). Z's scale is relative to MediaPipe's world coordinates.
                // The client-side SceneView3D will handle appropriate scaling of Z for rendering.
                let x = Float(jointData.x)
                let y = Float(jointData.y)
                let z = Float(jointData.z ?? 0.0) // Default z to 0.0 if server sends nil
                singleFramePoses3D[jointNameString] = SIMD3<Float>(x, y, z)
            }
            if !singleFramePoses3D.isEmpty { // Only add if there are joints for this frame
                allFramesPoses3D.append(singleFramePoses3D)
            }
        }
        
        DispatchQueue.main.async {
            self.accumulatedPoses = allFramesPoses3D
            print("Successfully set \(self.accumulatedPoses.count) frames of 3D joint data for source: \(self.videoSource).")
            // If video is at the beginning and poses are loaded, set the first frame's pose.
            if !self.accumulatedPoses.isEmpty && self.player.currentTime() == .zero {
                 self.poses = self.accumulatedPoses[0]
            }
        }
    }

    // Defines the connections for the 2D skeleton overlay.
    private func setupStringBodyConnectionsFor2DOverlay() {
        // These connections use standard MediaPipe joint names (or remapped ones like "LeftToe").
        // Ensure these names match what the server provides and what PoseOverlayView expects.
        bodyConnections = [
            // Torso
            StringBodyConnection(from: "Nose", to: "LeftShoulder"),
            StringBodyConnection(from: "Nose", to: "RightShoulder"),
            StringBodyConnection(from: "LeftShoulder", to: "RightShoulder"),
            StringBodyConnection(from: "LeftShoulder", to: "LeftHip"),
            StringBodyConnection(from: "RightShoulder", to: "RightHip"),
            StringBodyConnection(from: "LeftHip", to: "RightHip"),

            // Left Arm
            StringBodyConnection(from: "LeftShoulder", to: "LeftElbow"),
            StringBodyConnection(from: "LeftElbow", to: "LeftWrist"),
            
            // Right Arm
            StringBodyConnection(from: "RightShoulder", to: "RightElbow"),
            StringBodyConnection(from: "RightElbow", to: "RightWrist"),

            // Left Leg
            StringBodyConnection(from: "LeftHip", to: "LeftKnee"),
            StringBodyConnection(from: "LeftKnee", to: "LeftAnkle"),
            StringBodyConnection(from: "LeftAnkle", to: "LeftHeel"),
            StringBodyConnection(from: "LeftHeel", to: "LeftToe"), // "LeftToe" is often "LeftFootIndex" from MediaPipe

            // Right Leg
            StringBodyConnection(from: "RightHip", to: "RightKnee"),
            StringBodyConnection(from: "RightKnee", to: "RightAnkle"),
            StringBodyConnection(from: "RightAnkle", to: "RightHeel"),
            StringBodyConnection(from: "RightHeel", to: "RightToe") // "RightToe" is often "RightFootIndex"
        ]
    }

    // Asynchronously loads video properties and sets up the player item.
    private func prepareVideoPlayback() {
        Task { [weak self] in // Use weak self to avoid retain cycles
            guard let self = self else { return }

            do {
                // Load video tracks to get orientation and size
                let tracks = try await self.asset.load(.tracks)
                if let videoTrack = tracks.first(where: { $0.mediaType == .video }) {
                    let transform = try await videoTrack.load(.preferredTransform)
                    self.videoOrientation = self.orientation(from: transform) // Determine video orientation
                    self.videoSize = try await videoTrack.load(.naturalSize) // Get natural video size
                    
                    // Switch to main thread to update UI-related properties and setup player
                    await MainActor.run {
                        self.setupPlayerItemAndObservers()
                    }
                } else {
                    print("Error: No video track found in asset for \(self.videoSource) - URL: \(self.videoURL.lastPathComponent).")
                    await MainActor.run { self.isVideoReady = false }
                }
            } catch {
                print("Error loading asset tracks or properties for \(self.videoSource) - URL: \(self.videoURL.lastPathComponent): \(error)")
                await MainActor.run { self.isVideoReady = false }
            }
        }
    }
    
    // Sets up the AVPlayerItem and observers for playback status.
    private func setupPlayerItemAndObservers() {
        let playerItem = AVPlayerItem(asset: asset)
        
        // Observe player item status to know when it's ready to play
        playerItemStatusObserver = playerItem.observe(\.status, options: [.new, .initial]) { [weak self] item, _ in
            guard let self = self else { return }
            DispatchQueue.main.async {
                switch item.status {
                case .readyToPlay:
                    self.isVideoReady = true
                    self.setupDisplayLink() // Start display link for frame-by-frame updates
                    print("PlayerItem ready for \(self.videoSource) (3D). Video size: \(self.videoSize). URL: \(self.videoURL.lastPathComponent)")
                    // If poses are already loaded and video is at start, display the first pose
                    if !self.accumulatedPoses.isEmpty && self.player.currentTime().seconds == 0 {
                        self.poses = self.accumulatedPoses[0]
                    }
                case .failed:
                    let errorDesc = item.error?.localizedDescription ?? "Unknown error"
                    print("PlayerItem failed for \(self.videoSource) (URL: \(self.videoURL.lastPathComponent)): \(errorDesc).")
                    self.isVideoReady = false
                default: // .unknown
                    self.isVideoReady = false
                }
            }
        }

        // Observe when video plays to the end
        didPlayToEndTimeObserver = NotificationCenter.default.addObserver(
            forName: .AVPlayerItemDidPlayToEndTime,
            object: playerItem,
            queue: .main
        ) { [weak self] _ in
            self?.isPlaying = false
            // Optionally, seek to zero to allow replay:
            // self?.player.seek(to: .zero)
        }
        
        player.replaceCurrentItem(with: playerItem)
        player.isMuted = (videoSource == .secondary) // Mute model/secondary video by default
        player.allowsExternalPlayback = true
    }

    // Sets up the CADisplayLink for synchronizing pose updates with screen refresh rate.
    private func setupDisplayLink() {
        displayLink?.invalidate() // Invalidate existing display link if any
        displayLink = CADisplayLink(target: self, selector: #selector(displayLinkDidFire))
        displayLink?.add(to: .main, forMode: .common) // Add to main run loop
        displayLink?.isPaused = !isPlaying // Pause if not playing
    }

    // Called by CADisplayLink on each screen refresh.
    @objc private func displayLinkDidFire(_ link: CADisplayLink) {
        // Only update poses if player is playing and pose data is available.
        guard player.timeControlStatus == .playing, !accumulatedPoses.isEmpty else {
            return
        }
        
        let currentTime = player.currentTime()
        // Ensure video track and frame rate are available for accurate frame indexing.
        guard let videoTrack = asset.tracks(withMediaType: .video).first,
              videoTrack.nominalFrameRate > 0 else {
            // If no track info, clear current poses to avoid displaying stale data.
            if !poses.isEmpty { DispatchQueue.main.async { self.poses = [:] } }
            return
        }
        
        let frameRate = videoTrack.nominalFrameRate
        let currentFrameIndex = Int(CMTimeGetSeconds(currentTime) * Double(frameRate))

        // Update current poses if the frame index is valid.
        if currentFrameIndex >= 0 && currentFrameIndex < accumulatedPoses.count {
            let newPoseForCurrentFrame = accumulatedPoses[currentFrameIndex]
            // Only publish update if the pose has actually changed to prevent redundant UI updates.
            if self.poses != newPoseForCurrentFrame {
                 DispatchQueue.main.async { // Ensure UI updates are on the main thread
                    self.poses = newPoseForCurrentFrame
                 }
            }
        } else {
            // Frame index is out of bounds (e.g., video ended or data issue). Clear current poses.
            if !poses.isEmpty { DispatchQueue.main.async { self.poses = [:] } }
        }
    }
    
    // Determines video orientation from its transform.
    private func orientation(from transform: CGAffineTransform) -> CGImagePropertyOrientation {
        if transform.a == 0 && transform.b == 1.0 && transform.c == -1.0 && transform.d == 0 { return .right }
        if transform.a == 0 && transform.b == -1.0 && transform.c == 1.0 && transform.d == 0 { return .left }
        if transform.a == -1.0 && transform.b == 0 && transform.c == 0 && transform.d == -1.0 { return .down }
        return .up // Default orientation
    }

    // MARK: - Playback Controls
    func play() {
        if isVideoReady && !isPlaying {
            player.play()
            isPlaying = true
            displayLink?.isPaused = false // Resume display link
        }
    }

    func pause() {
        if isPlaying {
            player.pause()
            isPlaying = false
            displayLink?.isPaused = true // Pause display link
            // Trigger one last pose update on pause to ensure the correct frame's pose is displayed.
            if let currentDisplayLink = displayLink {
                displayLinkDidFire(currentDisplayLink)
            }
        }
    }

    func togglePlayPause() {
        if isPlaying { pause() } else { play() }
    }
    
    func restart() {
        player.seek(to: .zero) { [weak self] finished in
            guard let self = self else { return }
            if finished {
                // Immediately update to the first frame's pose after seeking.
                if !self.accumulatedPoses.isEmpty {
                    DispatchQueue.main.async {
                        self.poses = self.accumulatedPoses[0]
                    }
                }
                self.play() // Start playing after seeking to beginning
            }
        }
    }
    
    // Clears all loaded pose data.
    func clearAllPoseData() {
        DispatchQueue.main.async {
            self.accumulatedPoses.removeAll()
            self.poses.removeAll()
            self.originalServerFrames.removeAll()
            print("Cleared all 3D pose data for source: \(self.videoSource). URL: \(self.videoURL.lastPathComponent)")
        }
    }

    // MARK: - Cleanup
    // Call this method when the ViewModel is no longer needed to release resources.
    func cleanup() {
        print("VideoPlayerViewModel (3D) cleanup initiated for \(videoSource). URL: \(self.videoURL.lastPathComponent)")
        pause() // Ensure player and display link are paused
        player.replaceCurrentItem(with: nil) // Release current player item
        
        displayLink?.invalidate() // Stop and remove display link
        displayLink = nil
        
        playerItemStatusObserver?.invalidate() // Remove KVO
        playerItemStatusObserver = nil
        
        if let observer = didPlayToEndTimeObserver { // Remove notification observer
            NotificationCenter.default.removeObserver(observer)
            didPlayToEndTimeObserver = nil
        }
        
        cancellables.forEach { $0.cancel() } // Cancel any Combine subscriptions
        cancellables.removeAll()
        
        clearAllPoseData() // Clear pose data arrays
        print("VideoPlayerViewModel (3D) cleaned up for \(videoSource). URL: \(self.videoURL.lastPathComponent)")
    }

    deinit {
        print("VideoPlayerViewModel (3D) deinit for \(videoSource). URL: \(self.videoURL.lastPathComponent)")
        // Cleanup should ideally be called explicitly by the owner of this ViewModel.
        // However, as a safeguard, attempt cleanup here too.
        cleanup()
    }
}

// Ensure Pose3DBody.VideoSource is defined, e.g., in BodyPoseTypes.swift
// enum VideoSource { case primary, secondary }
// struct Pose3DBody { let videoSource: VideoSource; /* ... other properties ... */ }
</file>

<file path="MoveInsight.xcodeproj/project.pbxproj">
// !$*UTF8*$!
{
	archiveVersion = 1;
	classes = {
	};
	objectVersion = 77;
	objects = {

/* Begin PBXFileReference section */
		CB1C3FAE2D9D493B008FC7AB /* MoveInsight.app */ = {isa = PBXFileReference; explicitFileType = wrapper.application; includeInIndex = 0; path = MoveInsight.app; sourceTree = BUILT_PRODUCTS_DIR; };
/* End PBXFileReference section */

/* Begin PBXFileSystemSynchronizedBuildFileExceptionSet section */
		CB6878622DD191DB00A36A08 /* Exceptions for "MoveInsight" folder in "MoveInsight" target */ = {
			isa = PBXFileSystemSynchronizedBuildFileExceptionSet;
			membershipExceptions = (
				Info.plist,
			);
			target = CB1C3FAD2D9D493B008FC7AB /* MoveInsight */;
		};
/* End PBXFileSystemSynchronizedBuildFileExceptionSet section */

/* Begin PBXFileSystemSynchronizedRootGroup section */
		CB1C3FB02D9D493B008FC7AB /* MoveInsight */ = {
			isa = PBXFileSystemSynchronizedRootGroup;
			exceptions = (
				CB6878622DD191DB00A36A08 /* Exceptions for "MoveInsight" folder in "MoveInsight" target */,
			);
			path = MoveInsight;
			sourceTree = "<group>";
		};
/* End PBXFileSystemSynchronizedRootGroup section */

/* Begin PBXFrameworksBuildPhase section */
		CB1C3FAB2D9D493B008FC7AB /* Frameworks */ = {
			isa = PBXFrameworksBuildPhase;
			buildActionMask = 2147483647;
			files = (
			);
			runOnlyForDeploymentPostprocessing = 0;
		};
/* End PBXFrameworksBuildPhase section */

/* Begin PBXGroup section */
		CB1C3FA52D9D493B008FC7AB = {
			isa = PBXGroup;
			children = (
				CB1C3FB02D9D493B008FC7AB /* MoveInsight */,
				CB1C3FAF2D9D493B008FC7AB /* Products */,
			);
			sourceTree = "<group>";
		};
		CB1C3FAF2D9D493B008FC7AB /* Products */ = {
			isa = PBXGroup;
			children = (
				CB1C3FAE2D9D493B008FC7AB /* MoveInsight.app */,
			);
			name = Products;
			sourceTree = "<group>";
		};
/* End PBXGroup section */

/* Begin PBXNativeTarget section */
		CB1C3FAD2D9D493B008FC7AB /* MoveInsight */ = {
			isa = PBXNativeTarget;
			buildConfigurationList = CB1C3FBC2D9D493D008FC7AB /* Build configuration list for PBXNativeTarget "MoveInsight" */;
			buildPhases = (
				CB1C3FAA2D9D493B008FC7AB /* Sources */,
				CB1C3FAB2D9D493B008FC7AB /* Frameworks */,
				CB1C3FAC2D9D493B008FC7AB /* Resources */,
			);
			buildRules = (
			);
			dependencies = (
			);
			fileSystemSynchronizedGroups = (
				CB1C3FB02D9D493B008FC7AB /* MoveInsight */,
			);
			name = MoveInsight;
			packageProductDependencies = (
			);
			productName = MoveInsight;
			productReference = CB1C3FAE2D9D493B008FC7AB /* MoveInsight.app */;
			productType = "com.apple.product-type.application";
		};
/* End PBXNativeTarget section */

/* Begin PBXProject section */
		CB1C3FA62D9D493B008FC7AB /* Project object */ = {
			isa = PBXProject;
			attributes = {
				BuildIndependentTargetsInParallel = 1;
				LastSwiftUpdateCheck = 1610;
				LastUpgradeCheck = 1610;
				TargetAttributes = {
					CB1C3FAD2D9D493B008FC7AB = {
						CreatedOnToolsVersion = 16.1;
					};
				};
			};
			buildConfigurationList = CB1C3FA92D9D493B008FC7AB /* Build configuration list for PBXProject "MoveInsight" */;
			developmentRegion = en;
			hasScannedForEncodings = 0;
			knownRegions = (
				en,
				Base,
				"zh-Hans",
			);
			mainGroup = CB1C3FA52D9D493B008FC7AB;
			minimizedProjectReferenceProxies = 1;
			preferredProjectObjectVersion = 77;
			productRefGroup = CB1C3FAF2D9D493B008FC7AB /* Products */;
			projectDirPath = "";
			projectRoot = "";
			targets = (
				CB1C3FAD2D9D493B008FC7AB /* MoveInsight */,
			);
		};
/* End PBXProject section */

/* Begin PBXResourcesBuildPhase section */
		CB1C3FAC2D9D493B008FC7AB /* Resources */ = {
			isa = PBXResourcesBuildPhase;
			buildActionMask = 2147483647;
			files = (
			);
			runOnlyForDeploymentPostprocessing = 0;
		};
/* End PBXResourcesBuildPhase section */

/* Begin PBXSourcesBuildPhase section */
		CB1C3FAA2D9D493B008FC7AB /* Sources */ = {
			isa = PBXSourcesBuildPhase;
			buildActionMask = 2147483647;
			files = (
			);
			runOnlyForDeploymentPostprocessing = 0;
		};
/* End PBXSourcesBuildPhase section */

/* Begin XCBuildConfiguration section */
		CB1C3FBA2D9D493D008FC7AB /* Debug */ = {
			isa = XCBuildConfiguration;
			buildSettings = {
				ALWAYS_SEARCH_USER_PATHS = NO;
				ASSETCATALOG_COMPILER_GENERATE_SWIFT_ASSET_SYMBOL_EXTENSIONS = YES;
				CLANG_ANALYZER_NONNULL = YES;
				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
				CLANG_ENABLE_MODULES = YES;
				CLANG_ENABLE_OBJC_ARC = YES;
				CLANG_ENABLE_OBJC_WEAK = YES;
				CLANG_WARN_BLOCK_CAPTURE_AUTORELEASING = YES;
				CLANG_WARN_BOOL_CONVERSION = YES;
				CLANG_WARN_COMMA = YES;
				CLANG_WARN_CONSTANT_CONVERSION = YES;
				CLANG_WARN_DEPRECATED_OBJC_IMPLEMENTATIONS = YES;
				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
				CLANG_WARN_EMPTY_BODY = YES;
				CLANG_WARN_ENUM_CONVERSION = YES;
				CLANG_WARN_INFINITE_RECURSION = YES;
				CLANG_WARN_INT_CONVERSION = YES;
				CLANG_WARN_NON_LITERAL_NULL_CONVERSION = YES;
				CLANG_WARN_OBJC_IMPLICIT_RETAIN_SELF = YES;
				CLANG_WARN_OBJC_LITERAL_CONVERSION = YES;
				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
				CLANG_WARN_QUOTED_INCLUDE_IN_FRAMEWORK_HEADER = YES;
				CLANG_WARN_RANGE_LOOP_ANALYSIS = YES;
				CLANG_WARN_STRICT_PROTOTYPES = YES;
				CLANG_WARN_SUSPICIOUS_MOVE = YES;
				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
				CLANG_WARN_UNREACHABLE_CODE = YES;
				CLANG_WARN__DUPLICATE_METHOD_MATCH = YES;
				COPY_PHASE_STRIP = NO;
				DEBUG_INFORMATION_FORMAT = dwarf;
				ENABLE_STRICT_OBJC_MSGSEND = YES;
				ENABLE_TESTABILITY = YES;
				ENABLE_USER_SCRIPT_SANDBOXING = YES;
				GCC_C_LANGUAGE_STANDARD = gnu17;
				GCC_DYNAMIC_NO_PIC = NO;
				GCC_NO_COMMON_BLOCKS = YES;
				GCC_OPTIMIZATION_LEVEL = 0;
				GCC_PREPROCESSOR_DEFINITIONS = (
					"DEBUG=1",
					"$(inherited)",
				);
				GCC_WARN_64_TO_32_BIT_CONVERSION = YES;
				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
				GCC_WARN_UNDECLARED_SELECTOR = YES;
				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
				GCC_WARN_UNUSED_FUNCTION = YES;
				GCC_WARN_UNUSED_VARIABLE = YES;
				IPHONEOS_DEPLOYMENT_TARGET = 18.1;
				LOCALIZATION_PREFERS_STRING_CATALOGS = YES;
				MTL_ENABLE_DEBUG_INFO = INCLUDE_SOURCE;
				MTL_FAST_MATH = YES;
				ONLY_ACTIVE_ARCH = YES;
				SDKROOT = iphoneos;
				SWIFT_ACTIVE_COMPILATION_CONDITIONS = "DEBUG $(inherited)";
				SWIFT_OPTIMIZATION_LEVEL = "-Onone";
			};
			name = Debug;
		};
		CB1C3FBB2D9D493D008FC7AB /* Release */ = {
			isa = XCBuildConfiguration;
			buildSettings = {
				ALWAYS_SEARCH_USER_PATHS = NO;
				ASSETCATALOG_COMPILER_GENERATE_SWIFT_ASSET_SYMBOL_EXTENSIONS = YES;
				CLANG_ANALYZER_NONNULL = YES;
				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
				CLANG_ENABLE_MODULES = YES;
				CLANG_ENABLE_OBJC_ARC = YES;
				CLANG_ENABLE_OBJC_WEAK = YES;
				CLANG_WARN_BLOCK_CAPTURE_AUTORELEASING = YES;
				CLANG_WARN_BOOL_CONVERSION = YES;
				CLANG_WARN_COMMA = YES;
				CLANG_WARN_CONSTANT_CONVERSION = YES;
				CLANG_WARN_DEPRECATED_OBJC_IMPLEMENTATIONS = YES;
				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
				CLANG_WARN_EMPTY_BODY = YES;
				CLANG_WARN_ENUM_CONVERSION = YES;
				CLANG_WARN_INFINITE_RECURSION = YES;
				CLANG_WARN_INT_CONVERSION = YES;
				CLANG_WARN_NON_LITERAL_NULL_CONVERSION = YES;
				CLANG_WARN_OBJC_IMPLICIT_RETAIN_SELF = YES;
				CLANG_WARN_OBJC_LITERAL_CONVERSION = YES;
				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
				CLANG_WARN_QUOTED_INCLUDE_IN_FRAMEWORK_HEADER = YES;
				CLANG_WARN_RANGE_LOOP_ANALYSIS = YES;
				CLANG_WARN_STRICT_PROTOTYPES = YES;
				CLANG_WARN_SUSPICIOUS_MOVE = YES;
				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
				CLANG_WARN_UNREACHABLE_CODE = YES;
				CLANG_WARN__DUPLICATE_METHOD_MATCH = YES;
				COPY_PHASE_STRIP = NO;
				DEBUG_INFORMATION_FORMAT = "dwarf-with-dsym";
				ENABLE_NS_ASSERTIONS = NO;
				ENABLE_STRICT_OBJC_MSGSEND = YES;
				ENABLE_USER_SCRIPT_SANDBOXING = YES;
				GCC_C_LANGUAGE_STANDARD = gnu17;
				GCC_NO_COMMON_BLOCKS = YES;
				GCC_WARN_64_TO_32_BIT_CONVERSION = YES;
				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
				GCC_WARN_UNDECLARED_SELECTOR = YES;
				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
				GCC_WARN_UNUSED_FUNCTION = YES;
				GCC_WARN_UNUSED_VARIABLE = YES;
				IPHONEOS_DEPLOYMENT_TARGET = 18.1;
				LOCALIZATION_PREFERS_STRING_CATALOGS = YES;
				MTL_ENABLE_DEBUG_INFO = NO;
				MTL_FAST_MATH = YES;
				SDKROOT = iphoneos;
				SWIFT_COMPILATION_MODE = wholemodule;
				VALIDATE_PRODUCT = YES;
			};
			name = Release;
		};
		CB1C3FBD2D9D493D008FC7AB /* Debug */ = {
			isa = XCBuildConfiguration;
			buildSettings = {
				ASSETCATALOG_COMPILER_APPICON_NAME = AppIcon;
				ASSETCATALOG_COMPILER_GLOBAL_ACCENT_COLOR_NAME = AccentColor;
				CODE_SIGN_STYLE = Automatic;
				CURRENT_PROJECT_VERSION = 2;
				DEVELOPMENT_ASSET_PATHS = "\"MoveInsight/Preview Content\"";
				DEVELOPMENT_TEAM = 2VS9GH9QTB;
				ENABLE_PREVIEWS = YES;
				GENERATE_INFOPLIST_FILE = YES;
				INFOPLIST_FILE = MoveInsight/Info.plist;
				INFOPLIST_KEY_NSPhotoLibraryUsageDescription = "Needs permission to upload video";
				INFOPLIST_KEY_UIApplicationSceneManifest_Generation = YES;
				INFOPLIST_KEY_UIApplicationSupportsIndirectInputEvents = YES;
				INFOPLIST_KEY_UILaunchScreen_Generation = YES;
				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPad = "UIInterfaceOrientationPortrait UIInterfaceOrientationPortraitUpsideDown UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPhone = "UIInterfaceOrientationPortrait UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
				IPHONEOS_DEPLOYMENT_TARGET = 17.0;
				LD_RUNPATH_SEARCH_PATHS = (
					"$(inherited)",
					"@executable_path/Frameworks",
				);
				MARKETING_VERSION = 1.0;
				PRODUCT_BUNDLE_IDENTIFIER = com.MoveInsight;
				PRODUCT_NAME = "$(TARGET_NAME)";
				SUPPORTED_PLATFORMS = "iphoneos iphonesimulator";
				SUPPORTS_MACCATALYST = NO;
				SUPPORTS_MAC_DESIGNED_FOR_IPHONE_IPAD = NO;
				SUPPORTS_XR_DESIGNED_FOR_IPHONE_IPAD = NO;
				SWIFT_EMIT_LOC_STRINGS = YES;
				SWIFT_VERSION = 5.0;
				TARGETED_DEVICE_FAMILY = "1,2";
			};
			name = Debug;
		};
		CB1C3FBE2D9D493D008FC7AB /* Release */ = {
			isa = XCBuildConfiguration;
			buildSettings = {
				ASSETCATALOG_COMPILER_APPICON_NAME = AppIcon;
				ASSETCATALOG_COMPILER_GLOBAL_ACCENT_COLOR_NAME = AccentColor;
				CODE_SIGN_STYLE = Automatic;
				CURRENT_PROJECT_VERSION = 2;
				DEVELOPMENT_ASSET_PATHS = "\"MoveInsight/Preview Content\"";
				DEVELOPMENT_TEAM = 2VS9GH9QTB;
				ENABLE_PREVIEWS = YES;
				GENERATE_INFOPLIST_FILE = YES;
				INFOPLIST_FILE = MoveInsight/Info.plist;
				INFOPLIST_KEY_NSPhotoLibraryUsageDescription = "Needs permission to upload video";
				INFOPLIST_KEY_UIApplicationSceneManifest_Generation = YES;
				INFOPLIST_KEY_UIApplicationSupportsIndirectInputEvents = YES;
				INFOPLIST_KEY_UILaunchScreen_Generation = YES;
				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPad = "UIInterfaceOrientationPortrait UIInterfaceOrientationPortraitUpsideDown UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPhone = "UIInterfaceOrientationPortrait UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
				IPHONEOS_DEPLOYMENT_TARGET = 17.0;
				LD_RUNPATH_SEARCH_PATHS = (
					"$(inherited)",
					"@executable_path/Frameworks",
				);
				MARKETING_VERSION = 1.0;
				PRODUCT_BUNDLE_IDENTIFIER = com.MoveInsight;
				PRODUCT_NAME = "$(TARGET_NAME)";
				SUPPORTED_PLATFORMS = "iphoneos iphonesimulator";
				SUPPORTS_MACCATALYST = NO;
				SUPPORTS_MAC_DESIGNED_FOR_IPHONE_IPAD = NO;
				SUPPORTS_XR_DESIGNED_FOR_IPHONE_IPAD = NO;
				SWIFT_EMIT_LOC_STRINGS = YES;
				SWIFT_VERSION = 5.0;
				TARGETED_DEVICE_FAMILY = "1,2";
			};
			name = Release;
		};
/* End XCBuildConfiguration section */

/* Begin XCConfigurationList section */
		CB1C3FA92D9D493B008FC7AB /* Build configuration list for PBXProject "MoveInsight" */ = {
			isa = XCConfigurationList;
			buildConfigurations = (
				CB1C3FBA2D9D493D008FC7AB /* Debug */,
				CB1C3FBB2D9D493D008FC7AB /* Release */,
			);
			defaultConfigurationIsVisible = 0;
			defaultConfigurationName = Release;
		};
		CB1C3FBC2D9D493D008FC7AB /* Build configuration list for PBXNativeTarget "MoveInsight" */ = {
			isa = XCConfigurationList;
			buildConfigurations = (
				CB1C3FBD2D9D493D008FC7AB /* Debug */,
				CB1C3FBE2D9D493D008FC7AB /* Release */,
			);
			defaultConfigurationIsVisible = 0;
			defaultConfigurationName = Release;
		};
/* End XCConfigurationList section */
	};
	rootObject = CB1C3FA62D9D493B008FC7AB /* Project object */;
}
</file>

</files>
