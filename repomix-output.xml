This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
MoveInsight/
  Assets.xcassets/
    AccentColor.colorset/
      Contents.json
    AppIcon.appiconset/
      Contents.json
    player_shuttle_distance.imageset/
      Contents.json
    velocity_profile_chart.imageset/
      Contents.json
    Contents.json
  DepthAnythingV2SmallF16.mlpackage/
    Manifest.json
  en.lproj/
    Localizable.strings
  Preview Content/
    Preview Assets.xcassets/
      Contents.json
  zh-Hans.lproj/
    Localizable.strings
  BodyPoseTypes.swift
  ColorManager.swift
  CombinedVideo3DView.swift
  ContentView.swift
  CustomTabBar.swift
  DepthEstimationService.swift
  Extensions.swift
  HomeView.swift
  Info.plist
  ModelVideoLoader.swift
  MoveInsightApp.swift
  Pose3DProcessor.swift
  SceneView3D.swift
  TechniqueAnalysisService.swift
  TechniqueComparisonView.swift
  TechniqueDetailView.swift
  TechniquesListView.swift
  TechniqueVideoUploadView.swift
  UIComponents.swift
  UploadTabView.swift
  VideoPlayerRepresentable.swift
  VideoPlayerViewModel.swift
  VideoTransferUtils.swift
MoveInsight.xcodeproj/
  project.xcworkspace/
    contents.xcworkspacedata
  xcuserdata/
    charlie.xcuserdatad/
      xcschemes/
        xcschememanagement.plist
  project.pbxproj
MoveInsightServer/
  analysis_server.py
  requirements.txt
  swing_diagnose.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="MoveInsightServer/requirements.txt">
absl-py==2.2.2
annotated-types==0.7.0
anyio==4.9.0
attrs==25.3.0
cffi==1.17.1
click==8.2.0
contourpy==1.3.2
cycler==0.12.1
fastapi==0.115.12
flatbuffers==25.2.10
fonttools==4.58.0
h11==0.16.0
idna==3.10
jax==0.6.0
jaxlib==0.6.0
kiwisolver==1.4.8
matplotlib==3.10.3
mediapipe==0.10.21
ml_dtypes==0.5.1
numpy==1.26.4
opencv-contrib-python==4.11.0.86
opencv-python==4.11.0.86
opt_einsum==3.4.0
packaging==25.0
pillow==11.2.1
protobuf==4.25.7
pycparser==2.22
pydantic==2.11.4
pydantic_core==2.33.2
pyparsing==3.2.3
python-dateutil==2.9.0.post0
python-multipart==0.0.20
scipy==1.15.3
sentencepiece==0.2.0
setuptools==78.1.1
six==1.17.0
sniffio==1.3.1
sounddevice==0.5.1
starlette==0.46.2
typing-inspection==0.4.0
typing_extensions==4.13.2
uvicorn==0.34.2
wheel==0.45.1
</file>

<file path="MoveInsightServer/swing_diagnose.py">
# 需要判断的7个技术指标如下：
#   1. **shoulder_abduction**: 不要掉肘；
#   2. **elbow_flexion**: 肘关节收紧；
#   3. **elbow_lower**: 持拍手的肘部低于非持拍手的肘部；
#   4. **foot_direction_aligned**: 双脚的脚尖方向是否一致；
#   5. **proximal_to_distal_sequence**: 挥拍阶段上肢发力的顺序是否正确；
#   6. **hip_forward_shift**: 挥拍阶段髋部前移；
#   7. **trunk_rotation_completed**: 挥拍阶段躯干转体是否充分。

import numpy as np
def evaluate_swing_rules(keypoints, dominant_side='Right'):
    """
    KEYPOINTS 输入说明：

    本系统使用的关键点数据格式为 Python 字典（dict），名为 `keypoints`，其结构如下：

    keypoints = {
        'RightShoulder': np.ndarray of shape (T, 2),
        'LeftShoulder':  np.ndarray of shape (T, 2),
        'RightElbow':    np.ndarray of shape (T, 2),
        'LeftElbow':     np.ndarray of shape (T, 2),
        'RightWrist':    np.ndarray of shape (T, 2),
        'RightHand':     np.ndarray of shape (T, 2),  # 可选
        'RightHip':      np.ndarray of shape (T, 2),
        'LeftHip':       np.ndarray of shape (T, 2),
        'RightHeel':     np.ndarray of shape (T, 2),
        'RightToe':      np.ndarray of shape (T, 2),
        'LeftHeel':      np.ndarray of shape (T, 2),
        'LeftToe':       np.ndarray of shape (T, 2)
    }

    说明：
    - 每个 key 是关键点的英文全称；
    - 每个 value 是一个 numpy 数组，形状为 (T, 2)，其中：
        - T 表示视频的帧数；
        - 每一行是该帧的 (X, Y) 坐标；
        - 坐标单位可为像素或归一化值，但整组数据应保持一致。

    数据来源：
    - 可通过 Apple Vision, MediaPipe, OpenPose 等人体姿态识别工具生成；
    - 你应根据模型输出映射 keypoint 编号，统一重命名为上述英文标准名；
    - 可在中间添加字典映射，例如：
        joint_map = {0: 'RightShoulder', 1: 'RightElbow', ...}
    """
    result = {}
    D = dominant_side
    ND = 'Left' if D == 'Right' else 'Right'
    T = keypoints[f'{D}Shoulder'].shape[0]

    def angle_between(v1, v2):
        unit_v1 = v1 / (np.linalg.norm(v1) + 1e-6)
        unit_v2 = v2 / (np.linalg.norm(v2) + 1e-6)
        dot = np.clip(np.dot(unit_v1, unit_v2), -1.0, 1.0)
        return np.degrees(np.arccos(dot))

    def joint_angle(A, B, C):
        v1 = A - B
        v2 = C - B
        return angle_between(v1, v2)

    # Direction-based phase split using elbow movement
    elbow_x_vel = np.zeros(T)
    elbow_x_vel[1:] = keypoints[f'{D}Elbow'][1:, 0] - keypoints[f'{D}Elbow'][:-1, 0]

    # Apply smoothing to reduce noise (elbow is more active than shoulder, so medium window)
    window_size = min(5, T - 1)
    if window_size > 2:
        smoothed_vel = np.convolve(elbow_x_vel, np.ones(window_size) / window_size, mode='valid')
        padding = np.zeros(T - len(smoothed_vel))
        smoothed_vel = np.concatenate((padding, smoothed_vel))
    else:
        smoothed_vel = elbow_x_vel

    # Find the point where velocity changes from negative to positive
    # This indicates the elbow stops moving backward and starts moving forward
    swing_start = 0
    for t in range(1, T):
        if smoothed_vel[t - 1] < 0 and smoothed_vel[t] > 0:
            swing_start = t
            break

    # Fallback if no clear turning point found
    if swing_start == 0:
        vel_threshold = np.max(np.abs(smoothed_vel)) * 0.15
        for t in range(1, T):
            if smoothed_vel[t] > vel_threshold:
                swing_start = t
                break

        if swing_start == 0:
            swing_start = T // 2

    swing_end = T - 1

    # Keep the degenerate case check
    if swing_start >= swing_end:
        result.update({
            'shoulder_abduction': False,
            'elbow_flexion': False,
            'elbow_lower': False,
            'foot_direction_aligned': False,
            'proximal_to_distal_sequence': False,
            'hip_forward_shift': False,
            'trunk_rotation_completed': False
        })
        return result

    # 引拍期
    shoulder_angles = []
    elbow_angles = []
    for t in range(swing_start):
        shoulder_angles.append(joint_angle(keypoints[f'{D}Hip'][t], keypoints[f'{D}Shoulder'][t], keypoints[f'{D}Elbow'][t]))
        elbow_angles.append(joint_angle(keypoints[f'{D}Shoulder'][t], keypoints[f'{D}Elbow'][t], keypoints[f'{D}Wrist'][t]))

    result['shoulder_abduction'] = np.any((np.array(shoulder_angles) >= 60) & (np.array(shoulder_angles) <= 90))
    result['elbow_flexion'] = np.any(np.array(elbow_angles) < 90)
    result['elbow_lower'] = np.any(keypoints[f'{D}Elbow'][:swing_start, 1] > keypoints[f'{ND}Elbow'][:swing_start, 1])

    # foot direction
    v_r = keypoints['RightToe'][:swing_start] - keypoints['RightHeel'][:swing_start]
    v_l = keypoints['LeftToe'][:swing_start] - keypoints['LeftHeel'][:swing_start]
    foot_angles = [angle_between(vr, vl) for vr, vl in zip(v_r, v_l)]
    result['foot_direction_aligned'] = np.any(np.array(foot_angles) < 30)

    # 挥拍期
    def get_angle_series(A, B, C):
        return np.array([joint_angle(A[t], B[t], C[t]) for t in range(T)])

    shoulder_seq = get_angle_series(keypoints[f'{D}Hip'], keypoints[f'{D}Shoulder'], keypoints[f'{D}Elbow'])
    elbow_seq = get_angle_series(keypoints[f'{D}Shoulder'], keypoints[f'{D}Elbow'], keypoints[f'{D}Wrist'])
    wrist_seq = get_angle_series(keypoints[f'{D}Elbow'], keypoints[f'{D}Wrist'],
                                 keypoints[f'{D}Hand'] if f'{D}Hand' in keypoints else keypoints[f'{D}Wrist'])

    vel_shoulder = np.abs(np.diff(shoulder_seq))
    vel_elbow = np.abs(np.diff(elbow_seq))
    vel_wrist = np.abs(np.diff(wrist_seq))

    result['proximal_to_distal_sequence'] = False
    if swing_start < len(vel_shoulder):
        t_s = np.argmax(vel_shoulder[swing_start:]) + swing_start
        t_e = np.argmax(vel_elbow[swing_start:]) + swing_start
        t_w = np.argmax(vel_wrist[swing_start:]) + swing_start
        result['proximal_to_distal_sequence'] = t_s < t_e #手腕的识别不准先去掉了，只看了肩和肘的

    hip_center_x = 0.5 * (keypoints[f'{D}Hip'][:, 0] + keypoints[f'{ND}Hip'][:, 0])
    hip_range = np.max(hip_center_x) - np.min(hip_center_x)
    result['hip_forward_shift'] = hip_range >= 0.10

    dist_init = np.linalg.norm(keypoints[f'{D}Shoulder'][swing_start] - keypoints[f'{ND}Shoulder'][swing_start])
    dist_min = np.min([
        np.linalg.norm(keypoints[f'{D}Shoulder'][t] - keypoints[f'{ND}Shoulder'][t])
        for t in range(swing_start, swing_end + 1)
    ])
    result['trunk_rotation_completed'] = (dist_init - dist_min) / (dist_init + 1e-6) >= 0.5

    return result

def align_keypoints_with_interpolation(joint_data, frame_count):
    keypoints = {}

    for name, points in joint_data.items():
        pts = np.array(points)  # shape: (n, 2)
        n = len(pts)

        if n == frame_count:
            keypoints[name] = pts
        elif n >= frame_count - 2:
            # ⚠️ 允许最多缺 1-2 帧，尝试修复

            # 创建一个 shape=(frame_count, 2) 的空数组
            filled = np.full((frame_count, 2), np.nan)

            # 填入已知帧（假设 joint_data 是顺序连续 append 的）
            valid_indices = np.linspace(0, frame_count - 1, n, dtype=int)
            filled[valid_indices] = pts

            # 找出缺失帧，用线性插值补齐
            for dim in range(2):  # 对X和Y分别插值
                valid = ~np.isnan(filled[:, dim])
                if np.sum(valid) < 2:
                    print(f"❌ 无法插值 {name}，有效点太少")
                    break
                filled[:, dim] = np.interp(
                    np.arange(frame_count),
                    np.where(valid)[0],
                    filled[valid, dim]
                )

            keypoints[name] = filled

        else:
            print(f"⚠️ {name} 缺失太多（{n}/{frame_count}），丢弃该关键点")

    return keypoints

# #########################################测试函数功能############################################
T = 100
def gen_mock_data(x_shift=0.0):
    # Linear motion with optional shift
    return np.stack([np.linspace(0, 1, T) + x_shift, np.linspace(0.5, 0.5, T)], axis=1)

keypoints = {
    'RightShoulder': gen_mock_data(0.1),
    'LeftShoulder': gen_mock_data(-0.1),
    'RightElbow': gen_mock_data(0.2),
    'LeftElbow': gen_mock_data(-0.2),
    'RightWrist': gen_mock_data(0.3),
    'RightHand': gen_mock_data(0.35),
    'RightHip': gen_mock_data(0.0),
    'LeftHip': gen_mock_data(-0.05),
    'RightToe': gen_mock_data(0.2),
    'RightHeel': gen_mock_data(0.1),
    'LeftToe': gen_mock_data(-0.2),
    'LeftHeel': gen_mock_data(-0.1),
}

# Evaluate rules
results = evaluate_swing_rules(keypoints, dominant_side='Right')
print(results)
</file>

<file path="MoveInsight/Assets.xcassets/AccentColor.colorset/Contents.json">
{
  "colors" : [
    {
      "idiom" : "universal"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}
</file>

<file path="MoveInsight/Assets.xcassets/player_shuttle_distance.imageset/Contents.json">
{
  "images" : [
    {
      "filename" : "Screenshot 2025-04-25 at 20.31.05.png",
      "idiom" : "universal",
      "scale" : "1x"
    },
    {
      "idiom" : "universal",
      "scale" : "2x"
    },
    {
      "idiom" : "universal",
      "scale" : "3x"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}
</file>

<file path="MoveInsight/Assets.xcassets/velocity_profile_chart.imageset/Contents.json">
{
  "images" : [
    {
      "filename" : "velocity_profile_chart.png",
      "idiom" : "universal",
      "scale" : "1x"
    },
    {
      "idiom" : "universal",
      "scale" : "2x"
    },
    {
      "idiom" : "universal",
      "scale" : "3x"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}
</file>

<file path="MoveInsight/Assets.xcassets/Contents.json">
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}
</file>

<file path="MoveInsight/DepthAnythingV2SmallF16.mlpackage/Manifest.json">
{
    "fileFormatVersion": "1.0.0",
    "itemInfoEntries": {
        "20D05BA4-5503-4673-8F11-11EF6A25EE9D": {
            "author": "com.apple.CoreML",
            "description": "CoreML Model Weights",
            "name": "weights",
            "path": "com.apple.CoreML/weights"
        },
        "D698F85A-E9D9-47BF-8C1B-5A441F139CAA": {
            "author": "com.apple.CoreML",
            "description": "CoreML Model Specification",
            "name": "model.mlmodel",
            "path": "com.apple.CoreML/model.mlmodel"
        }
    },
    "rootModelIdentifier": "D698F85A-E9D9-47BF-8C1B-5A441F139CAA"
}
</file>

<file path="MoveInsight/en.lproj/Localizable.strings">
// Main navigation
"Home" = "Home";
"Training" = "Training";
"Videos" = "Videos";
"Messages" = "Messages";

// Upload functionality
"Upload Video" = "Upload Video";
"Uploading..." = "Uploading...";
"Close" = "Close";
"Upload Match Video" = "Upload Match Video";
"Upload Training Video" = "Upload Training Video";
"Match Video" = "Match Video";
"Training Video" = "Training Video";
"Loading Video..." = "Loading Video...";
"Preparing video..." = "Preparing video...";

// Permissions
"Photo Library Access Required" = "Photo Library Access Required";
"Permission to access your photo library is required to upload videos. Please grant access in Settings." = "Permission to access your photo library is required to upload videos. Please grant access in Settings.";
"Go to Settings" = "Go to Settings";
"Cancel" = "Cancel";

// Home View
"Good morning," = "Good morning,";
"Match Performance" = "Match Performance";
"/ last week" = "/ last week";
"Well done on swing path!" = "Well done on swing path!";
"Match History" = "Match History";
"Technicals" = "Technicals";
"Training Goals" = "Training Goals";
"Tutorials Specifically For You" = "Tutorials Specifically For You";

// Chart months
"Jan" = "Jan";
"Feb" = "Feb";
"Mar" = "Mar";
"Apr" = "Apr";
"May" = "May";

// Placeholder screens
"Training Screen" = "Training Screen";
"Upload Screen" = "Upload Screen";
"Videos Screen" = "Videos Screen";
"Messages Screen" = "Messages Screen";

// Numeric values (formats can be localized)
"2.3%" = "2.3%";
"4.3" = "4.3";

// Details screens
"Technicals Detail" = "Technicals Detail";
"Training Goals Detail" = "Training Goals Detail";

// Techniques List View
"Badminton Techniques" = "Badminton Techniques";
"Select a technique to upload and analyze your form" = "Select a technique to upload and analyze your form";
"Techniques" = "Techniques";

// Technique Names
"Backhand Clear" = "Backhand Clear";
"Underhand Clear" = "Underhand Clear";
"Overhead Clear" = "Overhead Clear";
"Drop Shot" = "Drop Shot";
"Smash" = "Smash";
"Net Shot" = "Net Shot";

// Technique Descriptions
"A clear shot played with the back of the hand facing forward." = "A clear shot played with the back of the hand facing forward.";
"A defensive shot played from below waist height, sending the shuttle high to the back of the opponent's court." = "A defensive shot played from below waist height, sending the shuttle high to the back of the opponent's court.";
"A powerful shot played from above the head, sending the shuttle to the back of the opponent's court." = "A powerful shot played from above the head, sending the shuttle to the back of the opponent's court.";
"A gentle shot that just clears the net and drops sharply on the other side." = "A gentle shot that just clears the net and drops sharply on the other side.";
"A powerful overhead shot hit steeply downward into the opponent's court." = "A powerful overhead shot hit steeply downward into the opponent's court.";
"A soft shot played near the net that just clears it and falls close to the net on the other side." = "A soft shot played near the net that just clears it and falls close to the net on the other side.";

// Technique Detail View
"Upload Your %@ Video" = "Upload Your %@ Video";
"Upload Second Video for Comparison" = "Upload Second Video for Comparison";
"Your Analyzed Video" = "Your Analyzed Video";
"Next Steps" = "Next Steps";
"Compare with Model Video" = "Compare with Model Video";
"Re-upload Primary Video" = "Re-upload Primary Video";
"Key Points for %@" = "Key Points for %@";
"Start with proper stance, feet shoulder-width apart." = "Start with proper stance, feet shoulder-width apart.";
"Grip the racket with a relaxed, comfortable hold." = "Grip the racket with a relaxed, comfortable hold.";
"Maintain balance throughout the motion." = "Maintain balance throughout the motion.";
"Keep your eye on the shuttle at all times." = "Keep your eye on the shuttle at all times.";
"Follow through with your swing for better control." = "Follow through with your swing for better control.";
"Processing video..." = "Processing video...";
"Uploading & Analyzing Primary Video..." = "Uploading & Analyzing Primary Video...";
"Uploading & Analyzing Comparison Video..." = "Uploading & Analyzing Comparison Video...";
"Processing Model Video..." = "Processing Model Video...";
"Error" = "Error";
"Try Upload Again" = "Try Upload Again";

// Technique Video Upload View
"Upload Comparison Video" = "Upload Comparison Video";
"Upload %@ Video" = "Upload %@ Video";
"Select a video of yourself performing the %@ technique." = "Select a video of yourself performing the %@ technique.";
"Select Video from Library" = "Select Video from Library";
"For best results, ensure your entire body is visible, and you are performing the technique from start to finish." = "For best results, ensure your entire body is visible, and you are performing the technique from start to finish.";

// Technique Comparison View
"%@ Analysis" = "%@ Analysis";
"Side by Side" = "Side by Side";
"3D Overlay" = "3D Overlay";
"Your Technique" = "Your Technique";
"Model Technique" = "Model Technique";
"Technique Analysis" = "Technique Analysis";
"3D Overlay (Conceptual)" = "3D Overlay (Conceptual)";
"Note: 3D rendering is currently simplified." = "Note: 3D rendering is currently simplified.";
"Analysis & Feedback" = "Analysis & Feedback";
"Analyzing your technique..." = "Analyzing your technique...";
"Overall Technique Score" = "Overall Technique Score";
"Compared to model performance" = "Compared to model performance";
"Technical Report" = "Technical Report";
"Loading technical report..." = "Loading technical report...";
"Scores: You vs. Model" = "Scores: You vs. Model";
"Your Score" = "Your Score";
"Model Score" = "Model Score";
"Technical Elements Breakdown" = "Technical Elements Breakdown";
"Element" = "Element";
"You" = "You";
"Model" = "Model";
"Improvement Suggestions" = "Improvement Suggestions";
"Excellent! All key technical elements are performed correctly." = "Excellent! All key technical elements are performed correctly.";
"Key Technique Elements" = "Key Technique Elements";
"Analysis data not available. Please ensure the video was processed." = "Analysis data not available. Please ensure the video was processed.";
"Technical analysis data not available." = "Technical analysis data not available.";
"Refresh Analysis" = "Refresh Analysis";
"Analysis Error" = "Analysis Error";
"Retry" = "Retry";

// String formatting for technique analysis
"%@: Well done!" = "%@: Well done!";
"%@: Focus on improving this aspect. Check tutorials for guidance." = "%@: Focus on improving this aspect. Check tutorials for guidance.";

// Upload Tab View
"Choose the type of video you want to upload" = "Choose the type of video you want to upload";
"Upload Technique Video" = "Upload Technique Video";
"Analyze and compare your badminton techniques with model performers" = "Analyze and compare your badminton techniques with model performers";
"Upload your match videos for performance analysis" = "Upload your match videos for performance analysis";

// Technical strings - converting for proper display
"shoulder_abduction" = "Shoulder Abduction";
"elbow_flexion" = "Elbow Flexion";
"elbow_lower" = "Elbow Position";
"foot_direction_aligned" = "Foot Direction";
"proximal_to_distal_sequence" = "Movement Sequence";
"hip_forward_shift" = "Hip Movement";
"trunk_rotation_completed" = "Trunk Rotation";
</file>

<file path="MoveInsight/Preview Content/Preview Assets.xcassets/Contents.json">
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}
</file>

<file path="MoveInsight/zh-Hans.lproj/Localizable.strings">
// Main navigation
"Home" = "首页";
"Training" = "训练";
"Videos" = "视频";
"Messages" = "消息";

// Upload functionality
"Upload Video" = "上传视频";
"Uploading..." = "上传中...";
"Close" = "关闭";
"Upload Match Video" = "上传比赛视频";
"Upload Training Video" = "上传训练视频";
"Match Video" = "比赛视频";
"Training Video" = "训练视频";
"Loading Video..." = "加载视频中...";
"Preparing video..." = "准备视频中...";

// Permissions
"Photo Library Access Required" = "需要访问照片库";
"Permission to access your photo library is required to upload videos. Please grant access in Settings." = "上传视频需要访问您的照片库。请在设置中授予权限。";
"Go to Settings" = "前往设置";
"Cancel" = "取消";

// Home View
"Good morning," = "早上好，";
"Match Performance" = "比赛表现";
"/ last week" = "/ 上周";
"Well done on swing path!" = "挥杆轨迹很棒！";
"Match History" = "比赛历史";
"Technicals" = "技术分析";
"Training Goals" = "训练目标";
"Tutorials Specifically For You" = "专为您定制的教程";

// Chart months
"Jan" = "一月";
"Feb" = "二月";
"Mar" = "三月";
"Apr" = "四月";
"May" = "五月";

// Placeholder screens
"Training Screen" = "训练界面";
"Upload Screen" = "上传界面";
"Videos Screen" = "视频界面";
"Messages Screen" = "消息界面";

// Numeric values (formats can be localized)
"2.3%" = "2.3%";
"4.3" = "4.3";

// Details screens
"Technicals Detail" = "技术分析详情";
"Training Goals Detail" = "训练目标详情";

// Techniques List View
"Badminton Techniques" = "羽毛球技术";
"Select a technique to upload and analyze your form" = "选择一项技术上传并分析您的表现";
"Techniques" = "技术";

// Technique Names
"Backhand Clear" = "反手高远球";
"Underhand Clear" = "正手高远球";
"Overhead Clear" = "头顶高远球";
"Drop Shot" = "吊球";
"Smash" = "杀球";
"Net Shot" = "网前球";

// Technique Descriptions
"A clear shot played with the back of the hand facing forward." = "一种以手背朝前的方式击打的高远球。";
"A defensive shot played from below waist height, sending the shuttle high to the back of the opponent's court." = "一种从腰部以下高度击打的防守球，将球高高地送到对方场地后方。";
"A powerful shot played from above the head, sending the shuttle to the back of the opponent's court." = "一种从头顶上方击打的有力球，将球送到对方场地后方。";
"A gentle shot that just clears the net and drops sharply on the other side." = "一种轻柔地刚好越过网，在对方场地陡然下落的球。";
"A powerful overhead shot hit steeply downward into the opponent's court." = "一种从高处陡然向下击打进入对方场地的有力球。";
"A soft shot played near the net that just clears it and falls close to the net on the other side." = "一种在网前轻柔地击打，刚好越过网并在对方网前落下的球。";

// Technique Detail View
"Upload Your %@ Video" = "上传您的%@视频";
"Upload Second Video for Comparison" = "上传第二个视频进行比较";
"Your Analyzed Video" = "您的分析视频";
"Next Steps" = "下一步";
"Compare with Model Video" = "与模范视频比较";
"Re-upload Primary Video" = "重新上传主视频";
"Key Points for %@" = "%@的关键点";
"Start with proper stance, feet shoulder-width apart." = "以正确的姿势开始，双脚与肩同宽。";
"Grip the racket with a relaxed, comfortable hold." = "以放松、舒适的方式握拍。";
"Maintain balance throughout the motion." = "在整个动作过程中保持平衡。";
"Keep your eye on the shuttle at all times." = "始终保持眼睛注视球。";
"Follow through with your swing for better control." = "跟随挥拍动作以获得更好的控制。";
"Processing video..." = "处理视频中...";
"Uploading & Analyzing Primary Video..." = "上传并分析主视频中...";
"Uploading & Analyzing Comparison Video..." = "上传并分析比较视频中...";
"Processing Model Video..." = "处理模范视频中...";
"Error" = "错误";
"Try Upload Again" = "重新尝试上传";

// Technique Video Upload View
"Upload Comparison Video" = "上传比较视频";
"Upload %@ Video" = "上传%@视频";
"Select a video of yourself performing the %@ technique." = "选择一个您自己执行%@技术的视频。";
"Select Video from Library" = "从相册中选择视频";
"For best results, ensure your entire body is visible, and you are performing the technique from start to finish." = "为了获得最佳效果，请确保您的整个身体可见，并且从头到尾完成技术动作。";

// Technique Comparison View
"%@ Analysis" = "%@分析";
"Side by Side" = "并排对比";
"3D Overlay" = "3D叠加";
"Your Technique" = "您的技术";
"Model Technique" = "模范技术";
"Technique Analysis" = "技术分析";
"3D Overlay (Conceptual)" = "3D叠加（概念性）";
"Note: 3D rendering is currently simplified." = "注意：3D渲染目前已简化。";
"Analysis & Feedback" = "分析与反馈";
"Analyzing your technique..." = "分析您的技术中...";
"Overall Technique Score" = "整体技术得分";
"Compared to model performance" = "与模范表现相比";
"Technical Report" = "技术报告";
"Loading technical report..." = "加载技术报告中...";
"Scores: You vs. Model" = "得分：您 vs 模范";
"Your Score" = "您的得分";
"Model Score" = "模范得分";
"Technical Elements Breakdown" = "技术元素分解";
"Element" = "元素";
"You" = "您";
"Model" = "模范";
"Improvement Suggestions" = "改进建议";
"Excellent! All key technical elements are performed correctly." = "太棒了！所有关键技术元素的执行都是正确的。";
"Key Technique Elements" = "关键技术元素";
"Analysis data not available. Please ensure the video was processed." = "分析数据不可用。请确保视频已处理。";
"Technical analysis data not available." = "技术分析数据不可用。";
"Refresh Analysis" = "刷新分析";
"Analysis Error" = "分析错误";
"Retry" = "重试";

// String formatting for technique analysis
"%@: Well done!" = "%@：做得好！";
"%@: Focus on improving this aspect. Check tutorials for guidance." = "%@：专注于改进这一方面。查看教程获取指导。";

// Upload Tab View
"Choose the type of video you want to upload" = "选择您要上传的视频类型";
"Upload Technique Video" = "上传技术视频";
"Analyze and compare your badminton techniques with model performers" = "分析并比较您的羽毛球技术与模范表演者";
"Upload your match videos for performance analysis" = "上传您的比赛视频进行表现分析";

// Technical strings - converting for proper display
"shoulder_abduction" = "肩部外展";
"elbow_flexion" = "肘部弯曲";
"elbow_lower" = "肘部位置";
"foot_direction_aligned" = "脚部方向";
"proximal_to_distal_sequence" = "运动顺序";
"hip_forward_shift" = "髋部移动";
"trunk_rotation_completed" = "躯干旋转";
</file>

<file path="MoveInsight/ColorManager.swift">
import SwiftUI

struct ColorManager {
    /// Brand purple color used in MoveInsight logo (#5C2D91)
    static let accentColor = Color(hex: "5C2D91")
    
    // Dynamic background: black in dark mode, white in light mode.
    static var background: Color {
        Color(UIColor { traitCollection in
            traitCollection.userInterfaceStyle == .dark ? UIColor.black : UIColor.white
        })
    }
    
    // Primary text color: white in dark mode, black in light mode.
    static var textPrimary: Color {
        Color(UIColor { traitCollection in
            traitCollection.userInterfaceStyle == .dark ? UIColor.white : UIColor.black
        })
    }
    
    // Secondary text color: light gray in dark mode, dark gray in light mode.
    static var textSecondary: Color {
        Color(UIColor { traitCollection in
            traitCollection.userInterfaceStyle == .dark ? UIColor.lightGray : UIColor.darkGray
        })
    }
    
    // Card background: a subtle gray that adapts to light/dark mode.
    static var cardBackground: Color {
        Color(UIColor { traitCollection in
            if traitCollection.userInterfaceStyle == .dark {
                return UIColor.systemGray6
            } else {
                return UIColor.systemGray5
            }
        })
    }
    
    /// Plus button color: white in light mode, black in dark mode.
    static var uploadPlusButtonColor: Color {
        Color(UIColor { traitCollection in
            traitCollection.userInterfaceStyle == .dark ? UIColor.black : UIColor.white
        })
    }
}

extension Color {
    init(hex: String) {
        let hex = hex.trimmingCharacters(in: CharacterSet.alphanumerics.inverted)
        var int: UInt64 = 0
        Scanner(string: hex).scanHexInt64(&int)
        let a, r, g, b: UInt64
        switch hex.count {
        case 3: // RGB (12-bit)
            (a, r, g, b) = (255, (int >> 8) * 17,
                                 (int >> 4 & 0xF) * 17,
                                 (int & 0xF) * 17)
        case 6: // RGB (24-bit)
            (a, r, g, b) = (255,
                           int >> 16,
                           int >> 8 & 0xFF,
                           int & 0xFF)
        case 8: // ARGB (32-bit)
            (a, r, g, b) = (int >> 24,
                           int >> 16 & 0xFF,
                           int >> 8 & 0xFF,
                           int & 0xFF)
        default:
            (a, r, g, b) = (255, 0, 0, 0) // fallback: opaque black
        }

        self.init(
            .sRGB,
            red: Double(r) / 255,
            green: Double(g) / 255,
            blue: Double(b) / 255,
            opacity: Double(a) / 255
        )
    }
}
</file>

<file path="MoveInsight/CombinedVideo3DView.swift">
import SwiftUI
import AVKit
import Combine // Keep for AVPlayer time observers etc.

// MARK: - Pose Overlay View (Adapted for String-based joints)
// This view is designed to work with [String: CGPoint] from VideoPlayerViewModel
// and [StringBodyConnection]
struct PoseOverlayViewForComparison: View { // Renamed to avoid conflict if defined elsewhere
    @ObservedObject var viewModel: VideoPlayerViewModel
    // viewModel.poses is [String: CGPoint]
    // viewModel.bodyConnections is [StringBodyConnection]

    var body: some View {
        Canvas { context, size in
            guard !viewModel.poses.isEmpty else { return }
            let currentPoses = viewModel.poses

            // Draw connections
            for connection in viewModel.bodyConnections {
                guard let fromPointNorm = currentPoses[connection.from],
                      let toPointNorm = currentPoses[connection.to] else {
                    continue
                }
                
                let fromCanvasPoint = CGPoint(x: fromPointNorm.x * size.width, y: fromPointNorm.y * size.height)
                let toCanvasPoint = CGPoint(x: toPointNorm.x * size.width, y: toPointNorm.y * size.height)
                
                var path = Path()
                path.move(to: fromCanvasPoint)
                path.addLine(to: toCanvasPoint)
                context.stroke(path, with: .color(ColorManager.accentColor.opacity(0.8)), lineWidth: 3)
            }

            // Draw joints
            for (_, jointPointNorm) in currentPoses {
                let jointCanvasPoint = CGPoint(x: jointPointNorm.x * size.width, y: jointPointNorm.y * size.height)
                let rect = CGRect(x: jointCanvasPoint.x - 4, y: jointCanvasPoint.y - 4, width: 8, height: 8)
                context.fill(Path(ellipseIn: rect), with: .color(Color.red.opacity(0.8)))
            }
        }
        .opacity(0.7)
    }
}


// MARK: - Combined 2D Video Comparison View
struct CombinedVideo2DComparisonView: View {
    @ObservedObject var primaryViewModel: VideoPlayerViewModel // User's video
    @ObservedObject var secondaryViewModel: VideoPlayerViewModel // Model or second user video
    
    // State for synchronized playback control
    @State private var currentTime: Double = 0
    @State private var isPlayingUserInitiated: Bool = false // Tracks if user explicitly hit play
    @State private var duration: Double = 0.1 // Initialize with a small default to prevent division by zero before loading
    @State private var isLoadingDuration: Bool = true // To show a loading state for duration

    // Time observer for the primary player (can drive updates for both)
    @State private var timeObserverToken: Any? = nil
    private let frameStep: Double = 1.0 / 30.0 // Assuming 30fps for step controls

    var body: some View {
        ZStack {
            ColorManager.background.ignoresSafeArea() // Ensure ColorManager is accessible

            VStack(spacing: 0) {
                if primaryViewModel.isVideoReady && secondaryViewModel.isVideoReady && !isLoadingDuration {
                    HStack(spacing: 8) {
                        videoPlayerWithOverlay(for: primaryViewModel, title: "Your Video")
                        videoPlayerWithOverlay(for: secondaryViewModel, title: "Comparison Video")
                    }
                    .padding(.horizontal, 8)
                    .padding(.top, 8)
                    
                    controlBar
                        .padding(.vertical, 10)

                } else {
                    ProgressView(isLoadingDuration ? "Loading video durations..." : "Loading Videos for Comparison...")
                        .tint(ColorManager.accentColor)
                        .frame(maxWidth: .infinity, maxHeight: .infinity)
                }
            }
        }
        .onAppear {
            isLoadingDuration = true
            Task {
                do {
                    // Load durations asynchronously and handle potential errors
                    let primaryDurationSeconds = try await primaryViewModel.asset.load(.duration).seconds
                    let secondaryDurationSeconds = try await secondaryViewModel.asset.load(.duration).seconds
                    // Update duration on the main thread
                    await MainActor.run {
                        self.duration = max(primaryDurationSeconds, secondaryDurationSeconds, 0.1) // Ensure duration is at least 0.1
                        self.isLoadingDuration = false
                         print("Durations loaded: Primary \(primaryDurationSeconds)s, Secondary \(secondaryDurationSeconds)s, Set Duration: \(self.duration)s")
                    }
                } catch {
                    // Handle error loading durations
                    await MainActor.run {
                        print("Error loading video durations: \(error)")
                        self.duration = 0.1 // Fallback duration
                        self.isLoadingDuration = false
                        // Optionally, show an error message to the user
                    }
                }
            }
            
            setupTimeObserver()
            
            primaryViewModel.player.isMuted = false
            secondaryViewModel.player.isMuted = true
            
            if primaryViewModel.isPlaying || secondaryViewModel.isPlaying {
                isPlayingUserInitiated = true
                primaryViewModel.play()
                secondaryViewModel.play()
            }
        }
        .onDisappear {
            removeTimeObserver()
            primaryViewModel.pause()
            secondaryViewModel.pause()
        }
        .onChange(of: primaryViewModel.isPlaying) { _ in // Use _ if newPrimaryIsPlaying is not directly used
             // Logic for isPlayingUserInitiated and player.rate can be complex
             // For now, ensure the play/pause button reflects isPlayingUserInitiated
             // and actual player rate.
             // Consider if primaryViewModel.isPlaying is always perfectly in sync with player.rate
        }
    }
    
    @ViewBuilder
    private func videoPlayerWithOverlay(for viewModel: VideoPlayerViewModel, title: String) -> some View {
        VStack {
            Text(title)
                .font(.caption)
                .foregroundColor(ColorManager.textSecondary)
            VideoPlayerRepresentable(player: viewModel.player, videoRect: .constant(CGRect()))
                .overlay(PoseOverlayViewForComparison(viewModel: viewModel))
                .aspectRatio(CGSize(width: 9, height: 16), contentMode: .fit)
                .background(Color.black)
                .cornerRadius(8)
                .shadow(radius: 3)
        }
    }
    
    private var controlBar: some View {
        HStack(spacing: 15) {
            Button(action: togglePlayPause) {
                // Base the icon on the actual playback rate of the primary player,
                // and whether the user intended it to play.
                Image(systemName: isPlayingUserInitiated && primaryViewModel.player.rate != 0 ? "pause.fill" : "play.fill")
                    .font(.title2)
            }

            Button(action: { step(by: -1) }) {
                Image(systemName: "backward.frame.fill")
                    .font(.title3)
            }
            
            Slider(
                value: $currentTime,
                in: 0...duration,
                onEditingChanged: { editing in
                    if editing {
                        if primaryViewModel.player.rate != 0 { primaryViewModel.pause() }
                        if secondaryViewModel.player.rate != 0 { secondaryViewModel.pause() }
                    } else {
                        seek(to: currentTime)
                        if isPlayingUserInitiated {
                            primaryViewModel.play()
                            secondaryViewModel.play()
                        }
                    }
                }
            )
            .accentColor(ColorManager.accentColor)
            // Disable slider if duration is not properly loaded or is zero
            .disabled(duration <= 0.1 || isLoadingDuration)


            Button(action: { step(by: 1) }) {
                Image(systemName: "forward.frame.fill")
                    .font(.title3)
            }
            
            Text(String(format: "%.2f / %.2f", currentTime, duration))
                .font(.caption)
                .foregroundColor(ColorManager.textSecondary)
                .frame(minWidth: 80, alignment: .leading)
        }
        .padding(.horizontal)
        .foregroundColor(ColorManager.accentColor)
    }

    // MARK: - Playback Control Logic
    private func togglePlayPause() {
        isPlayingUserInitiated.toggle()
        if isPlayingUserInitiated {
            primaryViewModel.play()
            secondaryViewModel.play()
        } else {
            primaryViewModel.pause()
            secondaryViewModel.pause()
        }
    }

    private func step(by frames: Int) {
        // Prevent stepping if duration is not loaded
        guard duration > 0.1 && !isLoadingDuration else { return }

        let newTime = max(0, min(duration, currentTime + Double(frames) * frameStep))
        seek(to: newTime)
        
        if !isPlayingUserInitiated {
            primaryViewModel.play()
            secondaryViewModel.play()
            DispatchQueue.main.asyncAfter(deadline: .now() + 0.05) {
                primaryViewModel.pause()
                secondaryViewModel.pause()
                self.currentTime = newTime
            }
        } else {
             self.currentTime = newTime
        }
    }

    private func seek(to timeSec: Double) {
        // Prevent seeking if duration is not loaded
        guard duration > 0.1 && !isLoadingDuration else { return }

        let cmTime = CMTime(seconds: timeSec, preferredTimescale: 600)
        let dispatchGroup = DispatchGroup()

        let wasPlaying = isPlayingUserInitiated && primaryViewModel.player.rate != 0
        if primaryViewModel.player.rate != 0 { primaryViewModel.pause() }
        if secondaryViewModel.player.rate != 0 { secondaryViewModel.pause() }


        dispatchGroup.enter()
        primaryViewModel.player.seek(to: cmTime, toleranceBefore: .zero, toleranceAfter: .zero) { _ in dispatchGroup.leave() }
        
        dispatchGroup.enter()
        secondaryViewModel.player.seek(to: cmTime, toleranceBefore: .zero, toleranceAfter: .zero) { _ in dispatchGroup.leave() }

        dispatchGroup.notify(queue: .main) {
            self.currentTime = timeSec
            if wasPlaying {
                primaryViewModel.play()
                secondaryViewModel.play()
            } else {
                // If paused, ensure the frame is visually updated.
                // This might involve telling the VideoPlayerViewModels to refresh their current pose
                // if their displayLink only fires on active playback.
                // For now, relying on the brief play/pause in step() for paused updates.
            }
        }
    }

    private func setupTimeObserver() {
        guard timeObserverToken == nil else { return }
        let interval = CMTime(seconds: frameStep / 2.0, preferredTimescale: CMTimeScale(NSEC_PER_SEC))
        
        timeObserverToken = primaryViewModel.player.addPeriodicTimeObserver(forInterval: interval, queue: .main) { [self] time in
            // Only update if not currently scrubbing the slider and duration is valid
            if !duration.isZero && !isLoadingDuration {
                 self.currentTime = time.seconds
            }
        }
    }

    private func removeTimeObserver() {
        if let token = timeObserverToken {
            primaryViewModel.player.removeTimeObserver(token)
            timeObserverToken = nil
        }
    }
}

// MARK: - Supporting Structures (Ensure these are defined in your project)
// struct ColorManager { static var accentColor: Color ... etc. }
// struct VideoPlayerRepresentable: UIViewRepresentable { ... }
// class VideoPlayerViewModel: ObservableObject { ... }
//      - Make sure VideoPlayerViewModel.asset.load(.duration) is available or adapt duration calculation.
//      - VideoPlayerViewModel.poses is [String: CGPoint]
//      - VideoPlayerViewModel.bodyConnections is [StringBodyConnection]
// struct StringBodyConnection: Identifiable { let id: UUID; let from: String; let to: String }
// enum Pose3DBody.VideoSource { case primary, secondary } // Or your equivalent type for VideoPlayerViewModel's videoSource
</file>

<file path="MoveInsight/CustomTabBar.swift">
import SwiftUI

// MARK: - Custom Tab Bar
struct CustomTabBar: View {
    @Binding var selectedTab: Int

    var body: some View {
        EvenlySpacedTabBar(selectedTab: $selectedTab)
    }
}

// MARK: - Evenly Spaced Tab Bar
struct EvenlySpacedTabBar: View {
    @Binding var selectedTab: Int
    
    var body: some View {
        HStack {
            Spacer()
            
            // Home button
            TabBarButtonEvenly(iconName: "house.fill", isSelected: selectedTab == 0) {
                selectedTab = 0
            }
            
            Spacer()
            
            // Training button
            TabBarButtonEvenly(iconName: "figure.run", isSelected: selectedTab == 1) {
                selectedTab = 1
            }
            
            Spacer()
            
            // Plus (upload) button – uses dynamic color for plus icon.
            Button(action: {
                // In this updated design, the Upload tab is directly rendered.
                selectedTab = 2
            }) {
                ZStack {
                    Circle()
                        .fill(ColorManager.accentColor)
                        .frame(width: 44, height: 44)
                    
                    Image(systemName: "plus")
                        .font(.system(size: 18, weight: .bold))
                        .foregroundColor(ColorManager.uploadPlusButtonColor)
                }
            }
            
            Spacer()
            
            // Videos button
            TabBarButtonEvenly(iconName: "play.rectangle.fill", isSelected: selectedTab == 3) {
                selectedTab = 3
            }
            
            Spacer()
            
            // Messages button
            TabBarButtonEvenly(iconName: "message.fill", isSelected: selectedTab == 4) {
                selectedTab = 4
            }
            
            Spacer()
        }
        .padding(.vertical, 10)
        .background(ColorManager.background)
        .overlay(
            Rectangle()
                .frame(height: 1)
                .foregroundColor(ColorManager.textSecondary.opacity(0.3)),
            alignment: .top
        )
    }
}

// MARK: - Tab Bar Button for Evenly Spaced Layout
struct TabBarButtonEvenly: View {
    let iconName: String
    let isSelected: Bool
    let action: () -> Void

    var body: some View {
        Button(action: action) {
            Image(systemName: iconName)
                .font(.system(size: 20))
                .foregroundColor(isSelected ? ColorManager.accentColor : ColorManager.textPrimary)
        }
    }
}

// MARK: - Legacy Tab Bar Button Component (kept for backward compatibility)
struct TabBarButton: View {
    let iconName: String
    let title: LocalizedStringKey
    let isSelected: Bool
    let action: () -> Void

    var body: some View {
        Button(action: action) {
            VStack(spacing: 3) {
                Image(systemName: iconName)
                    .font(.system(size: 18))
                    .foregroundColor(isSelected ? ColorManager.accentColor : ColorManager.textPrimary)

                Text(title)
                    .font(.system(size: 11))
                    .foregroundColor(isSelected ? ColorManager.accentColor : ColorManager.textPrimary)
            }
            .frame(maxWidth: .infinity)
        }
    }
}
</file>

<file path="MoveInsight/DepthEstimationService.swift">
//import CoreML
//import Vision
//import AVFoundation
//import Accelerate
//import UIKit
//
//class DepthEstimationService {
//    private var depthModel: MLModel?
//    
//    init() {
//        do {
//            let modelURL = Bundle.main.url(forResource: "DepthAnythingV2SmallF16", withExtension: "mlmodelc")!
//            let config = MLModelConfiguration()
//            self.depthModel = try MLModel(contentsOf: modelURL, configuration: config)
//            print("Loaded depth model successfully.")
//        } catch {
//            print("Failed to load depth model: \(error)")
//        }
//    }
//    
//    func estimateDepth(from pixelBuffer: CVPixelBuffer) -> CVPixelBuffer? {
//        guard let model = depthModel else {
//            print("Depth model not loaded")
//            return nil
//        }
//        
//        do {
//            // Resize according to model requirements
//            let resizedPixelBuffer = resizePixelBufferToRequiredSize(pixelBuffer)
//            let width = CVPixelBufferGetWidth(resizedPixelBuffer)
//            let height = CVPixelBufferGetHeight(resizedPixelBuffer)
//            print("Resized to \(width) x \(height)")
//            
//            // Create MLFeatureValue for the image
//            let imageFeatureValue = MLFeatureValue(pixelBuffer: resizedPixelBuffer)
//            
//            // Create input feature dictionary
//            let inputFeatures = try MLDictionaryFeatureProvider(dictionary: ["image": imageFeatureValue])
//            
//            // Make prediction
//            let outputFeatures = try model.prediction(from: inputFeatures)
//            
//            // Extract the depth map
//            if let depthFeatureValue = outputFeatures.featureValue(for: "depth"),
//               let depthPixelBuffer = depthFeatureValue.imageBufferValue {
//                return depthPixelBuffer
//            } else {
//                print("Failed to extract depth output")
//                return nil
//            }
//        } catch {
//            print("Depth estimation failed: \(error)")
//            return nil
//        }
//    }
//    
//    // Resize according to specific model requirements
//    private func resizePixelBufferToRequiredSize(_ pixelBuffer: CVPixelBuffer) -> CVPixelBuffer {
//        let width = CVPixelBufferGetWidth(pixelBuffer)
//        let height = CVPixelBufferGetHeight(pixelBuffer)
//        
//        // Target dimensions
//        // Width should be 518, height should be 392 (as per your example)
//        let targetWidth = 518
//        let targetHeight = 392
//        
//        // Create a CIImage from the pixel buffer
//        let ciImage = CIImage(cvPixelBuffer: pixelBuffer)
//        
//        // Scale the image
//        let scaleX = CGFloat(targetWidth) / CGFloat(width)
//        let scaleY = CGFloat(targetHeight) / CGFloat(height)
//        let scaledImage = ciImage.transformed(by: CGAffineTransform(scaleX: scaleX, y: scaleY))
//        
//        // Create a new pixel buffer
//        var newPixelBuffer: CVPixelBuffer?
//        let options = [
//            kCVPixelBufferCGImageCompatibilityKey: true,
//            kCVPixelBufferCGBitmapContextCompatibilityKey: true
//        ] as CFDictionary
//        
//        CVPixelBufferCreate(kCFAllocatorDefault,
//                           targetWidth,
//                           targetHeight,
//                           kCVPixelFormatType_32BGRA,
//                           options,
//                           &newPixelBuffer)
//        
//        guard let outputPixelBuffer = newPixelBuffer else {
//            return pixelBuffer // Return original if resize fails
//        }
//        
//        // Render the scaled CIImage to the new pixel buffer
//        let context = CIContext()
//        context.render(scaledImage, to: outputPixelBuffer)
//        
//        return outputPixelBuffer
//    }
//    
//    // Convert depth pixel buffer to UIImage for visualization
//    func depthMapToImage(from depthBuffer: CVPixelBuffer) -> UIImage? {
//        CVPixelBufferLockBaseAddress(depthBuffer, .readOnly)
//        defer { CVPixelBufferUnlockBaseAddress(depthBuffer, .readOnly) }
//        
//        // For Grayscale16Half format, we need to convert it to a format UIImage can use
//        let width = CVPixelBufferGetWidth(depthBuffer)
//        let height = CVPixelBufferGetHeight(depthBuffer)
//        
//        // Create a CIImage from the depth buffer
//        var ciImage: CIImage?
//        if let colorSpace = CGColorSpace(name: CGColorSpace.linearGray) {
//            ciImage = CIImage(cvImageBuffer: depthBuffer, options: [.colorSpace: colorSpace])
//        } else {
//            ciImage = CIImage(cvImageBuffer: depthBuffer)
//        }
//        
//        guard let image = ciImage else { return nil }
//        
//        // Apply visualization filters to make depth more visible
//        let normalizedImage = image.applyingFilter("CIColorControls", parameters: [
//            kCIInputContrastKey: 1.5,
//            kCIInputBrightnessKey: 0.2
//        ])
//        
//        // Create a colored version using the turbo colormap
//        let coloredImage = normalizedImage.applyingFilter("CIColorMap", parameters: [
//            "inputGradientImage": createTurboColormap()
//        ])
//        
//        // Convert to UIImage
//        let context = CIContext()
//        if let cgImage = context.createCGImage(coloredImage, from: coloredImage.extent) {
//            return UIImage(cgImage: cgImage)
//        }
//        
//        return nil
//    }
//    
//    // Create a turbo colormap for better depth visualization
//    private func createTurboColormap() -> CIImage {
//        let colors: [(CGFloat, CGFloat, CGFloat)] = [
//            (0.18995, 0.07176, 0.23217), // Dark blue
//            (0.19483, 0.22800, 0.47607), // Blue
//            (0.01555, 0.44879, 0.69486), // Light blue
//            (0.12943, 0.65563, 0.67862), // Cyan
//            (0.33486, 0.81853, 0.39915), // Green
//            (0.66724, 0.88581, 0.25420), // Yellow-green
//            (0.90480, 0.91255, 0.10421), // Yellow
//            (0.99796, 0.68829, 0.02774), // Orange
//            (0.95909, 0.37228, 0.01549), // Red-orange
//            (0.73683, 0.01779, 0.01820)  // Dark red
//        ]
//        
//        let width = 256
//        let gradientImage = CIImage(color: CIColor(red: 0, green: 0, blue: 0)).cropped(to: CGRect(x: 0, y: 0, width: width, height: 1))
//        
//        var result = gradientImage
//        
//        // Create gradient segments
//        for i in 0..<(colors.count-1) {
//            let startColor = CIColor(red: colors[i].0, green: colors[i].1, blue: colors[i].2)
//            let endColor = CIColor(red: colors[i+1].0, green: colors[i+1].1, blue: colors[i+1].2)
//            
//            let startX = (width * i) / (colors.count - 1)
//            let endX = (width * (i + 1)) / (colors.count - 1)
//            
//            let gradient = CIFilter(name: "CILinearGradient", parameters: [
//                "inputPoint0": CIVector(x: CGFloat(startX), y: 0),
//                "inputPoint1": CIVector(x: CGFloat(endX), y: 0),
//                "inputColor0": startColor,
//                "inputColor1": endColor
//            ])!.outputImage!
//            
//            // Blend with previous result
//            result = result.applyingFilter("CISourceOverCompositing", parameters: [
//                "inputBackgroundImage": gradient
//            ])
//        }
//        
//        return result
//    }
//}
</file>

<file path="MoveInsight/Info.plist">
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
    <key>NSAppTransportSecurity</key>
    <dict>
        <key>NSExceptionDomains</key>
        <dict>
            <key>115.188.74.78</key>
            <dict>
                <key>NSIncludesSubdomains</key>
                <true/>
                <key>NSExceptionAllowsInsecureHTTPLoads</key>
                <true/>
                <key>NSExceptionRequiresForwardSecrecy</key>
                <true/>
                <key>NSExceptionMinimumTLSVersion</key>
                <string>TLSv1.2</string>
                <key>NSThirdPartyExceptionAllowsInsecureHTTPLoads</key>
                <false/>
                <key>NSThirdPartyExceptionRequiresForwardSecrecy</key>
                <true/>
                <key>NSThirdPartyExceptionMinimumTLSVersion</key>
                <string>TLSv1.2</string>
                <key>NSRequiresCertificateTransparency</key>
                <false/>
            </dict>
        </dict>
    </dict>
</dict>
</plist>
</file>

<file path="MoveInsight/ModelVideoLoader.swift">
import Foundation
import AVKit

// This class helps manage model videos in the app's bundle
class ModelVideoLoader {
    static let shared = ModelVideoLoader()
    
    // Map of technique names to their video filenames (without extension)
    private let techniqueVideoMap: [String: String] = [
        "Underhand Clear": "underhand_clear_model",
        // Other techniques would be added here as they become available
    ]
    
    private init() {
        // Log available model videos on initialization
        print("ModelVideoLoader initialized with techniques: \(techniqueVideoMap.keys.joined(separator: ", "))")
    }
    
    // Get the URL for a specific technique's model video
    func getModelVideoURL(for technique: String) -> URL? {
        // Try to find the exact technique name in our map
        let cleanTechniqueName = technique.trimmingCharacters(in: .whitespacesAndNewlines)
        
        if let filename = techniqueVideoMap[cleanTechniqueName] {
            if let url = Bundle.main.url(forResource: filename, withExtension: "mov") {
                print("Found model video for \(cleanTechniqueName) at \(url.path)")
                return url
            }
        }
        
        // Try with a normalized technique name (lowercase, underscores)
        let normalizedName = cleanTechniqueName.lowercased().replacingOccurrences(of: " ", with: "_")
        if let url = Bundle.main.url(forResource: normalizedName, withExtension: "mov") {
            print("Found model video using normalized name \(normalizedName)")
            return url
        }
        
        // Special case for backhand clear (our only implemented video currently)
        if cleanTechniqueName.lowercased().contains("backhand") {
            if let fallbackURL = Bundle.main.url(forResource: "backhand_clear_model", withExtension: "mov") {
                print("Found underhand clear model video as fallback")
                return fallbackURL
            }
        }
        
        print("No model video found for technique: \(technique)")
        return nil
    }
    
    // Check if a model video exists for a specific technique
    func hasModelVideo(for technique: String) -> Bool {
        return getModelVideoURL(for: technique) != nil
    }
    
    // Create a VideoPlayerViewModel for a model video
    func createModelVideoViewModel(for technique: String) -> VideoPlayerViewModel? {
        guard let videoURL = getModelVideoURL(for: technique) else {
            print("Failed to create model ViewModel: no URL found for \(technique)")
            return nil
        }
        
        print("Creating model VideoPlayerViewModel for \(technique)")
        return VideoPlayerViewModel(
            videoURL: videoURL,
            videoSource: .secondary
        )
    }
}
</file>

<file path="MoveInsight/Pose3DProcessor.swift">
//import Vision
//import simd
//import CoreVideo
//
//class Pose3DProcessor {
//    
//    // Convert 2D pose point with depth to 3D
//    func convert2DPoseToWorldSpace(
//        pose: [VNHumanBodyPoseObservation.JointName: CGPoint],
//        depthBuffer: CVPixelBuffer,
//        videoSize: CGSize
//    ) -> [VNHumanBodyPoseObservation.JointName: SIMD3<Float>] {
//        var pose3D: [VNHumanBodyPoseObservation.JointName: SIMD3<Float>] = [:]
//        
//        // Get dimensions of depth buffer
//        let depthWidth = CVPixelBufferGetWidth(depthBuffer)
//        let depthHeight = CVPixelBufferGetHeight(depthBuffer)
//        
//        CVPixelBufferLockBaseAddress(depthBuffer, .readOnly)
//        defer { CVPixelBufferUnlockBaseAddress(depthBuffer, .readOnly) }
//        
//        // Get base address assuming it's grayscale16half format
//        guard let baseAddress = CVPixelBufferGetBaseAddress(depthBuffer) else {
//            return [:]
//        }
//        
//        let bytesPerRow = CVPixelBufferGetBytesPerRow(depthBuffer)
//        
//        for (joint, point) in pose {
//            // Convert normalized coordinates to pixel coordinates in the depth buffer
//            let depthX = Int(point.x * CGFloat(depthWidth))
//            let depthY = Int(point.y * CGFloat(depthHeight))
//            
//            // Ensure coordinates are within bounds
//            if depthX >= 0 && depthX < depthWidth && depthY >= 0 && depthY < depthHeight {
//                // Get depth value (16-bit half float)
//                let pixelAddress = baseAddress.advanced(by: depthY * bytesPerRow + depthX * 2)
//                let halfFloat = pixelAddress.assumingMemoryBound(to: UInt16.self).pointee
//                
//                // Convert half float to float
//                let depth = convertHalfToFloat(halfFloat)
//                
//                // Create 3D point: x, y from 2D pose, z from depth
//                let x = Float(point.x * 2 - 1) // Convert 0-1 to -1 to 1
//                let y = Float(1 - point.y * 2) // Convert 0-1 to 1 to -1 (y-axis flipped in 3D)
//                let z = depth
//                
//                pose3D[joint] = SIMD3<Float>(x, y, z)
//            }
//        }
//        
//        return pose3D
//    }
//    
//    // Helper to convert UInt16 half float to Float
//    private func convertHalfToFloat(_ half: UInt16) -> Float {
//        // Using a more direct bit manipulation approach
//        let sign = (half & 0x8000) != 0
//        let exponent = Int((half & 0x7C00) >> 10)
//        let fraction = half & 0x03FF
//        
//        // Handle special cases
//        if exponent == 0 {
//            if fraction == 0 {
//                return sign ? -0.0 : 0.0 // Zero
//            } else {
//                // Denormalized number
//                var result = Float(fraction) / Float(1024)
//                result *= pow(2.0, -14.0)
//                return sign ? -result : result
//            }
//        } else if exponent == 31 {
//            if fraction == 0 {
//                return sign ? Float.infinity : Float.infinity
//            } else {
//                return Float.nan
//            }
//        }
//        
//        // Normalized number
//        var result = Float(1 + Float(fraction) / 1024.0)
//        result *= pow(2.0, Float(exponent - 15))
//        return sign ? -result : result
//    }
//    
//    // Processing multiple poses from a frame
//    func process(
//        poses: [[VNHumanBodyPoseObservation.JointName: CGPoint]],
//        depthBuffer: CVPixelBuffer,
//        videoSize: CGSize
//    ) -> [[VNHumanBodyPoseObservation.JointName: SIMD3<Float>]] {
//        return poses.map { pose in
//            convert2DPoseToWorldSpace(
//                pose: pose,
//                depthBuffer: depthBuffer,
//                videoSize: videoSize
//            )
//        }
//    }
//}
</file>

<file path="MoveInsight/SceneView3D.swift">
import SwiftUI
import Vision
import SceneKit
import simd

// Scene delegate to maintain camera position across updates (can be kept if other 3D elements remain)
class SceneDelegate: NSObject, SCNSceneRendererDelegate, ObservableObject {
    var lastCameraTransform: SCNMatrix4?
    
    func renderer(_ renderer: SCNSceneRenderer, updateAtTime time: TimeInterval) {
        if let pointOfView = renderer.pointOfView {
            lastCameraTransform = pointOfView.transform
        }
    }
}

struct SceneView3D: UIViewRepresentable {
    // This view will now primarily be a placeholder or simplified
    // as 3D pose data from depth is no longer generated in-app.
    // If server sends 3D data in future, this can be re-enabled.
    // For now, it won't render skeletons as pose3DBodies will likely be empty.

    var pose3DBodies: [Pose3DBody] // This will likely be empty or unused
    var bodyConnections: [BodyConnection] // May not be used if skeletons aren't drawn
    @ObservedObject var sceneDelegate = SceneDelegate()
    
    func makeUIView(context: Context) -> SCNView {
        let sceneView = SCNView()
        let scene = SCNScene()
        
        setupCamera(in: scene)
        setupLighting(in: scene)
        
        // Add floor grid (can be kept for context if view is still used)
        let floor = createFloorGrid()
        floor.position = SCNVector3(0, -0.5, 0) // Adjust Y if needed
        scene.rootNode.addChildNode(floor)
        
        sceneView.scene = scene
        sceneView.backgroundColor = UIColor.systemBackground
        sceneView.allowsCameraControl = true // Allow interaction if view is visible
        sceneView.showsStatistics = false
        sceneView.delegate = sceneDelegate
        
        return sceneView
    }
    
    func updateUIView(_ uiView: SCNView, context: Context) {
        // Clear previous custom nodes (skeletons, heads)
        uiView.scene?.rootNode.childNodes.forEach { node in
            if node.name == "skeleton" || node.name == "headNode" { // Example names
                node.removeFromParentNode()
            }
        }

        // --- 3D Skeleton Rendering Logic ---
        // The following logic for addSkeletonToScene and addHeadToScene
        // would rely on pose3DBodies being populated with 3D data.
        // Since we've removed in-app depth estimation and 3D pose processing,
        // pose3DBodies will likely be empty or contain only 2D data (if adapted).
        //
        // For a 2D render view, this SCNView might not be the primary display
        // for poses anymore. Overlays directly on the VideoPlayerRepresentable
        // would handle 2D poses.
        //
        // If you intend to show something in this 3D scene (e.g., a static model,
        // or if the server *does* send 3D joint data in the future),
        // then the rendering logic would go here.
        //
        // For now, as per "comment out the implementation and replace with 2d render view",
        // we will not add new skeletons here unless pose3DBodies is meaningfully populated.
        // If pose3DBodies is populated with 2D data (e.g., Z=0), the existing
        // functions might draw flat skeletons, which could be a form of 2D representation.

        /*
        // Example: If pose3DBodies could contain 2D data adapted for 3D (Z=0)
        if let primaryPose = pose3DBodies.first(where: { $0.videoSource == .primary }) {
            // Ensure your addSkeletonToScene and addHeadToScene can handle
            // poses where Z might be consistently 0 or based on 2D projection.
            // addSkeletonToScene(body: primaryPose, scene: uiView.scene, position: SIMD3<Float>(0,0,0))
            // addHeadToScene(body: primaryPose, scene: uiView.scene, position: SIMD3<Float>(0,0,0))
        }
        if let secondaryPose = pose3DBodies.first(where: { $0.videoSource == .secondary }) {
            // addSkeletonToScene(body: secondaryPose, scene: uiView.scene, position: SIMD3<Float>(1,0,0)) // Offset for comparison
            // addHeadToScene(body: secondaryPose, scene: uiView.scene, position: SIMD3<Float>(1,0,0))
        }
        */
        
        // If no 3D poses are available, this view will just show the camera, lights, and floor.
        print("SceneView3D updated. pose3DBodies count: \(pose3DBodies.count). If 0, no skeletons drawn.")
    }
    
    private func setupCamera(in scene: SCNScene) {
        let camera = SCNCamera()
        camera.usesOrthographicProjection = false // Perspective for a more natural view
        camera.fieldOfView = 50
        camera.zNear = 0.1
        camera.zFar = 100
        
        let cameraNode = SCNNode()
        cameraNode.camera = camera
        
        if let lastTransform = sceneDelegate.lastCameraTransform {
            cameraNode.transform = lastTransform
        } else {
            cameraNode.position = SCNVector3(0, 0.5, 2.5) // Slightly elevated and back
            cameraNode.look(at: SCNVector3(0, 0, 0))
        }
        scene.rootNode.addChildNode(cameraNode)
    }
    
    private func setupLighting(in scene: SCNScene) {
        let ambientLight = SCNLight()
        ambientLight.type = .ambient
        ambientLight.intensity = 200 // Softer ambient light
        ambientLight.color = UIColor(white: 0.7, alpha: 1.0)
        let ambientLightNode = SCNNode()
        ambientLightNode.light = ambientLight
        scene.rootNode.addChildNode(ambientLightNode)
        
        let directionalLight = SCNLight()
        directionalLight.type = .directional
        directionalLight.intensity = 800
        directionalLight.castsShadow = true
        directionalLight.shadowMode = .deferred // Better shadow quality
        directionalLight.shadowColor = UIColor.black.withAlphaComponent(0.5)
        directionalLight.shadowSampleCount = 8 // Smoother shadow edges
        
        let directionalLightNode = SCNNode()
        directionalLightNode.light = directionalLight
        directionalLightNode.position = SCNVector3(-3, 5, 4)
        directionalLightNode.look(at: SCNVector3(0, 0, 0))
        scene.rootNode.addChildNode(directionalLightNode)
    }
    
    private func createFloorGrid() -> SCNNode {
        let floorGeometry = SCNPlane(width: 10, height: 10)
        let material = SCNMaterial()
        material.diffuse.contents = UIColor.systemGray3 // A neutral floor color
        // Optional: Add a grid texture or lines programmatically if needed
        // For a simple grid:
        material.diffuse.contents = UIImage(named: "grid_texture") ?? UIColor.systemGray3 // if you have a grid texture
        material.isDoubleSided = true
        floorGeometry.materials = [material]
        
        let floorNode = SCNNode(geometry: floorGeometry)
        floorNode.name = "floor"
        floorNode.eulerAngles.x = -.pi / 2 // Rotate to be flat
        return floorNode
    }

    // --- Functions for drawing 3D skeletons (addSkeletonToScene, addHeadToScene, etc.) ---
    // These functions (createJointNode, connectJoints, createBoneBetween, findLowestPoint, calculateSkeletonHeight)
    // would be kept if pose3DBodies were populated with 3D data.
    // Since the primary goal is 2D rendering now, their direct usage in updateUIView is commented out.
    // If you decide to adapt them for a "flat 3D" representation (e.g., Z=0), they can be called.
    // For brevity, I'm not re-listing their entire implementations here but they would remain
    // in the file if there's any chance of future 3D use or adaptation.
}
</file>

<file path="MoveInsight/TechniqueAnalysisService.swift">
import Foundation
import Combine
import AVFoundation // For URL

// MARK: - Data Structures for Server Communication

// For single video analysis (already defined)
struct ServerJointData: Codable {
    let x: Double
    let y: Double
    let confidence: Double? // Optional: if MediaPipe provides and server sends it
}

struct ServerFrameData: Codable {
    let joints: [String: ServerJointData] // Key is joint name string
}

struct VideoAnalysisResponse: Codable {
    let totalFrames: Int
    let jointDataPerFrame: [ServerFrameData]

    enum CodingKeys: String, CodingKey {
        case totalFrames = "total_frames"
        case jointDataPerFrame = "joint_data_per_frame"
    }
}

// MARK: - New Structures for Technique Comparison Request
// This structure will be sent TO the server for comparison
struct TechniqueComparisonRequestData: Codable {
    let userVideoFrames: [ServerFrameData]
    let modelVideoFrames: [ServerFrameData]
    let dominantSide: String // e.g., "Right" or "Left"
    // Add any other parameters your server might need for comparison

    enum CodingKeys: String, CodingKey {
        case userVideoFrames = "user_video_frames"
        case modelVideoFrames = "model_video_frames"
        case dominantSide = "dominant_side"
    }
}

class TechniqueAnalysisService {
    // Ensure this URL points to your server's correct IP/domain and port
    private let serverBaseURL = URL(string: "http://115.188.74.78:8000")! // Replace with your actual server URL

    // Function to upload a single video and get its joint data
    func analyzeVideoByUploading(videoURL: URL, dominantSide: String) -> AnyPublisher<VideoAnalysisResponse, Error> {
        let endpoint = serverBaseURL.appendingPathComponent("/analyze/video_upload/")
        
        var request = URLRequest(url: endpoint)
        request.httpMethod = "POST"
        
        let boundary = "Boundary-\(UUID().uuidString)"
        request.setValue("multipart/form-data; boundary=\(boundary)", forHTTPHeaderField: "Content-Type")
        
        var httpBody = Data()
        
        // Add dominant_side part
        httpBody.append("--\(boundary)\r\n".data(using: .utf8)!)
        httpBody.append("Content-Disposition: form-data; name=\"dominant_side\"\r\n\r\n".data(using: .utf8)!)
        httpBody.append("\(dominantSide)\r\n".data(using: .utf8)!)
        
        // Add video file part
        do {
            let videoData = try Data(contentsOf: videoURL)
            let filename = videoURL.lastPathComponent
            let mimetype = "video/mp4"
            
            httpBody.append("--\(boundary)\r\n".data(using: .utf8)!)
            httpBody.append("Content-Disposition: form-data; name=\"file\"; filename=\"\(filename)\"\r\n".data(using: .utf8)!)
            httpBody.append("Content-Type: \(mimetype)\r\n\r\n".data(using: .utf8)!)
            httpBody.append(videoData)
            httpBody.append("\r\n".data(using: .utf8)!)
        } catch {
            print("Error reading video data: \(error)")
            return Fail(error: error).eraseToAnyPublisher()
        }
        
        httpBody.append("--\(boundary)--\r\n".data(using: .utf8)!)
        request.httpBody = httpBody
        
        print("Uploading video to \(endpoint)... with dominant side: \(dominantSide)")

        return URLSession.shared.dataTaskPublisher(for: request)
            .tryMap { output in
                guard let httpResponse = output.response as? HTTPURLResponse else {
                    throw URLError(.badServerResponse)
                }
                print("Server response status code for single video upload: \(httpResponse.statusCode)")
                if !(200...299).contains(httpResponse.statusCode) {
                    if let responseString = String(data: output.data, encoding: .utf8) {
                        print("Server error response: \(responseString)")
                    }
                    throw URLError(.init(rawValue: httpResponse.statusCode), userInfo: [NSLocalizedDescriptionKey: "Server returned status \(httpResponse.statusCode) for single video upload"])
                }
                return output.data
            }
            .decode(type: VideoAnalysisResponse.self, decoder: JSONDecoder())
            .receive(on: DispatchQueue.main)
            .eraseToAnyPublisher()
    }

    // MARK: - New Function for Technique Comparison
    // This function sends two sets of joint data to the server for comparison.
    // IMPORTANT: You need to implement the corresponding endpoint on your server.
    func requestTechniqueComparison(
        userFrames: [ServerFrameData],
        modelFrames: [ServerFrameData],
        dominantSide: String
    ) -> AnyPublisher<ComparisonResult, Error> {
        
        // *** IMPORTANT: Replace with your actual new server endpoint for comparison ***
        let endpoint = serverBaseURL.appendingPathComponent("/analyze/technique_comparison/")
        print("Requesting technique comparison from: \(endpoint)")

        var request = URLRequest(url: endpoint)
        request.httpMethod = "POST"
        request.setValue("application/json", forHTTPHeaderField: "Content-Type")

        let requestData = TechniqueComparisonRequestData(
            userVideoFrames: userFrames,
            modelVideoFrames: modelFrames,
            dominantSide: dominantSide
        )

        do {
            request.httpBody = try JSONEncoder().encode(requestData)
            if let jsonString = String(data: request.httpBody!, encoding: .utf8) {
                 print("Sending comparison request JSON: \(jsonString.prefix(1000))...") // Log first 1000 chars
            }
        } catch {
            print("Error encoding comparison request data: \(error)")
            return Fail(error: error).eraseToAnyPublisher()
        }

        return URLSession.shared.dataTaskPublisher(for: request)
            .tryMap { output in
                guard let httpResponse = output.response as? HTTPURLResponse else {
                    throw URLError(.badServerResponse)
                }
                print("Server response status code for comparison: \(httpResponse.statusCode)")
                if !(200...299).contains(httpResponse.statusCode) {
                    if let responseString = String(data: output.data, encoding: .utf8) {
                        print("Server error response (comparison): \(responseString)")
                    }
                    throw URLError(.init(rawValue: httpResponse.statusCode), userInfo: [NSLocalizedDescriptionKey: "Server returned status \(httpResponse.statusCode) for comparison"])
                }
                // For debugging: print raw server response
                // if let jsonString = String(data: output.data, encoding: .utf8) {
                //     print("Raw server response JSON (comparison): \(jsonString)")
                // }
                return output.data
            }
            .decode(type: ComparisonResult.self, decoder: JSONDecoder())
            .receive(on: DispatchQueue.main)
            .eraseToAnyPublisher()
    }
}

// Helper extension (already defined)
extension Data {
    mutating func append(_ string: String) {
        if let data = string.data(using: .utf8) {
            append(data)
        }
    }
}
</file>

<file path="MoveInsight/TechniqueDetailView.swift">
import SwiftUI
import AVKit
import Combine

// MARK: - Pose Overlay View (Modified for String-based joints)
// This view is now designed to work with [String: CGPoint] from VideoPlayerViewModel
struct PoseOverlayView: View {
    @ObservedObject var viewModel: VideoPlayerViewModel
    // viewModel.poses is now [String: CGPoint]
    // viewModel.bodyConnections is now [StringBodyConnection]

    var body: some View {
        Canvas { context, size in
            // Ensure currentPoses is not empty and has data
            guard !viewModel.poses.isEmpty else { return }
            let currentPoses = viewModel.poses // [String: CGPoint]

            // Draw connections
            for connection in viewModel.bodyConnections { // Iterating over [StringBodyConnection]
                // Get the CGPoints for 'from' and 'to' joints using their string names
                guard let fromPointNorm = currentPoses[connection.from],
                      let toPointNorm = currentPoses[connection.to] else {
                    // print("Warning: Missing joint for connection: \(connection.from) or \(connection.to)")
                    continue
                }
                
                // Server coordinates are already normalized (0-1, top-left origin)
                // Scale them to the canvas size
                let fromCanvasPoint = CGPoint(x: fromPointNorm.x * size.width, y: fromPointNorm.y * size.height)
                let toCanvasPoint = CGPoint(x: toPointNorm.x * size.width, y: toPointNorm.y * size.height)
                
                var path = Path()
                path.move(to: fromCanvasPoint)
                path.addLine(to: toCanvasPoint)
                context.stroke(path, with: .color(ColorManager.accentColor.opacity(0.8)), lineWidth: 3)
            }

            // Draw joints
            for (_, jointPointNorm) in currentPoses { // Iterating over [String: CGPoint]
                // Scale normalized point to canvas size
                let jointCanvasPoint = CGPoint(x: jointPointNorm.x * size.width, y: jointPointNorm.y * size.height)
                let rect = CGRect(x: jointCanvasPoint.x - 4, y: jointCanvasPoint.y - 4, width: 8, height: 8) // Slightly larger points
                context.fill(Path(ellipseIn: rect), with: .color(Color.red.opacity(0.8)))
            }
        }
        .opacity(0.7) // Keep overlay slightly transparent
        // .drawingGroup() // Consider adding for performance with complex drawings, test if needed
    }
}


// MARK: - KeyPointRow (Keep as is or adapt if needed)
struct KeyPointRow: View {
    let icon: String
    let text: String
    
    var body: some View {
        HStack(spacing: 12) {
            Image(systemName: icon)
                .font(.system(size: 18))
                .foregroundColor(ColorManager.accentColor)
                .frame(width: 24, height: 24)
            Text(LocalizedStringKey(text))
                .font(.subheadline)
                .foregroundColor(ColorManager.textPrimary)
            Spacer()
        }
    }
}

// MARK: - Main Detail View (TechniqueDetailView)
struct TechniqueDetailView: View {
    let technique: BadmintonTechnique // Ensure BadmintonTechnique struct is defined
    
    @StateObject private var videoVMContainer = VideoVMContainer()
    @StateObject private var comparisonVideoVMContainer = VideoVMContainer()
    
    @State private var showUploadOptions = false
    @State private var isVideoBeingUploadedOrProcessed = false
    @State private var processingStatusMessage = NSLocalizedString("Processing video...", comment: "")
    @State private var analysisError: String? = nil
    
    @State private var isUploadingForComparison = false
    @State private var navigateToComparisonView = false
    
    @State private var shouldShowProcessedPrimaryVideo = false

    @State private var primaryVideoServerFrames: [ServerFrameData]? = nil
    @State private var comparisonVideoServerFrames: [ServerFrameData]? = nil
    @State private var comparisonAnalysisResult: ComparisonResult? = nil
    @State private var isFetchingComparisonAnalysis = false

    @State private var cancellables = Set<AnyCancellable>()
    private let analysisService = TechniqueAnalysisService()
    
    var body: some View {
        ZStack {
            ColorManager.background.ignoresSafeArea()
            
            ScrollView {
                VStack(spacing: 24) {
                    techniqueHeader
                    
                    if isVideoBeingUploadedOrProcessed {
                        processingIndicator
                    } else if isFetchingComparisonAnalysis {
                        VStack(spacing: 16) {
                            ProgressView().progressViewStyle(CircularProgressViewStyle(tint: ColorManager.accentColor)).scaleEffect(1.5)
                            Text(LocalizedStringKey("Comparing techniques..."))
                                .foregroundColor(ColorManager.textPrimary)
                        }
                        .frame(height: 300).frame(maxWidth: .infinity).background(Color.black.opacity(0.05)).cornerRadius(12).padding(.horizontal)
                    } else if let error = analysisError {
                        errorView(error)
                    } else if shouldShowProcessedPrimaryVideo, let primaryVM = videoVMContainer.viewModel {
                        processedVideoView(viewModel: primaryVM)
                        comparisonOptionsView(primaryVM: primaryVM)
                    } else {
                        uploadButton(isComparisonUpload: false)
                    }
                    
                    keyPointsSection
                }
                .padding(.bottom, 32)
            }
            .background(
                NavigationLink(
                    destination: navigationDestinationView(),
                    isActive: $navigateToComparisonView
                ) { EmptyView() }
            )
        }
        .navigationTitle(technique.name)
        .navigationBarTitleDisplayMode(.inline)
        .sheet(isPresented: $showUploadOptions) {
            TechniqueVideoUploadView(technique: technique, isComparison: isUploadingForComparison) { videoURL in
                self.showUploadOptions = false
                if let url = videoURL {
                    DispatchQueue.main.asyncAfter(deadline: .now() + 0.1) {
                        if self.isUploadingForComparison {
                            handleComparisonVideoSelected(url)
                        } else {
                            handlePrimaryVideoSelected(url)
                        }
                    }
                }
            }
        }
    }

    // MARK: - Subviews
    private var techniqueHeader: some View {
        VStack(spacing: 12) {
            ZStack {
                Circle().fill(ColorManager.accentColor.opacity(0.2)).frame(width: 80, height: 80)
                Image(systemName: technique.iconName).font(.system(size: 36)).foregroundColor(ColorManager.accentColor)
            }
            Text(technique.name).font(.title).foregroundColor(ColorManager.textPrimary).multilineTextAlignment(.center)
            Text(technique.description).font(.body).foregroundColor(ColorManager.textSecondary).multilineTextAlignment(.center).padding(.horizontal, 24)
        }.padding(.top, 24)
    }

    private var processingIndicator: some View {
        VStack(spacing: 16) {
            ProgressView().progressViewStyle(CircularProgressViewStyle(tint: ColorManager.accentColor)).scaleEffect(1.5)
            Text(LocalizedStringKey(processingStatusMessage))
                .foregroundColor(ColorManager.textPrimary)
        }
        .frame(height: 300).frame(maxWidth: .infinity).background(Color.black.opacity(0.05)).cornerRadius(12).padding(.horizontal)
    }

    private func errorView(_ errorMessage: String) -> some View {
        VStack(spacing: 16) {
            Image(systemName: "exclamationmark.triangle.fill").font(.largeTitle).foregroundColor(.red)
            Text(LocalizedStringKey("Error")).font(.title2).foregroundColor(ColorManager.textPrimary)
            Text(errorMessage).font(.body).foregroundColor(ColorManager.textSecondary).multilineTextAlignment(.center).padding(.horizontal)
            Button(LocalizedStringKey("Try Upload Again")) {
                resetToInitialUploadState()
                showUploadOptions = true
            }
            .padding().background(ColorManager.accentColor).foregroundColor(.white).cornerRadius(10)
        }
        .padding().frame(maxWidth: .infinity).background(ColorManager.cardBackground.opacity(0.7)).cornerRadius(12).padding(.horizontal)
    }
    
    private func resetToInitialUploadState() {
        analysisError = nil
        isVideoBeingUploadedOrProcessed = false
        isFetchingComparisonAnalysis = false
        
        videoVMContainer.viewModel?.cleanup()
        videoVMContainer.viewModel = nil
        primaryVideoServerFrames = nil
        
        comparisonVideoVMContainer.viewModel?.cleanup()
        comparisonVideoVMContainer.viewModel = nil
        comparisonVideoServerFrames = nil
        
        comparisonAnalysisResult = nil
        isUploadingForComparison = false
        shouldShowProcessedPrimaryVideo = false
        navigateToComparisonView = false
    }


    private func uploadButton(isComparisonUpload: Bool) -> some View {
        UploadButton(
            title: isComparisonUpload ? 
                LocalizedStringKey("Upload Second Video for Comparison") : 
                LocalizedStringKey(String(format: NSLocalizedString("Upload Your %@ Video", comment: ""), technique.name)),
            iconName: "video.badge.plus"
        ) {
            analysisError = nil
            self.isUploadingForComparison = isComparisonUpload
            if !isComparisonUpload {
                comparisonVideoVMContainer.viewModel?.cleanup()
                comparisonVideoVMContainer.viewModel = nil
                comparisonVideoServerFrames = nil
                comparisonAnalysisResult = nil
            }
            showUploadOptions = true
        }
        .padding(.top, 20)
    }
    
    private func processedVideoView(viewModel: VideoPlayerViewModel) -> some View {
        VStack(spacing: 12) {
            Text(LocalizedStringKey("Your Analyzed Video"))
                .font(.headline).foregroundColor(ColorManager.textPrimary)
            VideoPlayerRepresentable(player: viewModel.player, videoRect: .constant(CGRect()))
                .frame(height: 300).cornerRadius(12)
                .overlay(PoseOverlayView(viewModel: viewModel))
                .onAppear { viewModel.play() }
                .onDisappear { viewModel.pause() }
            HStack {
                Button(action: { viewModel.restart() }) { Image(systemName: "backward.end.fill") }
                Button(action: { viewModel.togglePlayPause() }) { Image(systemName: viewModel.isPlaying ? "pause.fill" : "play.fill") }
            }.font(.title2).padding().foregroundColor(ColorManager.accentColor)
        }
        .padding(.horizontal)
    }

    private func comparisonOptionsView(primaryVM: VideoPlayerViewModel) -> some View {
        VStack(spacing: 16) {
            Text(LocalizedStringKey("Next Steps"))
                .font(.title3).fontWeight(.semibold).foregroundColor(ColorManager.textPrimary)

            uploadButton(isComparisonUpload: true)

            if technique.hasModelVideo {
                Button {
                    guard primaryVM.originalServerFrames.first != nil else {
                        analysisError = "Primary video data not available for model comparison."
                        return
                    }
                    handleCompareWithModelVideo(primaryUserVM: primaryVM)
                } label: {
                    Label(LocalizedStringKey("Compare with Model Video"), systemImage: "person.crop.square.filled.and.at.rectangle")
                        .font(.headline).padding().frame(maxWidth: .infinity)
                }
                .buttonStyle(.bordered).tint(ColorManager.accentColor)
            }
            
            Button(LocalizedStringKey("Re-upload Primary Video")) {
                resetToInitialUploadState()
                showUploadOptions = true
            }
            .padding(.top, 10)
            .foregroundColor(ColorManager.accentColor)
        }
        .padding(.horizontal)
    }

    private var keyPointsSection: some View {
        VStack(alignment: .leading, spacing: 16) {
            Text(LocalizedStringKey(String(format: NSLocalizedString("Key Points for %@", comment: ""), technique.name)))
                .font(.headline).foregroundColor(ColorManager.textPrimary)
            KeyPointRow(icon: "figure.play", text: "Start with proper stance, feet shoulder-width apart.")
            KeyPointRow(icon: "hand.raised", text: "Grip the racket with a relaxed, comfortable hold.")
            KeyPointRow(icon: "arrow.up.and.down.and.arrow.left.and.right", text: "Maintain balance throughout the motion.")
            KeyPointRow(icon: "eye", text: "Keep your eye on the shuttle at all times.")
            KeyPointRow(icon: "figure.walk", text: "Follow through with your swing for better control.")
        }
        .padding().background(RoundedRectangle(cornerRadius: 12).fill(ColorManager.cardBackground)).padding(.horizontal)
    }

    @ViewBuilder
    private func navigationDestinationView() -> some View {
        if let userVM = videoVMContainer.viewModel,
           let compVM = comparisonVideoVMContainer.viewModel,
           userVM.isVideoReady, compVM.isVideoReady,
           primaryVideoServerFrames != nil,
           comparisonVideoServerFrames != nil {
            TechniqueComparisonView(
                technique: technique,
                userVideoViewModel: userVM,
                modelVideoViewModel: compVM,
                analysisResult: comparisonAnalysisResult
            )
        } else {
            ProgressView(LocalizedStringKey("Preparing comparison view..."))
        }
    }

    // MARK: - Video Handling & Analysis Logic
    private func handlePrimaryVideoSelected(_ url: URL) {
        isVideoBeingUploadedOrProcessed = true
        processingStatusMessage = NSLocalizedString("Uploading & Analyzing Primary Video...", comment: "")
        analysisError = nil
        shouldShowProcessedPrimaryVideo = false
        
        videoVMContainer.viewModel?.cleanup()
        videoVMContainer.viewModel = nil
        primaryVideoServerFrames = nil

        analysisService.analyzeVideoByUploading(videoURL: url, dominantSide: "Right")
            .sink(receiveCompletion: { completion in
                self.isVideoBeingUploadedOrProcessed = false
                if case let .failure(error) = completion {
                    self.analysisError = "Failed to analyze primary video: \(error.localizedDescription)"
                    self.shouldShowProcessedPrimaryVideo = false
                }
            }, receiveValue: { response in
                print("Primary video analysis successful. Received \(response.totalFrames) frames.")
                self.primaryVideoServerFrames = response.jointDataPerFrame
                
                let newVM = VideoPlayerViewModel(videoURL: url, videoSource: .primary)
                newVM.setServerProcessedJoints(response.jointDataPerFrame)
                self.videoVMContainer.viewModel = newVM
                
                self.listenForVMReadyAndSetShowFlag(vm: newVM, isPrimary: true)
            })
            .store(in: &cancellables)
    }

    private func handleComparisonVideoSelected(_ url: URL) {
        isVideoBeingUploadedOrProcessed = true
        processingStatusMessage = NSLocalizedString("Uploading & Analyzing Comparison Video...", comment: "")
        analysisError = nil
        
        comparisonVideoVMContainer.viewModel?.cleanup()
        comparisonVideoVMContainer.viewModel = nil
        comparisonVideoServerFrames = nil

        analysisService.analyzeVideoByUploading(videoURL: url, dominantSide: "Right")
            .sink(receiveCompletion: { completion in
                self.isVideoBeingUploadedOrProcessed = false
                if case let .failure(error) = completion {
                    self.analysisError = "Failed to analyze comparison video: \(error.localizedDescription)"
                }
            }, receiveValue: { response in
                print("Comparison video analysis successful. Received \(response.totalFrames) frames.")
                self.comparisonVideoServerFrames = response.jointDataPerFrame
                
                let newCompVM = VideoPlayerViewModel(videoURL: url, videoSource: .secondary)
                newCompVM.setServerProcessedJoints(response.jointDataPerFrame)
                self.comparisonVideoVMContainer.viewModel = newCompVM

                self.listenForVMReadyAndSetShowFlag(vm: newCompVM, isPrimary: false)
            })
            .store(in: &cancellables)
    }
    
    private func handleCompareWithModelVideo(primaryUserVM: VideoPlayerViewModel) {
        guard let modelVideoURL = ModelVideoLoader.shared.getModelVideoURL(for: technique.name) else {
            analysisError = "Model video for \(technique.name) not found."
            return
        }
        guard self.primaryVideoServerFrames != nil else {
            analysisError = "Primary video data is missing for model comparison."
            return
        }
        
        isVideoBeingUploadedOrProcessed = true
        processingStatusMessage = NSLocalizedString("Processing Model Video...", comment: "")
        analysisError = nil

        comparisonVideoVMContainer.viewModel?.cleanup()
        comparisonVideoVMContainer.viewModel = nil
        comparisonVideoServerFrames = nil
        
        analysisService.analyzeVideoByUploading(videoURL: modelVideoURL, dominantSide: "Right")
            .sink(receiveCompletion: { completion in
                self.isVideoBeingUploadedOrProcessed = false
                if case let .failure(error) = completion {
                    self.analysisError = "Failed to analyze model video: \(error.localizedDescription)"
                }
            }, receiveValue: { response in
                print("Model video analysis successful. Received \(response.totalFrames) frames.")
                self.comparisonVideoServerFrames = response.jointDataPerFrame
                
                let newModelVM = VideoPlayerViewModel(videoURL: modelVideoURL, videoSource: .secondary)
                newModelVM.setServerProcessedJoints(response.jointDataPerFrame)
                self.comparisonVideoVMContainer.viewModel = newModelVM
                
                self.listenForVMReadyAndSetShowFlag(vm: newModelVM, isPrimary: false)
            })
            .store(in: &cancellables)
    }

    private func listenForVMReadyAndSetShowFlag(vm: VideoPlayerViewModel, isPrimary: Bool) {
        var readyCancellable: AnyCancellable?
        
        readyCancellable = vm.$isVideoReady
            .combineLatest(vm.$accumulatedPoses.map { !$0.isEmpty })
            .filter { $0.0 && $0.1 } // Only proceed when both isVideoReady AND poses are available
            .first()
            .sink { [weak vm] _ in
                guard let strongVM = vm else {
                    readyCancellable?.cancel()
                    return
                }
                if isPrimary {
                    print("Primary VM is ready and poses are set.")
                    self.shouldShowProcessedPrimaryVideo = true
                } else {
                    print("Comparison VM is ready and poses are set.")
                    // Add a slight delay to ensure any remaining setup is complete
                    DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) {
                        self.triggerTechniqueComparison()
                    }
                }
                readyCancellable?.cancel()
            }
        
        if let rc = readyCancellable {
            self.cancellables.insert(rc)
        }
    }

    private func triggerTechniqueComparison() {
        guard let userFrames = primaryVideoServerFrames,
              let modelFrames = comparisonVideoServerFrames else {
            analysisError = "One or both video data sets are missing for comparison."
            isFetchingComparisonAnalysis = false
            return
        }
        
        let primaryVMPath = videoVMContainer.viewModel?.videoURL.lastPathComponent ?? "nil"
        let primaryVMIsReady = videoVMContainer.viewModel?.isVideoReady ?? false
        let comparisonVMPath = comparisonVideoVMContainer.viewModel?.videoURL.lastPathComponent ?? "nil"
        let comparisonVMIsReady = comparisonVideoVMContainer.viewModel?.isVideoReady ?? false

        print("DEBUG: Attempting comparison. Primary VM (\(primaryVMPath)) ready: \(primaryVMIsReady). Comparison VM (\(comparisonVMPath)) ready: \(comparisonVMIsReady)")
        
        guard let userVM = videoVMContainer.viewModel, userVM.isVideoReady,
              let modelVM = comparisonVideoVMContainer.viewModel, modelVM.isVideoReady else {
            analysisError = "Video players not ready for comparison."
            isFetchingComparisonAnalysis = false
            return
        }

        isFetchingComparisonAnalysis = true
        analysisError = nil
        
        print("Calling server for technique comparison with actual data...")
        analysisService.requestTechniqueComparison(
            userFrames: userFrames,
            modelFrames: modelFrames,
            dominantSide: "Right"
        )
        .sink(receiveCompletion: { completion in
            self.isFetchingComparisonAnalysis = false
            if case let .failure(error) = completion {
                self.analysisError = "Technique comparison failed: \(error.localizedDescription)"
                print("Comparison error: \(error)")
            }
        }, receiveValue: { result in
            self.comparisonAnalysisResult = result
            self.navigateToComparisonView = true
            print("Technique comparison successful. Navigating to comparison view.")
        })
        .store(in: &cancellables)
    }
}

class VideoVMContainer: ObservableObject { @Published var viewModel: VideoPlayerViewModel? }
</file>

<file path="MoveInsight/TechniquesListView.swift">
import SwiftUI
import AVKit

// Define a structure for badminton techniques
struct BadmintonTechnique: Identifiable {
    let id = UUID()
    let name: String
    let description: String
    let iconName: String
    
    // Flag to indicate if we have a model video for this technique
    let hasModelVideo: Bool
}

struct TechniquesListView: View {
    // List of all badminton techniques
    let techniques = [
        BadmintonTechnique(
            name: NSLocalizedString("Backhand Clear", comment: "Technique name"),
            description: NSLocalizedString("A clear shot played with the back of the hand facing forward.", comment: "Technique description"),
            iconName: "arrow.left.arrow.right",
            hasModelVideo: ModelVideoLoader.shared.hasModelVideo(for: "Backhand Clear")
        ),
        BadmintonTechnique(
            name: NSLocalizedString("Underhand Clear", comment: "Technique name"),
            description: NSLocalizedString("A defensive shot played from below waist height, sending the shuttle high to the back of the opponent's court.", comment: "Technique description"),
            iconName: "arrow.up.forward",
            hasModelVideo: ModelVideoLoader.shared.hasModelVideo(for: "Underhand Clear")
        ),
        BadmintonTechnique(
            name: NSLocalizedString("Overhead Clear", comment: "Technique name"),
            description: NSLocalizedString("A powerful shot played from above the head, sending the shuttle to the back of the opponent's court.", comment: "Technique description"),
            iconName: "arrow.down.forward",
            hasModelVideo: ModelVideoLoader.shared.hasModelVideo(for: "Overhead Clear")
        ),
        BadmintonTechnique(
            name: NSLocalizedString("Drop Shot", comment: "Technique name"),
            description: NSLocalizedString("A gentle shot that just clears the net and drops sharply on the other side.", comment: "Technique description"),
            iconName: "arrow.down",
            hasModelVideo: ModelVideoLoader.shared.hasModelVideo(for: "Drop Shot")
        ),
        BadmintonTechnique(
            name: NSLocalizedString("Smash", comment: "Technique name"),
            description: NSLocalizedString("A powerful overhead shot hit steeply downward into the opponent's court.", comment: "Technique description"),
            iconName: "bolt.fill",
            hasModelVideo: ModelVideoLoader.shared.hasModelVideo(for: "Smash")
        ),
        BadmintonTechnique(
            name: NSLocalizedString("Net Shot", comment: "Technique name"),
            description: NSLocalizedString("A soft shot played near the net that just clears it and falls close to the net on the other side.", comment: "Technique description"),
            iconName: "power.dotted",
            hasModelVideo: ModelVideoLoader.shared.hasModelVideo(for: "Net Shot")
        )
    ]
    
    var body: some View {
        ZStack {
            ColorManager.background.ignoresSafeArea()
            
            ScrollView {
                VStack(spacing: 16) {
                    Text(LocalizedStringKey("Badminton Techniques"))
                        .font(.title)
                        .foregroundColor(ColorManager.textPrimary)
                        .padding(.top, 24)
                    
                    Text(LocalizedStringKey("Select a technique to upload and analyze your form"))
                        .font(.subheadline)
                        .foregroundColor(ColorManager.textSecondary)
                        .multilineTextAlignment(.center)
                        .padding(.horizontal)
                        .padding(.bottom, 12)
                    
                    LazyVStack(spacing: 16) {
                        ForEach(techniques) { technique in
                            NavigationLink(destination: TechniqueDetailView(technique: technique)) {
                                TechniqueCard(technique: technique)
                            }
                            .buttonStyle(PlainButtonStyle())
                        }
                    }
                    .padding(.horizontal, 16)
                    .padding(.bottom, 24)
                }
            }
        }
        .navigationTitle(LocalizedStringKey("Techniques"))
        .navigationBarTitleDisplayMode(.inline)
    }
}

struct TechniqueCard: View {
    let technique: BadmintonTechnique
    
    var body: some View {
        HStack(spacing: 16) {
            // Technique icon
            ZStack {
                Circle()
                    .fill(ColorManager.accentColor.opacity(0.2))
                    .frame(width: 60, height: 60)
                
                Image(systemName: technique.iconName)
                    .font(.system(size: 24))
                    .foregroundColor(ColorManager.accentColor)
            }
            
            VStack(alignment: .leading, spacing: 4) {
                Text(technique.name)
                    .font(.headline)
                    .foregroundColor(ColorManager.textPrimary)
                
                Text(technique.description)
                    .font(.subheadline)
                    .foregroundColor(ColorManager.textSecondary)
                    .lineLimit(2)
            }
            
            Spacer()
            
            Image(systemName: "chevron.right")
                .foregroundColor(ColorManager.textSecondary)
        }
        .padding(16)
        .background(
            RoundedRectangle(cornerRadius: 12)
                .fill(ColorManager.cardBackground)
        )
    }
}
</file>

<file path="MoveInsight/TechniqueVideoUploadView.swift">
import SwiftUI
import PhotosUI
import AVKit

struct TechniqueVideoUploadView: View {
    let technique: BadmintonTechnique
    let isComparison: Bool 
    let onVideoSelected: (URL?) -> Void
    
    @State private var selectedItem: PhotosPickerItem?
    @State private var showPhotoPicker = false
    @State private var showPermissionAlert = false
    @State private var loadingMessage: String? = nil

    @Environment(\.presentationMode) var presentationMode
    
    var body: some View {
        ZStack {
            ColorManager.background.ignoresSafeArea()
            
            VStack(spacing: 24) {
                // Header
                VStack(spacing: 12) {
                    Text(isComparison ? 
                         LocalizedStringKey("Upload Comparison Video") : 
                         LocalizedStringKey(String(format: NSLocalizedString("Upload %@ Video", comment: ""), technique.name)))
                        .font(.title2)
                        .foregroundColor(ColorManager.textPrimary)
                        .padding(.top, 16)
                    
                    Text(LocalizedStringKey(String(format: NSLocalizedString("Select a video of yourself performing the %@ technique.", comment: ""), technique.name)))
                        .font(.subheadline)
                        .foregroundColor(ColorManager.textSecondary)
                        .multilineTextAlignment(.center)
                        .padding(.horizontal)
                }
                
                Spacer()
                
                if let message = loadingMessage {
                    VStack {
                        ProgressView()
                            .progressViewStyle(CircularProgressViewStyle(tint: ColorManager.accentColor))
                        Text(LocalizedStringKey(message))
                            .foregroundColor(ColorManager.textPrimary)
                            .padding(.top)
                    }
                } else {
                    // Upload button
                    Button(action: {
                        checkPermissionThenPick()
                    }) {
                        VStack(spacing: 16) {
                            Image(systemName: "video.badge.plus")
                                .font(.system(size: 40))
                                .foregroundColor(ColorManager.accentColor)
                            
                            Text(LocalizedStringKey("Select Video from Library"))
                                .font(.headline)
                                .foregroundColor(ColorManager.textPrimary)
                        }
                        .frame(maxWidth: .infinity)
                        .padding(48)
                        .background(
                            RoundedRectangle(cornerRadius: 12)
                                .stroke(ColorManager.accentColor, lineWidth: 2)
                                .background(ColorManager.cardBackground.cornerRadius(12))
                        )
                        .padding(.horizontal, 32)
                    }
                    
                    Text(LocalizedStringKey("For best results, ensure your entire body is visible, and you are performing the technique from start to finish."))
                        .font(.caption)
                        .foregroundColor(ColorManager.textSecondary)
                        .multilineTextAlignment(.center)
                        .padding(.horizontal, 32)
                }
                
                Spacer()
                
                // Bottom buttons
                Button(LocalizedStringKey("Cancel")) {
                    onVideoSelected(nil)
                    presentationMode.wrappedValue.dismiss()
                }
                .padding()
                .foregroundColor(ColorManager.textSecondary)
            }
            .padding(.horizontal, 16)
            .padding(.bottom, 16)
        }
        .photosPicker(
            isPresented: $showPhotoPicker,
            selection: $selectedItem,
            matching: .videos
        )
        .onChange(of: selectedItem) { newItem in
            guard let item = newItem else { return }
            loadingMessage = NSLocalizedString("Loading Video...", comment: "")
            
            // Load the video file URL
            item.loadTransferable(type: VideoItem.self) { result in
                DispatchQueue.main.async {
                    self.loadingMessage = nil 
                    switch result {
                    case .success(let videoItem?):
                        print("Video selected: \(videoItem.url)")
                        onVideoSelected(videoItem.url)
                    case .success(nil):
                        print("Video selection failed: No item returned.")
                        onVideoSelected(nil)
                    case .failure(let error):
                        print("Video selection failed with error: \(error.localizedDescription)")
                        onVideoSelected(nil)
                    }
                }
            }
        }
        .alert(LocalizedStringKey("Photo Library Access Required"), isPresented: $showPermissionAlert) {
            Button(LocalizedStringKey("Go to Settings")) {
                UIApplication.shared.open(URL(string: UIApplication.openSettingsURLString)!)
            }
            Button(LocalizedStringKey("Cancel"), role: .cancel) { }
        }
    }
    
    // MARK: - Permissions
    private func checkPermissionThenPick() {
        let status = PHPhotoLibrary.authorizationStatus(for: .readWrite)
        switch status {
        case .authorized, .limited:
            showPhotoPicker = true
        case .notDetermined:
            PHPhotoLibrary.requestAuthorization(for: .readWrite) { newStatus in
                DispatchQueue.main.async {
                    if newStatus == .authorized || newStatus == .limited {
                        showPhotoPicker = true
                    } else {
                        showPermissionAlert = true
                    }
                }
            }
        default:
            showPermissionAlert = true
        }
    }
}
</file>

<file path="MoveInsight/UIComponents.swift">
import SwiftUI

// MARK: - Custom Upload Button Style
struct UploadButton: View {
    let title: LocalizedStringKey
    let iconName: String
    let action: () -> Void

    var body: some View {
        Button(action: action) {
            HStack {
                Image(systemName: iconName)
                    .font(.system(size: 22))
                Text(title)
                    .font(.headline)
            }
            .foregroundColor(.white) // Always white text
            .frame(maxWidth: .infinity)
            .padding()
            .background(ColorManager.accentColor)
            .cornerRadius(12)
        }
        .padding(.horizontal)
    }
}

// MARK: - Padding Constant
// Define standard padding to use consistently
extension CGFloat {
    static let standard: CGFloat = 16.0
}
</file>

<file path="MoveInsight/VideoPlayerRepresentable.swift">
import SwiftUI
import AVKit

// MARK: - Video Player Representable (UIViewRepresentable)
struct VideoPlayerRepresentable: UIViewRepresentable {
    let player: AVPlayer
    @Binding var videoRect: CGRect // Binding to pass videoRect back out

    // Create the custom UIView subclass
    func makeUIView(context: Context) -> PlayerUIView {
        print("Making PlayerUIView")
        let view = PlayerUIView(player: player)
        // Set the callback to update the binding
        view.onVideoRectChange = { rect in
            DispatchQueue.main.async {
                if self.videoRect != rect {
                    self.videoRect = rect
                }
            }
        }
        return view
    }

    // Update the UIView
    func updateUIView(_ uiView: PlayerUIView, context: Context) {
        if uiView.playerLayer.player !== player {
            print("Updating player instance in PlayerUIView")
            uiView.playerLayer.player = player
        }
        uiView.onVideoRectChange = { rect in
            DispatchQueue.main.async {
                if self.videoRect != rect {
                    self.videoRect = rect
                }
            }
        }
        uiView.playerLayer.videoGravity = .resizeAspect
    }
    
    // Clean up resources if needed
    static func dismantleUIView(_ uiView: PlayerUIView, coordinator: ()) {
        print("Dismantling PlayerUIView")
        uiView.playerLayer.player = nil
        uiView.onVideoRectChange = nil // Clear callback
    }
}

// MARK: - Custom UIView for AVPlayerLayer
class PlayerUIView: UIView {
    // Callback closure to report videoRect changes
    var onVideoRectChange: ((CGRect) -> Void)?
    private var lastKnownVideoRect: CGRect = .zero // Store last rect to avoid redundant callbacks

    // Override the layerClass property to specify AVPlayerLayer
    override static var layerClass: AnyClass {
        AVPlayerLayer.self
    }

    // Convenience accessor for the layer as an AVPlayerLayer
    var playerLayer: AVPlayerLayer {
        return layer as! AVPlayerLayer
    }

    // Initializer to set the player on the layer
    init(player: AVPlayer) {
        super.init(frame: .zero)
        playerLayer.player = player
        playerLayer.videoGravity = .resizeAspect // Ensure video scales correctly
        playerLayer.backgroundColor = UIColor.black.cgColor // Set background color for the layer
        self.backgroundColor = .black // Set background for the view itself
        print("PlayerUIView initialized, player assigned to layer.")
    }

    required init?(coder: NSCoder) {
        fatalError("init(coder:) has not been implemented")
    }
    
    override func layoutSubviews() {
        super.layoutSubviews()
        // Ensure the player layer's frame always matches the view's bounds
        if playerLayer.frame != self.bounds {
            print("LayoutSubviews: Updating playerLayer frame to \(self.bounds)")
            playerLayer.frame = self.bounds
        }
         
        // Get the current videoRect and report if changed
        let currentVideoRect = playerLayer.videoRect
        if currentVideoRect != lastKnownVideoRect && !currentVideoRect.isInfinite && !currentVideoRect.isNull {
            print("LayoutSubviews: videoRect changed to \(currentVideoRect)")
            lastKnownVideoRect = currentVideoRect
            onVideoRectChange?(currentVideoRect) // Call the callback
        }
    }
}
</file>

<file path="MoveInsight/VideoTransferUtils.swift">
import SwiftUI
import PhotosUI

// MARK: - VideoItem Transferable
struct VideoItem: Transferable {
    let url: URL
    
    static var transferRepresentation: some TransferRepresentation {
        FileRepresentation(contentType: .movie) { movie in
            SentTransferredFile(movie.url)
        } importing: { received in
            // Copy to a temporary location to ensure we have access
            let tempDir = FileManager.default.temporaryDirectory
            let fileName = "\(UUID().uuidString).\(received.file.pathExtension)" // Ensure unique filename
            let copyURL = tempDir.appendingPathComponent(fileName)
            
            // Attempt to remove existing file at destination URL before copying
            try? FileManager.default.removeItem(at: copyURL)

            try FileManager.default.copyItem(at: received.file, to: copyURL)
            return Self.init(url: copyURL)
        }
    }
}
</file>

<file path="MoveInsight.xcodeproj/project.xcworkspace/contents.xcworkspacedata">
<?xml version="1.0" encoding="UTF-8"?>
<Workspace
   version = "1.0">
   <FileRef
      location = "self:">
   </FileRef>
</Workspace>
</file>

<file path="MoveInsight.xcodeproj/xcuserdata/charlie.xcuserdatad/xcschemes/xcschememanagement.plist">
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
	<key>SchemeUserState</key>
	<dict>
		<key>MoveInsight.xcscheme_^#shared#^_</key>
		<dict>
			<key>orderHint</key>
			<integer>0</integer>
		</dict>
	</dict>
</dict>
</plist>
</file>

<file path="MoveInsightServer/analysis_server.py">
from fastapi import FastAPI, HTTPException, Request, File, UploadFile, Form
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Dict, List, Any, Optional
import numpy as np
import logging
import time
import traceback
import cv2
import mediapipe as mp
import tempfile
import os

# Import functions from swing_diagnose.py
from swing_diagnose import evaluate_swing_rules, align_keypoints_with_interpolation

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("analysis_server")

# --- FastAPI App Setup ---
app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"],
)

@app.middleware("http")
async def log_requests(request: Request, call_next):
    start_time = time.time()
    client_host = request.client.host if request.client else "unknown"
    logger.info(f"Request started: {request.method} {request.url.path} - Client: {client_host}")
    try:
        response = await call_next(request)
        process_time = time.time() - start_time
        logger.info(f"Request completed: {request.method} {request.url.path} - {response.status_code} in {process_time:.2f}s")
        return response
    except Exception as e:
        logger.error(f"Request failed: {request.method} {request.url.path} - Error: {str(e)} - Traceback: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"Internal Server Error: {str(e)}")

# --- MediaPipe Pose Estimation Setup ---
mp_pose = mp.solutions.pose
JOINT_MAP = {
    0: 'Nose', 11: 'LeftShoulder', 12: 'RightShoulder', 13: 'LeftElbow', 14: 'RightElbow',
    15: 'LeftWrist', 16: 'RightWrist', 23: 'LeftHip', 24: 'RightHip', 25: 'LeftKnee',
    26: 'RightKnee', 27: 'LeftAnkle', 28: 'RightAnkle', 29: 'LeftHeel', 30: 'RightHeel',
    31: 'LeftFootIndex', 32: 'RightFootIndex'
}
JOINT_NAME_REMAPPING = { 'LeftFootIndex': 'LeftToe', 'RightFootIndex': 'RightToe' }

# --- Pydantic Models ---
class JointDataItem(BaseModel):
    x: float
    y: float
    confidence: Optional[float] = None

class FrameDataItem(BaseModel):
    joints: Dict[str, JointDataItem]

class VideoAnalysisResponseModel(BaseModel):
    total_frames: int
    joint_data_per_frame: List[FrameDataItem]
    swing_analysis: Optional[Dict[str, bool]] = None

# For Technique Comparison Endpoint
class TechniqueComparisonRequestDataModel(BaseModel):
    user_video_frames: List[FrameDataItem]
    model_video_frames: List[FrameDataItem]
    dominant_side: str

class ComparisonResultModel(BaseModel):
    user_score: float
    reference_score: float
    similarity: Dict[str, bool]
    user_details: Dict[str, bool]
    reference_details: Dict[str, bool]

# --- Helper Functions ---
def transform_pydantic_to_numpy(joint_data_per_frame_pydantic: List[FrameDataItem]) -> Dict[str, List]:
    """
    Transform pydantic models to format needed for align_keypoints_with_interpolation
    """
    joint_data = {}
    
    if not joint_data_per_frame_pydantic:
        return joint_data
    
    # Get all unique joint names
    all_joint_names = set()
    for frame_data in joint_data_per_frame_pydantic:
        all_joint_names.update(frame_data.joints.keys())
    
    # Initialize lists for each joint
    for joint_name in all_joint_names:
        joint_data[joint_name] = []
    
    # Fill in data frame by frame
    for frame_data in joint_data_per_frame_pydantic:
        for joint_name in all_joint_names:
            if joint_name in frame_data.joints:
                joint_info = frame_data.joints[joint_name]
                joint_data[joint_name].append([joint_info.x, joint_info.y])
    
    return joint_data

def transform_numpy_to_pydantic(keypoints: Dict[str, np.ndarray], frame_count: int) -> List[FrameDataItem]:
    """
    Transform numpy arrays back to pydantic models for response
    """
    result = []
    
    for frame_idx in range(frame_count):
        frame_joints = {}
        for joint_name, points_array in keypoints.items():
            if frame_idx < len(points_array):
                point = points_array[frame_idx]
                frame_joints[joint_name] = JointDataItem(
                    x=float(point[0]), 
                    y=float(point[1]),
                    confidence=0.9  # Default confidence if not available
                )
        
        result.append(FrameDataItem(joints=frame_joints))
    
    return result

# --- API Endpoints ---
@app.post("/analyze/video_upload/", response_model=VideoAnalysisResponseModel)
async def analyze_video_upload(
    file: UploadFile = File(...),
    dominant_side: str = Form("Right")
):
    logger.info(f"Received video upload: {file.filename}, dominant side: {dominant_side}")
    temp_video_path = ""
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as tmp:
            tmp.write(await file.read())
            temp_video_path = tmp.name
        logger.info(f"Video saved temporarily to: {temp_video_path}")

        # Dictionary to store joint data (key: joint name, value: list of [x,y] coordinates)
        joint_data = {}
        frame_count = 0

        with mp_pose.Pose(static_image_mode=False, model_complexity=1, min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose_estimator:
            cap = cv2.VideoCapture(temp_video_path)
            if not cap.isOpened():
                raise HTTPException(status_code=400, detail="Could not open video file.")

            # Initialize joint data dictionary
            for idx, joint_name in JOINT_MAP.items():
                # Apply remapping if needed
                mapped_name = JOINT_NAME_REMAPPING.get(joint_name, joint_name)
                joint_data[mapped_name] = []

            while cap.isOpened():
                success, frame = cap.read()
                if not success: break
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                results = pose_estimator.process(frame_rgb)
                
                if results.pose_landmarks:
                    for idx, lm in enumerate(results.pose_landmarks.landmark):
                        if idx in JOINT_MAP:
                            original_joint_name = JOINT_MAP[idx]
                            joint_name_for_client = JOINT_NAME_REMAPPING.get(original_joint_name, original_joint_name)
                            joint_data[joint_name_for_client].append([lm.x, lm.y])
                
                frame_count += 1
            
            cap.release()
        
        logger.info(f"Processed {frame_count} frames from video.")

        if frame_count == 0:
            logger.warning("No frames detected in the video.")
            return VideoAnalysisResponseModel(total_frames=0, joint_data_per_frame=[], swing_analysis=None)

        # Use align_keypoints_with_interpolation from swing_diagnose.py
        keypoints = align_keypoints_with_interpolation(joint_data, frame_count)
        
        # Convert keypoints to pydantic models for response
        joint_data_per_frame_pydantic = transform_numpy_to_pydantic(keypoints, frame_count)
        
        # Use evaluate_swing_rules from swing_diagnose.py
        swing_analysis_results = None
        required_keys = [
            'RightShoulder', 'LeftShoulder', 'RightElbow', 'LeftElbow', 'RightWrist',
            'RightHip', 'LeftHip', 'RightHeel', 'RightToe', 'LeftHeel', 'LeftToe'
        ]
        
        if all(k in keypoints for k in required_keys):
            try:
                swing_analysis_results = evaluate_swing_rules(keypoints, dominant_side=dominant_side)
                logger.info(f"Swing analysis for dominant side {dominant_side}: {swing_analysis_results}")
            except Exception as e:
                logger.error(f"Error in evaluate_swing_rules: {str(e)}")
                swing_analysis_results = {
                    'shoulder_abduction': False, 'elbow_flexion': False, 'elbow_lower': False,
                    'foot_direction_aligned': False, 'proximal_to_distal_sequence': False,
                    'hip_forward_shift': False, 'trunk_rotation_completed': False
                }
        else:
            logger.warning("Missing required key points for swing evaluation")
            swing_analysis_results = {
                'shoulder_abduction': False, 'elbow_flexion': False, 'elbow_lower': False,
                'foot_direction_aligned': False, 'proximal_to_distal_sequence': False,
                'hip_forward_shift': False, 'trunk_rotation_completed': False
            }

        logger.info(f"Returning {len(joint_data_per_frame_pydantic)} frames of aligned joint data and swing analysis.")
        return VideoAnalysisResponseModel(
            total_frames=frame_count,
            joint_data_per_frame=joint_data_per_frame_pydantic,
            swing_analysis=swing_analysis_results
        )
    finally:
        if temp_video_path and os.path.exists(temp_video_path):
            os.unlink(temp_video_path)
            logger.info(f"Temporary video file {temp_video_path} deleted.")

@app.post("/analyze/technique_comparison/", response_model=ComparisonResultModel)
async def analyze_technique_comparison(data: TechniqueComparisonRequestDataModel):
    logger.info(f"Received technique comparison request for dominant side: {data.dominant_side}")

    # Transform pydantic models to format needed for processing
    user_joint_data = transform_pydantic_to_numpy(data.user_video_frames)
    model_joint_data = transform_pydantic_to_numpy(data.model_video_frames)
    
    user_frame_count = len(data.user_video_frames)
    model_frame_count = len(data.model_video_frames)
    
    logger.info(f"Processing {user_frame_count} user video frames and {model_frame_count} model video frames for comparison.")
    
    # Use align_keypoints_with_interpolation to process the data
    user_keypoints = align_keypoints_with_interpolation(user_joint_data, user_frame_count)
    model_keypoints = align_keypoints_with_interpolation(model_joint_data, model_frame_count)
    
    # Use evaluate_swing_rules to analyze both videos
    user_swing_details = evaluate_swing_rules(user_keypoints, data.dominant_side)
    model_swing_details = evaluate_swing_rules(model_keypoints, data.dominant_side)
    
    # Calculate scores and similarity
    num_criteria = len(user_swing_details)
    
    user_correct_criteria = sum(1 for v in user_swing_details.values() if v)
    user_score = (user_correct_criteria / num_criteria) * 100.0 if num_criteria > 0 else 0.0

    model_correct_criteria = sum(1 for v in model_swing_details.values() if v)
    reference_score = (model_correct_criteria / num_criteria) * 100.0 if num_criteria > 0 else 0.0

    similarity = {}
    for rule_name in user_swing_details.keys():
        similarity[rule_name] = (user_swing_details.get(rule_name, False) == model_swing_details.get(rule_name, False))
    
    logger.info(f"Comparison complete: User Score {user_score}, Reference Score {reference_score}")

    return ComparisonResultModel(
        user_score=user_score,
        reference_score=reference_score,
        similarity=similarity,
        user_details=user_swing_details,
        reference_details=model_swing_details
    )

@app.get("/")
async def root():
    return {"status": "MoveInsight Analysis Server is running", "version": "1.2.0 - Using swing_diagnose.py"}

if __name__ == "__main__":
    import uvicorn
    logger.info("Starting MoveInsight Analysis Server...")
    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")
</file>

<file path="MoveInsight/Assets.xcassets/AppIcon.appiconset/Contents.json">
{
  "images" : [
    {
      "filename" : "d938011a54572aa285aa038b0f9eea3043b99b8ea3d567c2b9f568038a243413.png",
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    },
    {
      "appearances" : [
        {
          "appearance" : "luminosity",
          "value" : "dark"
        }
      ],
      "filename" : "d938011a54572aa285aa038b0f9eea3043b99b8ea3d567c2b9f568038a243413 1.png",
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    },
    {
      "appearances" : [
        {
          "appearance" : "luminosity",
          "value" : "tinted"
        }
      ],
      "filename" : "d938011a54572aa285aa038b0f9eea3043b99b8ea3d567c2b9f568038a243413 2.png",
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}
</file>

<file path="MoveInsight/BodyPoseTypes.swift">
import SwiftUI
import Vision
import simd

// MARK: - Body Connection Structure
// Defines a connection between two body joints for drawing the skeleton
struct BodyConnection: Identifiable {
    let id = UUID()
    let from: VNHumanBodyPoseObservation.JointName
    let to: VNHumanBodyPoseObservation.JointName
}

// MARK: - 3D Pose Point
// Represents a 3D pose point with position in world space
struct Pose3DPoint {
    let jointName: VNHumanBodyPoseObservation.JointName
    let position: SIMD3<Float> // x, y, z coordinates
    let confidence: Float
    
    init(jointName: VNHumanBodyPoseObservation.JointName,
         position: SIMD3<Float>,
         confidence: Float = 1.0) {
        self.jointName = jointName
        self.position = position
        self.confidence = confidence
    }
}

// MARK: - 3D Pose Body
// Represents a complete 3D human pose
struct Pose3DBody: Identifiable {
    let id = UUID()
    let joints: [VNHumanBodyPoseObservation.JointName: Pose3DPoint]
    let videoSource: VideoSource // Identifies which video this pose came from
    
    enum VideoSource {
        case primary
        case secondary
    }
    
    // Fixed initializer with proper closure parameters
    init(joints: [VNHumanBodyPoseObservation.JointName: SIMD3<Float>],
         videoSource: VideoSource) {
        self.videoSource = videoSource
        
        // Create a dictionary with jointName -> Pose3DPoint mapping
        var posePoints: [VNHumanBodyPoseObservation.JointName: Pose3DPoint] = [:]
        for (jointName, position) in joints {
            posePoints[jointName] = Pose3DPoint(jointName: jointName, position: position)
        }
        self.joints = posePoints
    }
}
</file>

<file path="MoveInsight/ContentView.swift">
import SwiftUI

struct ContentView: View {
    @State private var selectedTab = 0

    var body: some View {
        NavigationView {
            ZStack {
                // Main background
                ColorManager.background.ignoresSafeArea()

                // Content area based on selected tab
                VStack {
                    TabView(selection: $selectedTab) {
                        HomeView()
                            .tag(0)
                        
                        Text(LocalizedStringKey("Training Screen"))
                            .foregroundColor(ColorManager.textPrimary)
                            .tag(1)
                        
                        // Use the new UploadTabView directly in the Upload tab
                        UploadTabView()
                            .tag(2)
                        
                        Text(LocalizedStringKey("Videos Screen"))
                            .foregroundColor(ColorManager.textPrimary)
                            .tag(3)
                        
                        Text(LocalizedStringKey("Messages Screen"))
                            .foregroundColor(ColorManager.textPrimary)
                            .tag(4)
                    }
                    .tabViewStyle(PageTabViewStyle(indexDisplayMode: .never))
                    
                    // Custom tab bar now using the evenly-spaced implementation
                    CustomTabBar(selectedTab: $selectedTab)
                }
            }
            .navigationBarHidden(true)
        }
    }
}

// MARK: - Previews
struct ContentView_Previews: PreviewProvider {
    static var previews: some View {
        ContentView()
            .preferredColorScheme(.dark)
    }
}
</file>

<file path="MoveInsight/Extensions.swift">
import SwiftUI
import Combine

// MARK: - Publisher Extension
extension AnyCancellable {
    func cancel(after interval: TimeInterval) {
        DispatchQueue.main.asyncAfter(deadline: .now() + interval) {
            self.cancel()
        }
    }
}

// MARK: - View Extensions
extension View {
    // Apply a conditional modifier
    @ViewBuilder func applyIf<Content: View>(_ condition: Bool, content: (Self) -> Content) -> some View {
        if condition {
            content(self)
        } else {
            self
        }
    }
    
    // Add a shake effect to a view
    func shake(amount: CGFloat = 5, shakesPerUnit: CGFloat = 3, animationDuration: CGFloat = 0.7, isShaking: Bool = true) -> some View {
        self.modifier(ShakeEffect(amount: amount, shakesPerUnit: shakesPerUnit, animationDuration: animationDuration, isShaking: isShaking))
    }
}

// MARK: - Navigation Extensions
extension View {
    // Create a navigation link that's programmatically triggered
    func navigationLinkWithDestination<Destination: View>(isActive: Binding<Bool>, @ViewBuilder destination: @escaping () -> Destination) -> some View {
        ZStack {
            self
            
            NavigationLink(
                destination: destination(),
                isActive: isActive
            ) {
                EmptyView()
            }
            .hidden()
        }
    }
}

// MARK: - Shake Effect Modifier
struct ShakeEffect: ViewModifier {
    var amount: CGFloat = 5
    var shakesPerUnit: CGFloat = 3
    var animationDuration: CGFloat = 0.7
    var isShaking: Bool = true
    
    func body(content: Content) -> some View {
        content
            .offset(x: isShaking ? amount * sin(shakesPerUnit * .pi * animationDuration) : 0)
            .animation(
                isShaking ?
                    Animation.easeInOut(duration: animationDuration)
                    .repeatForever(autoreverses: true) :
                    .default,
                value: isShaking
            )
    }
}

// MARK: - Dynamic Height Modifier
struct DynamicHeightModifier: ViewModifier {
    @Binding var height: CGFloat
    
    func body(content: Content) -> some View {
        content
            .background(
                GeometryReader { geometry -> Color in
                    DispatchQueue.main.async {
                        self.height = geometry.size.height
                    }
                    return Color.clear
                }
            )
    }
}
</file>

<file path="MoveInsight/HomeView.swift">
import SwiftUI

struct HomeView: View {
    // Sample user data – name typically wouldn't be localized
    let username = "Zhang Wei"
    
    var body: some View {
        ZStack {
            // Main background
            ColorManager.background.ignoresSafeArea()
            
            // Content
            ScrollView {
                VStack(alignment: .leading, spacing: 24) {
                    // User greeting
                    HStack {
                        VStack(alignment: .leading) {
                            Text(LocalizedStringKey("Good morning,"))
                                .font(.system(size: 16))
                                .foregroundColor(ColorManager.textSecondary)
                            Text(username)
                                .font(.system(size: 24, weight: .bold))
                                .foregroundColor(ColorManager.textPrimary)
                        }
                        
                        Spacer()
                        
                        // Profile image
                        Image(systemName: "person.crop.circle.fill")
                            .resizable()
                            .frame(width: 40, height: 40)
                            .foregroundColor(ColorManager.accentColor)
                            .background(ColorManager.cardBackground)
                            .clipShape(Circle())
                    }
                    .padding(.top, 12)
                    
                    // Match Performance Card
                    PerformanceCard()
                    
                    // Technicals Section - Updated to navigate to TechniquesListView
                    NavigationLink(destination: TechniquesListView()) {
                        SectionCard(title: LocalizedStringKey("Technicals"))
                    }
                    
                    // Training Goals
                    NavigationLink(destination: Text(LocalizedStringKey("Training Goals Detail"))) {
                        GoalsCard()
                    }
                    
                    // Tutorials Section
                    Text(LocalizedStringKey("Tutorials Specifically For You"))
                        .font(.headline)
                        .foregroundColor(ColorManager.textPrimary)
                        .padding(.top, 5)
                }
                .padding(.horizontal, 16)
                .padding(.bottom, 20)
            }
        }
    }
}

// MARK: - Performance Card
struct PerformanceCard: View {
    var body: some View {
        VStack(alignment: .leading, spacing: 15) {
            HStack {
                Text(LocalizedStringKey("Match Performance"))
                    .font(.headline)
                    .foregroundColor(ColorManager.textPrimary)
                
                Spacer()
                
                Image(systemName: "arrow.up.right.square")
                    .foregroundColor(ColorManager.textSecondary)
            }
            
            HStack(alignment: .top, spacing: 15) {
                // Improvement stat
                VStack(alignment: .leading, spacing: 2) {
                    HStack {
                        Image(systemName: "chart.line.uptrend.xyaxis")
                            .foregroundColor(ColorManager.accentColor)
                        
                        Text(LocalizedStringKey("2.3%"))
                            .fontWeight(.bold)
                            .foregroundColor(ColorManager.textPrimary)
                    }
                    
                    Text(LocalizedStringKey("/ last week"))
                        .font(.caption)
                        .foregroundColor(ColorManager.textSecondary)
                    
                    Text(LocalizedStringKey("Well done on swing path!"))
                        .font(.caption)
                        .foregroundColor(ColorManager.textSecondary)
                        .padding(.top, 5)
                }
                
                Spacer()
                
                // Rating gauge using only purple with a gradient
                ZStack {
                    Circle()
                        .trim(from: 0, to: 0.75)
                        .stroke(
                            AngularGradient(
                                gradient: Gradient(colors: [ColorManager.accentColor.opacity(0.6), ColorManager.accentColor]),
                                center: .center,
                                startAngle: .degrees(0),
                                endAngle: .degrees(270)
                            ),
                            style: StrokeStyle(lineWidth: 8, lineCap: .round)
                        )
                        .frame(width: 80, height: 80)
                        .rotationEffect(.degrees(135))
                    
                    Text(LocalizedStringKey("4.3"))
                        .font(.system(size: 24, weight: .bold))
                        .foregroundColor(ColorManager.textPrimary)
                }
            }
            
            // Progress chart
            Chart()
                .frame(height: 120)
                .padding(.vertical, 8)
            
            // Match history link
            HStack {
                Text(LocalizedStringKey("Match History"))
                    .foregroundColor(ColorManager.textSecondary)
                    .font(.subheadline)
                
                Spacer()
                
                Image(systemName: "chevron.right")
                    .foregroundColor(ColorManager.textSecondary)
                    .font(.caption)
            }
        }
        .padding(20)
        .background(
            RoundedRectangle(cornerRadius: 16)
                .fill(ColorManager.cardBackground.opacity(0.8))
        )
    }
}

// MARK: - Simple Chart Component
struct Chart: View {
    let months = ["Jan", "Feb", "Mar", "Apr", "May"]
    
    var body: some View {
        GeometryReader { geometry in
            let width = geometry.size.width
            let height = geometry.size.height
            
            HStack(spacing: 0) {
                ForEach(months, id: \.self) { month in
                    Text(LocalizedStringKey(month))
                        .font(.system(size: 8))
                        .foregroundColor(ColorManager.textSecondary)
                        .frame(width: width / CGFloat(months.count))
                }
            }
            .position(x: width / 2, y: height - 5)
            
            VStack(spacing: 8) {
                ForEach(["4", "3", "2", "1", "0"], id: \.self) { value in
                    Text(value)
                        .font(.system(size: 8))
                        .foregroundColor(ColorManager.textSecondary)
                }
            }
            .position(x: 8, y: height / 2)
            
            Path { path in
                let points = [
                    CGPoint(x: width * 0.1, y: height * 0.7),
                    CGPoint(x: width * 0.3, y: height * 0.5),
                    CGPoint(x: width * 0.5, y: height * 0.4),
                    CGPoint(x: width * 0.7, y: height * 0.35),
                    CGPoint(x: width * 0.9, y: height * 0.3)
                ]
                
                path.move(to: points[0])
                for point in points.dropFirst() {
                    path.addLine(to: point)
                }
            }
            .stroke(ColorManager.textPrimary, lineWidth: 1.5)
        }
    }
}

// MARK: - Section Card
struct SectionCard: View {
    let title: LocalizedStringKey
    
    var body: some View {
        HStack {
            Text(title)
                .font(.headline)
                .foregroundColor(ColorManager.textPrimary)
            
            Spacer()
            
            Image(systemName: "chevron.right")
                .foregroundColor(ColorManager.textSecondary)
        }
        .padding(20)
        .background(
            RoundedRectangle(cornerRadius: 16)
                .fill(ColorManager.cardBackground.opacity(0.8))
        )
    }
}

// MARK: - Goals Card
struct GoalsCard: View {
    var body: some View {
        HStack {
            VStack(alignment: .leading) {
                Text(LocalizedStringKey("Training Goals"))
                    .font(.headline)
                    .foregroundColor(ColorManager.textPrimary)
                
                HStack(spacing: 8) {
                    ForEach(0..<4) { _ in
                        Image(systemName: "checkmark.circle.fill")
                            .foregroundColor(.green)
                    }
                    
                    Image(systemName: "checkmark.circle.fill")
                        .foregroundColor(ColorManager.textSecondary.opacity(0.5))
                }
            }
            
            Spacer()
            
            Image(systemName: "chevron.right")
                .foregroundColor(ColorManager.textSecondary)
        }
        .padding(20)
        .background(
            RoundedRectangle(cornerRadius: 16)
                .fill(ColorManager.cardBackground.opacity(0.8))
        )
    }
}
</file>

<file path="MoveInsight/MoveInsightApp.swift">
import SwiftUI

@main
struct MoveInsightApp: App {
    init() {
        // Set the accent color for the entire app
        UINavigationBar.appearance().tintColor = UIColor(Color.accentColor)
    }
    
    var body: some Scene {
        WindowGroup {
            ContentView()
                .accentColor(ColorManager.accentColor)
        }
    }
}

// For iOS 17+, we can use the newer API too
#if swift(>=5.9)
extension MoveInsightApp {
    @ViewBuilder
    private func contentWithTint() -> some View {
        if #available(iOS 17.0, *) {
            ContentView()
                .preferredColorScheme(.dark)
                .tint(ColorManager.accentColor)
        } else {
            ContentView()
                .preferredColorScheme(.dark)
                .accentColor(ColorManager.accentColor)
        }
    }
}
#endif
</file>

<file path="MoveInsight/TechniqueComparisonView.swift">
import SwiftUI
import AVKit
import Combine

// MARK: - Data Structures for Analysis Results

// This structure must match the JSON response from your server's /analyze/comparison endpoint
struct ComparisonResult: Codable {
    let userScore: Double
    let referenceScore: Double
    let similarity: [String: Bool]
    let userDetails: [String: Bool]
    let referenceDetails: [String: Bool]

    enum CodingKeys: String, CodingKey {
        case userScore = "user_score"
        case referenceScore = "reference_score"
        case similarity
        case userDetails = "user_details"
        case referenceDetails = "reference_details"
    }
}

// Feedback item component (defined locally for this view)
struct FeedbackItem: View {
    let title: String
    let description: String
    let score: Int // Represents a visual score (e.g., 0-100) for the item color

    var body: some View {
        HStack(alignment: .center, spacing: 16) {
            // Score circle
            ZStack {
                Circle()
                    .fill(scoreColor.opacity(0.2))
                    .frame(width: 40, height: 40)
                
                Text("\(score)%") // Displaying the visual score
                    .font(.system(size: 12, weight: .bold))
                    .foregroundColor(scoreColor)
            }
            
            // Feedback text
            VStack(alignment: .leading, spacing: 4) {
                Text(title)
                    .font(.subheadline)
                    .foregroundColor(ColorManager.textPrimary)
                
                Text(description)
                    .font(.caption)
                    .foregroundColor(ColorManager.textSecondary)
                    .lineLimit(nil) // Allow multiple lines for description
            }
            Spacer() // Ensure content aligns left
        }
    }
    
    // Color based on the score
    private var scoreColor: Color {
        if score >= 90 {
            return .green
        } else if score >= 75 {
            return .yellow // Or a more distinct "good but not perfect" color
        } else if score >= 60 {
            return .orange
        } else {
            return .red
        }
    }
}


// MARK: - Main Comparison View
struct TechniqueComparisonView: View {
    let technique: BadmintonTechnique
    @ObservedObject var userVideoViewModel: VideoPlayerViewModel
    @ObservedObject var modelVideoViewModel: VideoPlayerViewModel
    
    @State private var comparisonMode: ComparisonMode = .sideBySide
    @State private var selectedReportTab: ReportTab = .overview
    
    // Analysis state - This should be populated by the calling view or a service
    @State var analysisResult: ComparisonResult? // Made non-private for potential external setting
    @State private var isAnalyzing = false // Controls local loading indicators if analysis is re-fetched
    @State private var analysisError: String? = nil
    @State private var cancellables = Set<AnyCancellable>()
    
    enum ComparisonMode {
        case sideBySide
        case overlay3D
    }
    
    enum ReportTab {
        case overview
        case technical
    }
    
    var body: some View {
        ZStack {
            ColorManager.background.ignoresSafeArea()
            
            ScrollView {
                VStack(spacing: 24) {
                    Text(LocalizedStringKey(String(format: NSLocalizedString("%@ Analysis", comment: ""), technique.name)))
                        .font(.title2).foregroundColor(ColorManager.textPrimary).padding(.top, 16)
                    
                    Picker("Comparison Mode", selection: $comparisonMode) {
                        Text(LocalizedStringKey("Side by Side")).tag(ComparisonMode.sideBySide)
                        Text(LocalizedStringKey("3D Overlay")).tag(ComparisonMode.overlay3D)
                    }
                    .pickerStyle(SegmentedPickerStyle()).padding(.horizontal, 20)
                    
                    if comparisonMode == .sideBySide {
                        sideBySideComparisonView
                    } else {
                        overlay3DComparisonView
                    }
                    
                    Picker("Report Type", selection: $selectedReportTab) {
                        Text(LocalizedStringKey("Overview")).tag(ReportTab.overview)
                        Text(LocalizedStringKey("Technical")).tag(ReportTab.technical)
                    }
                    .pickerStyle(SegmentedPickerStyle()).padding(.horizontal, 20).padding(.top, 8)
                    
                    switch selectedReportTab {
                    case .overview:
                        overviewAnalysisSection
                    case .technical:
                        technicalAnalysisSection
                    }
                }
                .padding(.bottom, 32)
            }
        }
        .navigationTitle(LocalizedStringKey("Technique Analysis"))
        .navigationBarTitleDisplayMode(.inline)
        .onAppear {
            modelVideoViewModel.player.isMuted = true
            userVideoViewModel.player.isMuted = false
            userVideoViewModel.play()
            modelVideoViewModel.play()
            
            // If analysisResult is nil when the view appears, it means it wasn't passed.
            // The analysis should ideally be fetched by the parent view (TechniqueDetailView)
            // and passed into this view.
            if analysisResult == nil && !isAnalyzing {
                 print("TechniqueComparisonView appeared. Analysis result is nil. Consider fetching or ensuring it's passed.")
                 // If this view is responsible for fetching, you'd call a fetch method here.
                 // For example: fetchAnalysisData()
            }
        }
        .onDisappear {
            userVideoViewModel.pause()
            modelVideoViewModel.pause()
            cancellables.forEach { $0.cancel() }
            cancellables.removeAll()
        }
    }
    
    // MARK: - Subviews for Comparison Modes
    private var sideBySideComparisonView: some View {
        VStack(spacing: 16) {
            HStack(spacing: 8) {
                videoPlayerCard(title: LocalizedStringKey("Your Technique"), viewModel: userVideoViewModel, borderColor: .blue)
                videoPlayerCard(title: LocalizedStringKey("Model Technique"), viewModel: modelVideoViewModel, borderColor: .red)
            }
            .padding(.horizontal, 8)
            
            playbackControls()
        }
    }

    private func videoPlayerCard(title: LocalizedStringKey, viewModel: VideoPlayerViewModel, borderColor: Color) -> some View {
        VStack {
            Text(title).font(.subheadline).foregroundColor(ColorManager.textPrimary)
            VideoPlayerRepresentable(player: viewModel.player, videoRect: .constant(CGRect()))
                .frame(height: 240).cornerRadius(12)
                .overlay(PoseOverlayView(viewModel: viewModel))
                .overlay(RoundedRectangle(cornerRadius: 12).stroke(borderColor, lineWidth: 2))
        }
        .frame(maxWidth: .infinity)
    }

    private func playbackControls() -> some View {
        HStack {
            Button(action: {
                if userVideoViewModel.isPlaying {
                    userVideoViewModel.pause(); modelVideoViewModel.pause()
                } else {
                    userVideoViewModel.play(); modelVideoViewModel.play()
                }
            }) { Image(systemName: userVideoViewModel.isPlaying ? "pause.fill" : "play.fill") }
            
            Button(action: {
                userVideoViewModel.restart(); modelVideoViewModel.restart()
            }) { Image(systemName: "backward.end.fill") }
        }
        .font(.title2).padding().foregroundColor(ColorManager.accentColor)
    }
    
    private var overlay3DComparisonView: some View {
        VStack(spacing: 16) {
            Text(LocalizedStringKey("3D Overlay (Conceptual)")).font(.headline).foregroundColor(ColorManager.textPrimary)
            Text(LocalizedStringKey("Note: 3D rendering is currently simplified.")).font(.caption).foregroundColor(ColorManager.textSecondary).padding(.horizontal).multilineTextAlignment(.center)
            CombinedVideo2DComparisonView(primaryViewModel: userVideoViewModel, secondaryViewModel: modelVideoViewModel)
                .frame(height: 400).cornerRadius(12)
                .overlay(RoundedRectangle(cornerRadius: 12).stroke(ColorManager.accentColor, lineWidth: 2))
                .padding(.horizontal, 16)
        }
    }
    
    // MARK: - Analysis Display Sections
    private var overviewAnalysisSection: some View {
        VStack(alignment: .leading, spacing: 20) {
            Text(LocalizedStringKey("Analysis & Feedback")).font(.headline).foregroundColor(ColorManager.textPrimary).padding(.horizontal, 20)
            
            if isAnalyzing {
                loadingView(message: LocalizedStringKey("Analyzing your technique..."))
            } else if let error = analysisError {
                errorDisplayView(error)
            } else if let result = analysisResult {
                // Corrected: Accessing properties of ComparisonResult
                techniqueScoreView(score: result.userScore, title: LocalizedStringKey("Overall Technique Score"), subtitle: LocalizedStringKey("Compared to model performance"))
                feedbackDetailsView(details: result.userDetails) // Corrected: Passing userDetails
            } else {
                noAnalysisDataView()
            }
        }
    }

    private var technicalAnalysisSection: some View {
        VStack(alignment: .leading, spacing: 20) {
            Text(LocalizedStringKey("Technical Report")).font(.headline).foregroundColor(ColorManager.textPrimary).padding(.horizontal, 20)
            
            if isAnalyzing {
                loadingView(message: LocalizedStringKey("Loading technical report..."))
            } else if let error = analysisError {
                errorDisplayView(error)
            } else if let result = analysisResult {
                // Corrected: Accessing properties of ComparisonResult
                scoresComparisonView(userScore: result.userScore, referenceScore: result.referenceScore)
                technicalElementsView(similarity: result.similarity, userDetails: result.userDetails, referenceDetails: result.referenceDetails)
                improvementSuggestionsView(userDetails: result.userDetails) // Corrected: Passing userDetails
            } else {
                noAnalysisDataView(message: LocalizedStringKey("Technical analysis data not available."))
            }
        }
    }

    // MARK: - Helper Views for Analysis Display
    private func loadingView(message: LocalizedStringKey) -> some View {
        HStack {
            Spacer()
            VStack(spacing: 16) {
                ProgressView().progressViewStyle(CircularProgressViewStyle(tint: ColorManager.accentColor)).scaleEffect(1.5)
                Text(message).font(.subheadline).foregroundColor(ColorManager.textPrimary)
            }
            .padding(.vertical, 40)
            Spacer()
        }
    }
    
    private func errorDisplayView(_ error: String) -> some View {
        VStack(alignment: .center, spacing: 16) {
            Image(systemName: "exclamationmark.triangle.fill").font(.system(size: 40)).foregroundColor(.orange)
            Text(LocalizedStringKey("Analysis Error")).font(.headline).foregroundColor(ColorManager.textPrimary)
            Text(error).font(.subheadline).foregroundColor(ColorManager.textSecondary).multilineTextAlignment(.center)
            Button(LocalizedStringKey("Retry")) {
                // Implement retry logic if this view is responsible for fetching
                // fetchAnalysisData()
                print("Retry tapped. Implement fetching logic if needed.")
            }
            .padding().buttonStyle(.borderedProminent).tint(ColorManager.accentColor)
        }
        .frame(maxWidth: .infinity).padding(.vertical, 30).padding(.horizontal, 20)
        .background(ColorManager.cardBackground.opacity(0.5)).cornerRadius(12).padding(.horizontal, 20)
    }

    private func noAnalysisDataView(message: LocalizedStringKey = LocalizedStringKey("Analysis data not available. Please ensure the video was processed.")) -> some View {
        VStack {
            Text(message)
                .font(.caption).foregroundColor(ColorManager.textSecondary)
                .padding(.horizontal, 20).multilineTextAlignment(.center)
            Button(LocalizedStringKey("Refresh Analysis")) { // Placeholder for re-fetching
                 // fetchAnalysisData()
                 print("Refresh Analysis Tapped. Implement fetching logic if needed.")
            }
            .padding().buttonStyle(.bordered).tint(ColorManager.accentColor)
        }
        .padding(.vertical, 20)
    }

    private func techniqueScoreView(score: Double, title: LocalizedStringKey, subtitle: LocalizedStringKey) -> some View {
        HStack {
            VStack(alignment: .leading, spacing: 4) {
                Text(title).font(.subheadline).foregroundColor(ColorManager.textPrimary)
                Text(subtitle).font(.caption).foregroundColor(ColorManager.textSecondary)
            }
            Spacer()
            scoreRingView(score: score, color: ColorManager.accentColor, size: 70)
        }
        .padding(.horizontal, 20).padding(.vertical, 16)
        .background(ColorManager.cardBackground.opacity(0.5)).cornerRadius(12).padding(.horizontal, 20)
    }

    // Corrected: Passing `userDetails` from `ComparisonResult`
    private func feedbackDetailsView(details: [String: Bool]) -> some View {
        VStack(alignment: .leading, spacing: 12) {
            Text(LocalizedStringKey("Key Technique Elements")).font(.subheadline).foregroundColor(ColorManager.textPrimary)
            ForEach(Array(details.keys.sorted()), id: \.self) { key in // Iterating over userDetails
                let passed = details[key] ?? false
                FeedbackItem( // This now correctly finds FeedbackItem struct
                    title: formatRuleName(key),
                    description: getDescription(for: key, passed: passed),
                    score: passed ? 95 : 65 // Example visual scores
                )
            }
        }
        .padding(20).background(ColorManager.cardBackground.opacity(0.5)).cornerRadius(12).padding(.horizontal, 20)
    }

    // Corrected: Passing scores from `ComparisonResult`
    private func scoresComparisonView(userScore: Double, referenceScore: Double) -> some View {
        VStack(alignment: .leading, spacing: 16) {
            Text(LocalizedStringKey("Scores: You vs. Model")).font(.subheadline).foregroundColor(ColorManager.textPrimary)
            HStack(spacing: 20) {
                scoreRingView(title: LocalizedStringKey("Your Score"), score: userScore, color: .blue, size: 70)
                scoreRingView(title: LocalizedStringKey("Model Score"), score: referenceScore, color: .red, size: 70)
            }.frame(maxWidth: .infinity)
        }
        .padding(20).background(ColorManager.cardBackground.opacity(0.5)).cornerRadius(12).padding(.horizontal, 20)
    }

    private func scoreRingView(title: LocalizedStringKey? = nil, score: Double, color: Color, size: CGFloat) -> some View {
        VStack(alignment: .center, spacing: 8) {
            if let title = title {
                Text(title).font(.caption).foregroundColor(ColorManager.textSecondary)
            }
            ZStack {
                Circle().stroke(color.opacity(0.3), lineWidth: 8).frame(width: size, height: size)
                Circle().trim(from: 0, to: CGFloat(score / 100.0))
                    .stroke(color, style: StrokeStyle(lineWidth: 8, lineCap: .round))
                    .frame(width: size, height: size).rotationEffect(.degrees(-90))
                Text("\(Int(score))%").font(.system(size: size * 0.25, weight: .bold)).foregroundColor(ColorManager.textPrimary)
            }
        }.frame(maxWidth: .infinity)
    }
    
    // Corrected: Passing `similarity`, `userDetails`, `referenceDetails` from `ComparisonResult`
    private func technicalElementsView(similarity: [String: Bool], userDetails: [String: Bool], referenceDetails: [String: Bool]) -> some View {
        VStack(alignment: .leading, spacing: 12) {
            Text(LocalizedStringKey("Technical Elements Breakdown")).font(.subheadline).foregroundColor(ColorManager.textPrimary)
            HStack {
                Text(LocalizedStringKey("Element")).font(.system(size: 14, weight: .semibold)).foregroundColor(ColorManager.textSecondary).frame(maxWidth: .infinity, alignment: .leading)
                Text(LocalizedStringKey("You")).font(.system(size: 14, weight: .semibold)).foregroundColor(ColorManager.textSecondary).frame(width: 60, alignment: .center)
                Text(LocalizedStringKey("Model")).font(.system(size: 14, weight: .semibold)).foregroundColor(ColorManager.textSecondary).frame(width: 60, alignment: .center)
            }.padding(.bottom, 4)

            ForEach(Array(similarity.keys.sorted()), id: \.self) { key in // Iterating over similarity keys
                let userPassed = userDetails[key] ?? false
                let modelPassed = referenceDetails[key] ?? false
                HStack {
                    Text(formatRuleName(key)).font(.system(size: 15)).foregroundColor(ColorManager.textPrimary).frame(maxWidth: .infinity, alignment: .leading)
                    Image(systemName: userPassed ? "checkmark.circle.fill" : "xmark.circle.fill").foregroundColor(userPassed ? .green : .orange).frame(width: 60, alignment: .center)
                    Image(systemName: modelPassed ? "checkmark.circle.fill" : "xmark.circle.fill").foregroundColor(modelPassed ? .green : .orange).frame(width: 60, alignment: .center)
                }
                .padding(.vertical, 6)
                .background((userPassed == modelPassed) ? Color.clear : Color.yellow.opacity(0.15)).cornerRadius(4)
            }
        }
        .padding(20).background(ColorManager.cardBackground.opacity(0.5)).cornerRadius(12).padding(.horizontal, 20)
    }

    // Corrected: Passing `userDetails` from `ComparisonResult`
    private func improvementSuggestionsView(userDetails: [String: Bool]) -> some View {
        VStack(alignment: .leading, spacing: 12) {
            Text(LocalizedStringKey("Improvement Suggestions")).font(.subheadline).foregroundColor(ColorManager.textPrimary)
            let improvementAreas = userDetails.filter { !$0.value }.keys.sorted() // Filter by userDetails
            
            if improvementAreas.isEmpty {
                Text(LocalizedStringKey("Excellent! All key technical elements are performed correctly.")).font(.system(size: 15)).foregroundColor(.green).padding(.vertical, 8)
            } else {
                ForEach(improvementAreas, id: \.self) { key in
                    Text("• \(getDescription(for: key, passed: false))").font(.system(size: 15)).foregroundColor(ColorManager.textPrimary)
                }
            }
        }
        .padding(20).background(ColorManager.cardBackground.opacity(0.5)).cornerRadius(12).padding(.horizontal, 20)
    }
    
    // MARK: - Helper Functions
    private func formatRuleName(_ rule: String) -> String {
        // First check if there's a localized version of this technical term
        let localizedKey = NSLocalizedString(rule, comment: "Technical term")
        
        // If we get back the same string, it means there's no localization
        // In that case, format it for display
        if localizedKey == rule {
            return rule.replacingOccurrences(of: "_", with: " ").capitalized
        }
        
        return localizedKey
    }
    
    private func getDescription(for rule: String, passed: Bool) -> String {
        let baseMessage = formatRuleName(rule)
        if passed {
            return String(format: NSLocalizedString("%@: Well done!", comment: ""), baseMessage)
        } else {
            return String(format: NSLocalizedString("%@: Focus on improving this aspect. Check tutorials for guidance.", comment: ""), baseMessage)
        }
    }
}
</file>

<file path="MoveInsight/UploadTabView.swift">
import SwiftUI
import PhotosUI
import AVKit

struct UploadTabView: View {
    @State private var navigateToTechniquesList = false
    
    var body: some View {
        NavigationView {
            ZStack {
                ColorManager.background.ignoresSafeArea()
                
                VStack(spacing: 30) {
                    // Header
                    Text(LocalizedStringKey("Upload Video"))
                        .font(.title)
                        .foregroundColor(ColorManager.textPrimary)
                        .padding(.top, 40)
                    
                    Text(LocalizedStringKey("Choose the type of video you want to upload"))
                        .font(.subheadline)
                        .foregroundColor(ColorManager.textSecondary)
                        .multilineTextAlignment(.center)
                        .padding(.horizontal, 32)
                    
                    Spacer()
                    
                    // Technique Video Option
                    NavigationLink(destination: TechniquesListView(), isActive: $navigateToTechniquesList) {
                        EmptyView()
                    }
                    
                    Button(action: {
                        navigateToTechniquesList = true
                    }) {
                        UploadOptionCard(
                            title: LocalizedStringKey("Upload Technique Video"),
                            description: LocalizedStringKey("Analyze and compare your badminton techniques with model performers"),
                            icon: "figure.badminton"
                        )
                    }
                    .buttonStyle(ScaleButtonStyle())
                    
                    // Match Video Option
                    Button(action: {
                        // Do nothing for now, as per requirements
                    }) {
                        UploadOptionCard(
                            title: LocalizedStringKey("Upload Match Video"),
                            description: LocalizedStringKey("Upload your match videos for performance analysis"),
                            icon: "sportscourt"
                        )
                    }
                    .buttonStyle(ScaleButtonStyle())
                    
                    Spacer()
                }
                .padding(.horizontal, 20)
            }
            .navigationBarHidden(true)
        }
    }
}

// Card view for upload options
struct UploadOptionCard: View {
    let title: LocalizedStringKey
    let description: LocalizedStringKey
    let icon: String
    
    var body: some View {
        HStack(spacing: 20) {
            // Icon
            ZStack {
                Circle()
                    .fill(ColorManager.accentColor.opacity(0.2))
                    .frame(width: 70, height: 70)
                
                Image(systemName: icon)
                    .font(.system(size: 30))
                    .foregroundColor(ColorManager.accentColor)
            }
            
            // Text content
            VStack(alignment: .leading, spacing: 8) {
                Text(title)
                    .font(.headline)
                    .foregroundColor(ColorManager.textPrimary)
                
                Text(description)
                    .font(.subheadline)
                    .foregroundColor(ColorManager.textSecondary)
                    .lineLimit(2)
            }
            
            Spacer()
            
            // Arrow
            Image(systemName: "chevron.right")
                .foregroundColor(ColorManager.textSecondary)
        }
        .padding(20)
        .background(
            RoundedRectangle(cornerRadius: 16)
                .fill(ColorManager.cardBackground)
        )
    }
}

struct ScaleButtonStyle: ButtonStyle {
    func makeBody(configuration: Configuration) -> some View {
        configuration.label
            .scaleEffect(configuration.isPressed ? 0.97 : 1.0)
            .animation(.easeInOut(duration: 0.2), value: configuration.isPressed)
    }
}
</file>

<file path="MoveInsight/VideoPlayerViewModel.swift">
import SwiftUI
import AVKit
// Vision is no longer strictly needed for pose types if we use String keys,
// but CGPoint is from CoreGraphics, which is fine.
import Combine
import simd // For SIMD3 if you were to use 3D poses from server

// Data structure from server (ensure this matches your TechniqueAnalysisService.swift)
// struct ServerJointData: Codable {
//     let x: Double
//     let y: Double
//     let confidence: Double?
// }
// struct ServerFrameData: Codable {
//     let joints: [String: ServerJointData]
// }


// Define a simple struct for body connections using String joint names
struct StringBodyConnection: Identifiable {
    let id = UUID()
    let from: String // Server joint name
    let to: String   // Server joint name
}

// MARK: - Video Player View Model
class VideoPlayerViewModel: ObservableObject {
    let videoURL: URL
    let player: AVPlayer
    let asset: AVAsset

    // Published properties to update the UI
    // Poses will now be [String: CGPoint] using server joint names
    @Published var poses: [String: CGPoint] = [:]
    @Published var accumulatedPoses: [[String: CGPoint]] = []
    
    // Store the original server frames
    @Published var originalServerFrames: [ServerFrameData] = []

    // Body connections will use String joint names
    @Published var bodyConnections: [StringBodyConnection] = []
    
    @Published var isVideoReady = false
    @Published var isPlaying = false
    @Published var videoOrientation: CGImagePropertyOrientation = .up
    
    private(set) var videoSize: CGSize = .zero
    let videoSource: Pose3DBody.VideoSource // Assuming Pose3DBody.VideoSource is still a relevant enum for you
    
    private var displayLink: CADisplayLink?
    private var playerItemStatusObserver: NSKeyValueObservation?
    private var didPlayToEndTimeObserver: NSObjectProtocol?
    private var cancellables = Set<AnyCancellable>()

    // No longer need serverToVisionJointMap if we use server names directly for drawing

    init(videoURL: URL, videoSource: Pose3DBody.VideoSource) {
        self.videoURL = videoURL
        self.asset = AVAsset(url: videoURL)
        self.player = AVPlayer()
        self.videoSource = videoSource
        print("VideoPlayerViewModel initialized with URL: \(videoURL.path) for source: \(videoSource)")
        setupCustomBodyConnections() // Use custom string-based connections
        prepareVideoPlayback()
    }
    
    func setServerProcessedJoints(_ serverJoints: [ServerFrameData]) {
        self.originalServerFrames = serverJoints // Store original data

        var allFramesPoses: [[String: CGPoint]] = []
        for serverFrame in serverJoints {
            var framePoses: [String: CGPoint] = [:]
            for (jointNameString, jointData) in serverFrame.joints {
                // Use server coordinates directly, no flipping.
                // Assumes server coordinates are normalized [0,1] with (0,0) at top-left,
                // which matches typical screen coordinate systems for drawing.
                let point = CGPoint(x: jointData.x, y: jointData.y)
                framePoses[jointNameString] = point
            }
            allFramesPoses.append(framePoses)
        }
        DispatchQueue.main.async {
            self.accumulatedPoses = allFramesPoses
            print("Successfully set \(self.accumulatedPoses.count) frames of joint data for source: \(self.videoSource). Using direct server coordinates.")
        }
    }

    private func setupCustomBodyConnections() {
        // Define connections using the exact string names your server provides for MediaPipe joints
        // This list is based on common MediaPipe pose landmarks. Adjust if your server uses different names.
        bodyConnections = [
            // Torso
            StringBodyConnection(from: "Nose", to: "LeftShoulder"), // Or connect Nose to a "Neck" if available
            StringBodyConnection(from: "Nose", to: "RightShoulder"),
            StringBodyConnection(from: "LeftShoulder", to: "RightShoulder"),
            StringBodyConnection(from: "LeftShoulder", to: "LeftHip"),
            StringBodyConnection(from: "RightShoulder", to: "RightHip"),
            StringBodyConnection(from: "LeftHip", to: "RightHip"),

            // Left Arm
            StringBodyConnection(from: "LeftShoulder", to: "LeftElbow"),
            StringBodyConnection(from: "LeftElbow", to: "LeftWrist"),
            
            // Right Arm
            StringBodyConnection(from: "RightShoulder", to: "RightElbow"),
            StringBodyConnection(from: "RightElbow", to: "RightWrist"),

            // Left Leg
            StringBodyConnection(from: "LeftHip", to: "LeftKnee"),
            StringBodyConnection(from: "LeftKnee", to: "LeftAnkle"),
            StringBodyConnection(from: "LeftAnkle", to: "LeftHeel"),
            StringBodyConnection(from: "LeftHeel", to: "LeftFootIndex"), // "LeftFootIndex" is MediaPipe's "Toe"

            // Right Leg
            StringBodyConnection(from: "RightHip", to: "RightKnee"),
            StringBodyConnection(from: "RightKnee", to: "RightAnkle"),
            StringBodyConnection(from: "RightAnkle", to: "RightHeel"),
            StringBodyConnection(from: "RightHeel", to: "RightFootIndex") // "RightFootIndex" is MediaPipe's "Toe"
        ]
        // You can add more connections if needed, e.g., Wrist to Hand, Ankle to FootIndex directly.
        // Example:
        // StringBodyConnection(from: "LeftWrist", to: "LeftPinky"), // If server provides hand landmarks
        // StringBodyConnection(from: "LeftAnkle", to: "LeftFootIndex"), // Direct ankle to toe
    }

    private func prepareVideoPlayback() {
        Task { [weak self] in
            guard let self = self else { return }

            do {
                let tracks = try await self.asset.load(.tracks)
                if let videoTrack = tracks.first(where: { $0.mediaType == .video }) {
                    let transform = try await videoTrack.load(.preferredTransform)
                    let orientation = self.orientation(from: transform)
                    let size = try await videoTrack.load(.naturalSize)
                    
                    await MainActor.run {
                        self.videoOrientation = orientation
                        self.videoSize = size
                        self.setupPlayerItemAndObservers()
                    }
                } else {
                    print("Error: No video track found in asset for \(self.videoSource).")
                    await MainActor.run { self.isVideoReady = false }
                }
            } catch {
                print("Error loading asset tracks or transform for \(self.videoSource): \(error)")
                await MainActor.run { self.isVideoReady = false }
            }
        }
    }
    
    private func setupPlayerItemAndObservers() {
        let playerItem = AVPlayerItem(asset: asset)
        playerItemStatusObserver = playerItem.observe(\.status, options: [.new, .initial]) { [weak self] item, _ in
            guard let self = self else { return }
            DispatchQueue.main.async {
                switch item.status {
                case .readyToPlay:
                    self.isVideoReady = true
                    self.setupDisplayLink()
                    print("PlayerItem ready for \(self.videoSource). Video size: \(self.videoSize).")
                case .failed:
                    let errorDesc = item.error?.localizedDescription ?? "Unknown error"
                    print("PlayerItem failed for \(self.videoSource) (URL: \(self.videoURL.lastPathComponent)): \(errorDesc). Current isVideoReady was: \(self.isVideoReady)")
                    if let underlyingError = item.error {
                        print("Detailed underlying error: \(underlyingError)")
                    }
                    self.isVideoReady = false
                default:
                    self.isVideoReady = false
                }
            }
        }

        didPlayToEndTimeObserver = NotificationCenter.default.addObserver(forName: .AVPlayerItemDidPlayToEndTime, object: playerItem, queue: .main) { [weak self] _ in
            self?.isPlaying = false
            self?.player.seek(to: .zero)
        }
        
        player.replaceCurrentItem(with: playerItem)
        player.isMuted = (videoSource == .secondary)
        player.allowsExternalPlayback = true
    }

    private func setupDisplayLink() {
        displayLink?.invalidate()
        displayLink = CADisplayLink(target: self, selector: #selector(displayLinkDidFire))
        displayLink?.add(to: .main, forMode: .common)
        displayLink?.isPaused = !isPlaying
    }

    @objc private func displayLinkDidFire(_ link: CADisplayLink) {
        guard player.timeControlStatus == .playing, !accumulatedPoses.isEmpty else {
            if !poses.isEmpty { DispatchQueue.main.async { self.poses = [:] } }
            return
        }
        
        let currentTime = player.currentTime()
        guard let videoTrack = asset.tracks(withMediaType: .video).first, videoTrack.nominalFrameRate > 0 else {
            if !poses.isEmpty { DispatchQueue.main.async { self.poses = [:] } }
            return
        }
        let frameRate = videoTrack.nominalFrameRate
        let currentFrameIndex = Int(CMTimeGetSeconds(currentTime) * Double(frameRate))

        if currentFrameIndex >= 0 && currentFrameIndex < accumulatedPoses.count {
            DispatchQueue.main.async {
                if self.poses != self.accumulatedPoses[currentFrameIndex] { // Only update if different
                     self.poses = self.accumulatedPoses[currentFrameIndex]
                }
            }
        } else {
            if !poses.isEmpty { DispatchQueue.main.async { self.poses = [:] } }
        }
    }
    
    private func orientation(from transform: CGAffineTransform) -> CGImagePropertyOrientation {
        if transform.a == 0 && transform.b == 1.0 && transform.c == -1.0 && transform.d == 0 { return .right }
        if transform.a == 0 && transform.b == -1.0 && transform.c == 1.0 && transform.d == 0 { return .left }
        if transform.a == -1.0 && transform.b == 0 && transform.c == 0 && transform.d == -1.0 { return .down }
        return .up
    }

    // MARK: - Playback Controls
    func play() {
        if isVideoReady && !isPlaying {
            player.play()
            isPlaying = true
            displayLink?.isPaused = false
        }
    }
    func pause() {
        if isPlaying {
            player.pause()
            isPlaying = false
            displayLink?.isPaused = true
        }
    }
    func togglePlayPause() { if isPlaying { pause() } else { play() } }
    func restart() {
        player.seek(to: .zero) { [weak self] finished in
            if finished {
                self?.play()
            }
        }
    }
    
    func clearAllPoseData() {
        DispatchQueue.main.async {
            self.accumulatedPoses.removeAll()
            self.poses.removeAll()
            self.originalServerFrames.removeAll()
            print("Cleared all pose data for source: \(self.videoSource).")
        }
    }

    // MARK: - Cleanup
    func cleanup() {
        print("VideoPlayerViewModel cleanup initiated for \(videoSource).")
        pause()
        player.replaceCurrentItem(with: nil)
        displayLink?.invalidate()
        displayLink = nil
        playerItemStatusObserver?.invalidate()
        playerItemStatusObserver = nil
        if let observer = didPlayToEndTimeObserver {
            NotificationCenter.default.removeObserver(observer)
            didPlayToEndTimeObserver = nil
        }
        cancellables.forEach { $0.cancel() }
        cancellables.removeAll()
        clearAllPoseData()
        print("VideoPlayerViewModel cleaned up for \(videoSource).")
    }

    deinit {
        print("VideoPlayerViewModel deinit for \(videoSource)")
        displayLink?.invalidate()
        playerItemStatusObserver?.invalidate()
        if let observer = didPlayToEndTimeObserver {
            NotificationCenter.default.removeObserver(observer)
        }
    }
}

// Ensure Pose3DBody.VideoSource is defined elsewhere, e.g.:
// enum VideoSource { case primary, secondary }
// struct Pose3DBody { let videoSource: VideoSource } // Simplified for this context
</file>

<file path="MoveInsight.xcodeproj/project.pbxproj">
// !$*UTF8*$!
{
	archiveVersion = 1;
	classes = {
	};
	objectVersion = 77;
	objects = {

/* Begin PBXFileReference section */
		CB1C3FAE2D9D493B008FC7AB /* MoveInsight.app */ = {isa = PBXFileReference; explicitFileType = wrapper.application; includeInIndex = 0; path = MoveInsight.app; sourceTree = BUILT_PRODUCTS_DIR; };
/* End PBXFileReference section */

/* Begin PBXFileSystemSynchronizedBuildFileExceptionSet section */
		CB6878622DD191DB00A36A08 /* Exceptions for "MoveInsight" folder in "MoveInsight" target */ = {
			isa = PBXFileSystemSynchronizedBuildFileExceptionSet;
			membershipExceptions = (
				Info.plist,
			);
			target = CB1C3FAD2D9D493B008FC7AB /* MoveInsight */;
		};
/* End PBXFileSystemSynchronizedBuildFileExceptionSet section */

/* Begin PBXFileSystemSynchronizedRootGroup section */
		CB1C3FB02D9D493B008FC7AB /* MoveInsight */ = {
			isa = PBXFileSystemSynchronizedRootGroup;
			exceptions = (
				CB6878622DD191DB00A36A08 /* Exceptions for "MoveInsight" folder in "MoveInsight" target */,
			);
			path = MoveInsight;
			sourceTree = "<group>";
		};
/* End PBXFileSystemSynchronizedRootGroup section */

/* Begin PBXFrameworksBuildPhase section */
		CB1C3FAB2D9D493B008FC7AB /* Frameworks */ = {
			isa = PBXFrameworksBuildPhase;
			buildActionMask = 2147483647;
			files = (
			);
			runOnlyForDeploymentPostprocessing = 0;
		};
/* End PBXFrameworksBuildPhase section */

/* Begin PBXGroup section */
		CB1C3FA52D9D493B008FC7AB = {
			isa = PBXGroup;
			children = (
				CB1C3FB02D9D493B008FC7AB /* MoveInsight */,
				CB1C3FAF2D9D493B008FC7AB /* Products */,
			);
			sourceTree = "<group>";
		};
		CB1C3FAF2D9D493B008FC7AB /* Products */ = {
			isa = PBXGroup;
			children = (
				CB1C3FAE2D9D493B008FC7AB /* MoveInsight.app */,
			);
			name = Products;
			sourceTree = "<group>";
		};
/* End PBXGroup section */

/* Begin PBXNativeTarget section */
		CB1C3FAD2D9D493B008FC7AB /* MoveInsight */ = {
			isa = PBXNativeTarget;
			buildConfigurationList = CB1C3FBC2D9D493D008FC7AB /* Build configuration list for PBXNativeTarget "MoveInsight" */;
			buildPhases = (
				CB1C3FAA2D9D493B008FC7AB /* Sources */,
				CB1C3FAB2D9D493B008FC7AB /* Frameworks */,
				CB1C3FAC2D9D493B008FC7AB /* Resources */,
			);
			buildRules = (
			);
			dependencies = (
			);
			fileSystemSynchronizedGroups = (
				CB1C3FB02D9D493B008FC7AB /* MoveInsight */,
			);
			name = MoveInsight;
			packageProductDependencies = (
			);
			productName = MoveInsight;
			productReference = CB1C3FAE2D9D493B008FC7AB /* MoveInsight.app */;
			productType = "com.apple.product-type.application";
		};
/* End PBXNativeTarget section */

/* Begin PBXProject section */
		CB1C3FA62D9D493B008FC7AB /* Project object */ = {
			isa = PBXProject;
			attributes = {
				BuildIndependentTargetsInParallel = 1;
				LastSwiftUpdateCheck = 1610;
				LastUpgradeCheck = 1610;
				TargetAttributes = {
					CB1C3FAD2D9D493B008FC7AB = {
						CreatedOnToolsVersion = 16.1;
					};
				};
			};
			buildConfigurationList = CB1C3FA92D9D493B008FC7AB /* Build configuration list for PBXProject "MoveInsight" */;
			developmentRegion = en;
			hasScannedForEncodings = 0;
			knownRegions = (
				en,
				Base,
				"zh-Hans",
			);
			mainGroup = CB1C3FA52D9D493B008FC7AB;
			minimizedProjectReferenceProxies = 1;
			preferredProjectObjectVersion = 77;
			productRefGroup = CB1C3FAF2D9D493B008FC7AB /* Products */;
			projectDirPath = "";
			projectRoot = "";
			targets = (
				CB1C3FAD2D9D493B008FC7AB /* MoveInsight */,
			);
		};
/* End PBXProject section */

/* Begin PBXResourcesBuildPhase section */
		CB1C3FAC2D9D493B008FC7AB /* Resources */ = {
			isa = PBXResourcesBuildPhase;
			buildActionMask = 2147483647;
			files = (
			);
			runOnlyForDeploymentPostprocessing = 0;
		};
/* End PBXResourcesBuildPhase section */

/* Begin PBXSourcesBuildPhase section */
		CB1C3FAA2D9D493B008FC7AB /* Sources */ = {
			isa = PBXSourcesBuildPhase;
			buildActionMask = 2147483647;
			files = (
			);
			runOnlyForDeploymentPostprocessing = 0;
		};
/* End PBXSourcesBuildPhase section */

/* Begin XCBuildConfiguration section */
		CB1C3FBA2D9D493D008FC7AB /* Debug */ = {
			isa = XCBuildConfiguration;
			buildSettings = {
				ALWAYS_SEARCH_USER_PATHS = NO;
				ASSETCATALOG_COMPILER_GENERATE_SWIFT_ASSET_SYMBOL_EXTENSIONS = YES;
				CLANG_ANALYZER_NONNULL = YES;
				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
				CLANG_ENABLE_MODULES = YES;
				CLANG_ENABLE_OBJC_ARC = YES;
				CLANG_ENABLE_OBJC_WEAK = YES;
				CLANG_WARN_BLOCK_CAPTURE_AUTORELEASING = YES;
				CLANG_WARN_BOOL_CONVERSION = YES;
				CLANG_WARN_COMMA = YES;
				CLANG_WARN_CONSTANT_CONVERSION = YES;
				CLANG_WARN_DEPRECATED_OBJC_IMPLEMENTATIONS = YES;
				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
				CLANG_WARN_EMPTY_BODY = YES;
				CLANG_WARN_ENUM_CONVERSION = YES;
				CLANG_WARN_INFINITE_RECURSION = YES;
				CLANG_WARN_INT_CONVERSION = YES;
				CLANG_WARN_NON_LITERAL_NULL_CONVERSION = YES;
				CLANG_WARN_OBJC_IMPLICIT_RETAIN_SELF = YES;
				CLANG_WARN_OBJC_LITERAL_CONVERSION = YES;
				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
				CLANG_WARN_QUOTED_INCLUDE_IN_FRAMEWORK_HEADER = YES;
				CLANG_WARN_RANGE_LOOP_ANALYSIS = YES;
				CLANG_WARN_STRICT_PROTOTYPES = YES;
				CLANG_WARN_SUSPICIOUS_MOVE = YES;
				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
				CLANG_WARN_UNREACHABLE_CODE = YES;
				CLANG_WARN__DUPLICATE_METHOD_MATCH = YES;
				COPY_PHASE_STRIP = NO;
				DEBUG_INFORMATION_FORMAT = dwarf;
				ENABLE_STRICT_OBJC_MSGSEND = YES;
				ENABLE_TESTABILITY = YES;
				ENABLE_USER_SCRIPT_SANDBOXING = YES;
				GCC_C_LANGUAGE_STANDARD = gnu17;
				GCC_DYNAMIC_NO_PIC = NO;
				GCC_NO_COMMON_BLOCKS = YES;
				GCC_OPTIMIZATION_LEVEL = 0;
				GCC_PREPROCESSOR_DEFINITIONS = (
					"DEBUG=1",
					"$(inherited)",
				);
				GCC_WARN_64_TO_32_BIT_CONVERSION = YES;
				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
				GCC_WARN_UNDECLARED_SELECTOR = YES;
				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
				GCC_WARN_UNUSED_FUNCTION = YES;
				GCC_WARN_UNUSED_VARIABLE = YES;
				IPHONEOS_DEPLOYMENT_TARGET = 18.1;
				LOCALIZATION_PREFERS_STRING_CATALOGS = YES;
				MTL_ENABLE_DEBUG_INFO = INCLUDE_SOURCE;
				MTL_FAST_MATH = YES;
				ONLY_ACTIVE_ARCH = YES;
				SDKROOT = iphoneos;
				SWIFT_ACTIVE_COMPILATION_CONDITIONS = "DEBUG $(inherited)";
				SWIFT_OPTIMIZATION_LEVEL = "-Onone";
			};
			name = Debug;
		};
		CB1C3FBB2D9D493D008FC7AB /* Release */ = {
			isa = XCBuildConfiguration;
			buildSettings = {
				ALWAYS_SEARCH_USER_PATHS = NO;
				ASSETCATALOG_COMPILER_GENERATE_SWIFT_ASSET_SYMBOL_EXTENSIONS = YES;
				CLANG_ANALYZER_NONNULL = YES;
				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
				CLANG_ENABLE_MODULES = YES;
				CLANG_ENABLE_OBJC_ARC = YES;
				CLANG_ENABLE_OBJC_WEAK = YES;
				CLANG_WARN_BLOCK_CAPTURE_AUTORELEASING = YES;
				CLANG_WARN_BOOL_CONVERSION = YES;
				CLANG_WARN_COMMA = YES;
				CLANG_WARN_CONSTANT_CONVERSION = YES;
				CLANG_WARN_DEPRECATED_OBJC_IMPLEMENTATIONS = YES;
				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
				CLANG_WARN_EMPTY_BODY = YES;
				CLANG_WARN_ENUM_CONVERSION = YES;
				CLANG_WARN_INFINITE_RECURSION = YES;
				CLANG_WARN_INT_CONVERSION = YES;
				CLANG_WARN_NON_LITERAL_NULL_CONVERSION = YES;
				CLANG_WARN_OBJC_IMPLICIT_RETAIN_SELF = YES;
				CLANG_WARN_OBJC_LITERAL_CONVERSION = YES;
				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
				CLANG_WARN_QUOTED_INCLUDE_IN_FRAMEWORK_HEADER = YES;
				CLANG_WARN_RANGE_LOOP_ANALYSIS = YES;
				CLANG_WARN_STRICT_PROTOTYPES = YES;
				CLANG_WARN_SUSPICIOUS_MOVE = YES;
				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
				CLANG_WARN_UNREACHABLE_CODE = YES;
				CLANG_WARN__DUPLICATE_METHOD_MATCH = YES;
				COPY_PHASE_STRIP = NO;
				DEBUG_INFORMATION_FORMAT = "dwarf-with-dsym";
				ENABLE_NS_ASSERTIONS = NO;
				ENABLE_STRICT_OBJC_MSGSEND = YES;
				ENABLE_USER_SCRIPT_SANDBOXING = YES;
				GCC_C_LANGUAGE_STANDARD = gnu17;
				GCC_NO_COMMON_BLOCKS = YES;
				GCC_WARN_64_TO_32_BIT_CONVERSION = YES;
				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
				GCC_WARN_UNDECLARED_SELECTOR = YES;
				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
				GCC_WARN_UNUSED_FUNCTION = YES;
				GCC_WARN_UNUSED_VARIABLE = YES;
				IPHONEOS_DEPLOYMENT_TARGET = 18.1;
				LOCALIZATION_PREFERS_STRING_CATALOGS = YES;
				MTL_ENABLE_DEBUG_INFO = NO;
				MTL_FAST_MATH = YES;
				SDKROOT = iphoneos;
				SWIFT_COMPILATION_MODE = wholemodule;
				VALIDATE_PRODUCT = YES;
			};
			name = Release;
		};
		CB1C3FBD2D9D493D008FC7AB /* Debug */ = {
			isa = XCBuildConfiguration;
			buildSettings = {
				ASSETCATALOG_COMPILER_APPICON_NAME = AppIcon;
				ASSETCATALOG_COMPILER_GLOBAL_ACCENT_COLOR_NAME = AccentColor;
				CODE_SIGN_STYLE = Automatic;
				CURRENT_PROJECT_VERSION = 2;
				DEVELOPMENT_ASSET_PATHS = "\"MoveInsight/Preview Content\"";
				DEVELOPMENT_TEAM = 2VS9GH9QTB;
				ENABLE_PREVIEWS = YES;
				GENERATE_INFOPLIST_FILE = YES;
				INFOPLIST_FILE = MoveInsight/Info.plist;
				INFOPLIST_KEY_NSPhotoLibraryUsageDescription = "Needs permission to upload video";
				INFOPLIST_KEY_UIApplicationSceneManifest_Generation = YES;
				INFOPLIST_KEY_UIApplicationSupportsIndirectInputEvents = YES;
				INFOPLIST_KEY_UILaunchScreen_Generation = YES;
				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPad = "UIInterfaceOrientationPortrait UIInterfaceOrientationPortraitUpsideDown UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPhone = "UIInterfaceOrientationPortrait UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
				IPHONEOS_DEPLOYMENT_TARGET = 17.0;
				LD_RUNPATH_SEARCH_PATHS = (
					"$(inherited)",
					"@executable_path/Frameworks",
				);
				MARKETING_VERSION = 1.0;
				PRODUCT_BUNDLE_IDENTIFIER = com.MoveInsight;
				PRODUCT_NAME = "$(TARGET_NAME)";
				SUPPORTED_PLATFORMS = "iphoneos iphonesimulator";
				SUPPORTS_MACCATALYST = NO;
				SUPPORTS_MAC_DESIGNED_FOR_IPHONE_IPAD = NO;
				SUPPORTS_XR_DESIGNED_FOR_IPHONE_IPAD = NO;
				SWIFT_EMIT_LOC_STRINGS = YES;
				SWIFT_VERSION = 5.0;
				TARGETED_DEVICE_FAMILY = "1,2";
			};
			name = Debug;
		};
		CB1C3FBE2D9D493D008FC7AB /* Release */ = {
			isa = XCBuildConfiguration;
			buildSettings = {
				ASSETCATALOG_COMPILER_APPICON_NAME = AppIcon;
				ASSETCATALOG_COMPILER_GLOBAL_ACCENT_COLOR_NAME = AccentColor;
				CODE_SIGN_STYLE = Automatic;
				CURRENT_PROJECT_VERSION = 2;
				DEVELOPMENT_ASSET_PATHS = "\"MoveInsight/Preview Content\"";
				DEVELOPMENT_TEAM = 2VS9GH9QTB;
				ENABLE_PREVIEWS = YES;
				GENERATE_INFOPLIST_FILE = YES;
				INFOPLIST_FILE = MoveInsight/Info.plist;
				INFOPLIST_KEY_NSPhotoLibraryUsageDescription = "Needs permission to upload video";
				INFOPLIST_KEY_UIApplicationSceneManifest_Generation = YES;
				INFOPLIST_KEY_UIApplicationSupportsIndirectInputEvents = YES;
				INFOPLIST_KEY_UILaunchScreen_Generation = YES;
				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPad = "UIInterfaceOrientationPortrait UIInterfaceOrientationPortraitUpsideDown UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPhone = "UIInterfaceOrientationPortrait UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
				IPHONEOS_DEPLOYMENT_TARGET = 17.0;
				LD_RUNPATH_SEARCH_PATHS = (
					"$(inherited)",
					"@executable_path/Frameworks",
				);
				MARKETING_VERSION = 1.0;
				PRODUCT_BUNDLE_IDENTIFIER = com.MoveInsight;
				PRODUCT_NAME = "$(TARGET_NAME)";
				SUPPORTED_PLATFORMS = "iphoneos iphonesimulator";
				SUPPORTS_MACCATALYST = NO;
				SUPPORTS_MAC_DESIGNED_FOR_IPHONE_IPAD = NO;
				SUPPORTS_XR_DESIGNED_FOR_IPHONE_IPAD = NO;
				SWIFT_EMIT_LOC_STRINGS = YES;
				SWIFT_VERSION = 5.0;
				TARGETED_DEVICE_FAMILY = "1,2";
			};
			name = Release;
		};
/* End XCBuildConfiguration section */

/* Begin XCConfigurationList section */
		CB1C3FA92D9D493B008FC7AB /* Build configuration list for PBXProject "MoveInsight" */ = {
			isa = XCConfigurationList;
			buildConfigurations = (
				CB1C3FBA2D9D493D008FC7AB /* Debug */,
				CB1C3FBB2D9D493D008FC7AB /* Release */,
			);
			defaultConfigurationIsVisible = 0;
			defaultConfigurationName = Release;
		};
		CB1C3FBC2D9D493D008FC7AB /* Build configuration list for PBXNativeTarget "MoveInsight" */ = {
			isa = XCConfigurationList;
			buildConfigurations = (
				CB1C3FBD2D9D493D008FC7AB /* Debug */,
				CB1C3FBE2D9D493D008FC7AB /* Release */,
			);
			defaultConfigurationIsVisible = 0;
			defaultConfigurationName = Release;
		};
/* End XCConfigurationList section */
	};
	rootObject = CB1C3FA62D9D493B008FC7AB /* Project object */;
}
</file>

</files>
